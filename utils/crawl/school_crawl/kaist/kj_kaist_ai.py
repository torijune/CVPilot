import requests
from bs4 import BeautifulSoup
import json
import re
from typing import Dict, List, Optional
import time

class KAISTGSAICrawler:
    def __init__(self):
        self.base_url = "https://gsai.kaist.ac.kr"
        self.people_url = "https://gsai.kaist.ac.kr/people-2/?lang=ko"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
    
    def get_page_content(self, url: str) -> Optional[str]:
        """ì›¹í˜ì´ì§€ ë‚´ìš©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            return response.text
        except requests.RequestException as e:
            print(f"âŒ í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def extract_professor_info(self, professor_element) -> Dict:
        """ê°œë³„ êµìˆ˜ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        info = {}
        
        try:
            # ì´ë¦„ ì¶”ì¶œ - strong íƒœê·¸ì—ì„œ ì°¾ê¸°
            name_element = professor_element.find('strong')
            if name_element:
                name_text = name_element.get_text(strip=True)
                # ê´„í˜¸ ì•ˆì˜ ì§ìœ„ ì œê±° (ì˜ˆ: "ì •ì†¡ (ëŒ€í•™ì›ì¥)" -> "ì •ì†¡")
                name = re.sub(r'\s*\([^)]*\)', '', name_text)
                info['name'] = name.strip()
            
            # ëª¨ë“  í…ìŠ¤íŠ¸ ì •ë³´ ì¶”ì¶œ
            text_content = professor_element.get_text()
            
            # ì—°êµ¬ë¶„ì•¼ ì¶”ì¶œ
            research_match = re.search(r'ì—°êµ¬ë¶„ì•¼\s*:\s*([^\n]+)', text_content)
            if research_match:
                info['research_area'] = research_match.group(1).strip()
            
            # ì´ë©”ì¼ ì¶”ì¶œ
            email_match = re.search(r'ì´ë©”ì¼\s*:\s*([^\s\n]+)', text_content)
            if email_match:
                info['email'] = email_match.group(1).strip()
            
            # ì›¹ì‚¬ì´íŠ¸ ì¶”ì¶œ
            website_element = professor_element.find('a', href=True)
            if website_element:
                website_url = website_element.get('href')
                if website_url:
                    # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                    if website_url.startswith('/'):
                        website_url = self.base_url + website_url
                    elif not website_url.startswith('http'):
                        website_url = self.base_url + '/' + website_url
                    info['website'] = website_url
            
        except Exception as e:
            print(f"âš ï¸ êµìˆ˜ ì •ë³´ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return info
    
    def is_professor(self, text_content: str) -> bool:
        """í…ìŠ¤íŠ¸ê°€ êµìˆ˜ ì •ë³´ì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
        # êµìˆ˜ ê´€ë ¨ í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸
        professor_keywords = ['êµìˆ˜', 'ë¶€êµìˆ˜', 'ì¡°êµìˆ˜', 'ì„ì¢Œêµìˆ˜', 'Professor', 'Associate Professor', 'Assistant Professor']
        
        # ì§ìœ„ ì •ë³´ì—ì„œ êµìˆ˜ ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¸
        position_match = re.search(r'ì§ìœ„\s*:\s*([^\n]+)', text_content)
        if position_match:
            position = position_match.group(1).strip()
            for keyword in professor_keywords:
                if keyword in position:
                    return True
        
        # ì´ë¦„ì´ ìˆê³  ì—°êµ¬ë¶„ì•¼ê°€ ìˆëŠ” ê²½ìš° (êµìˆ˜ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ)
        if 'ì—°êµ¬ë¶„ì•¼' in text_content and 'ì´ë©”ì¼' in text_content:
            return True
        
        return False
    
    def crawl_professors(self) -> List[Dict]:
        """êµìˆ˜ ì •ë³´ë§Œ í¬ë¡¤ë§í•©ë‹ˆë‹¤."""
        print("ğŸ” KAIST GSAI êµìˆ˜ ì •ë³´ í¬ë¡¤ë§ ì‹œì‘...")
        
        content = self.get_page_content(self.people_url)
        if not content:
            return []
        
        soup = BeautifulSoup(content, 'html.parser')
        professors = []
        
        # ë” êµ¬ì²´ì ì¸ ì„ íƒìë¡œ êµìˆ˜ ì •ë³´ ì°¾ê¸°
        professor_containers = []
        
        # ë°©ë²• 1: íŠ¹ì • í´ë˜ìŠ¤ëª…ìœ¼ë¡œ ì°¾ê¸°
        possible_classes = ['professor', 'member', 'people', 'faculty', 'staff', 'team-member']
        for class_name in possible_classes:
            containers = soup.find_all(['div', 'article'], class_=re.compile(class_name, re.I))
            if containers:
                professor_containers.extend(containers)
                print(f"âœ… '{class_name}' í´ë˜ìŠ¤ë¡œ {len(containers)}ê°œ ì»¨í…Œì´ë„ˆ ë°œê²¬")
        
        # ë°©ë²• 2: íŠ¹ì • êµ¬ì¡° íŒ¨í„´ìœ¼ë¡œ ì°¾ê¸°
        if not professor_containers:
            # ì´ë¯¸ì§€ì™€ ì´ë¦„ì´ í•¨ê»˜ ìˆëŠ” êµ¬ì¡° ì°¾ê¸°
            all_divs = soup.find_all('div', recursive=True)
            for div in all_divs:
                # ì´ë¯¸ì§€ì™€ strong íƒœê·¸(ì´ë¦„)ê°€ ëª¨ë‘ ìˆëŠ” div ì°¾ê¸°
                if div.find('img') and div.find('strong'):
                    # í…ìŠ¤íŠ¸ì— "ì—°êµ¬ë¶„ì•¼" ë˜ëŠ” "ì´ë©”ì¼"ì´ í¬í•¨ëœ ê²½ìš°ë§Œ
                    text = div.get_text()
                    if 'ì—°êµ¬ë¶„ì•¼' in text or 'ì´ë©”ì¼' in text:
                        professor_containers.append(div)
        
        # ì¤‘ë³µ ì œê±° ë° êµìˆ˜ë§Œ í•„í„°ë§
        unique_containers = []
        seen_texts = set()
        
        for container in professor_containers:
            text_content = container.get_text().strip()
            if text_content and text_content not in seen_texts:
                # êµìˆ˜ì¸ì§€ í™•ì¸
                if self.is_professor(text_content):
                    unique_containers.append(container)
                    seen_texts.add(text_content)
        
        print(f"ğŸ“Š ë°œê²¬ëœ ê³ ìœ  êµìˆ˜ ì»¨í…Œì´ë„ˆ ìˆ˜: {len(unique_containers)}")
        
        for i, container in enumerate(unique_containers):
            print(f"\nğŸ” êµìˆ˜ {i+1} ì •ë³´ ì¶”ì¶œ ì¤‘...")
            
            professor_info = self.extract_professor_info(container)
            
            if professor_info.get('name'):
                # í•„ìš”í•œ ì •ë³´ë§Œ í¬í•¨
                filtered_info = {
                    'name': professor_info.get('name', ''),
                    'research_area': professor_info.get('research_area', ''),
                    'email': professor_info.get('email', ''),
                    'website': professor_info.get('website', '')
                }
                professors.append(filtered_info)
                print(f"âœ… {filtered_info['name']} - {filtered_info.get('research_area', 'N/A')}")
            else:
                print(f"âš ï¸ ì´ë¦„ì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” êµìˆ˜ ì •ë³´ ê±´ë„ˆë›°ê¸°")
            
            # ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ë”œë ˆì´
            time.sleep(0.2)
        
        return professors
    
    def save_to_json(self, professors: List[Dict], filename: str = "kaist_gsai_professors_only.json"):
        """êµìˆ˜ ì •ë³´ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
        data = {
            "university": "KAIST",
            "department": "Graduate School of AI",
            "crawled_at": time.strftime("%Y-%m-%d %H:%M:%S"),
            "total_professors": len(professors),
            "professors": professors
        }
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"âœ… êµìˆ˜ ì •ë³´ ì €ì¥ ì™„ë£Œ: {filename}")
            return True
        except Exception as e:
            print(f"âŒ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def print_summary(self, professors: List[Dict]):
        """í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½ì„ ì¶œë ¥í•©ë‹ˆë‹¤."""
        print(f"\nğŸ“Š í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½:")
        print(f"   ì´ êµìˆ˜ ìˆ˜: {len(professors)}")
        
        for i, prof in enumerate(professors, 1):
            print(f"\n   {i}. {prof.get('name', 'N/A')}")
            print(f"      ì—°êµ¬ë¶„ì•¼: {prof.get('research_area', 'N/A')}")
            print(f"      ì´ë©”ì¼: {prof.get('email', 'N/A')}")
            print(f"      ì›¹ì‚¬ì´íŠ¸: {prof.get('website', 'N/A')}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    crawler = KAISTGSAICrawler()
    
    # êµìˆ˜ ì •ë³´ í¬ë¡¤ë§
    professors = crawler.crawl_professors()
    
    if professors:
        # ê²°ê³¼ ì¶œë ¥
        crawler.print_summary(professors)
        
        # JSON íŒŒì¼ë¡œ ì €ì¥
        crawler.save_to_json(professors)
        
        print(f"\nğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ! ì´ {len(professors)}ëª…ì˜ êµìˆ˜ ì •ë³´ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
    else:
        print("âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: êµìˆ˜ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
