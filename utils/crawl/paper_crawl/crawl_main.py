import json
import importlib
import os
import csv
from typing import List, Dict, Any
from supabase import create_client, Client
from dotenv import load_dotenv
import time

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# Supabase í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
SUPABASE_URL = os.getenv("SUPABASE_URL")
# SUPABASE_ANON_KEY, SUPABASE_SERVICE_ROLE_KEY, SUPABASE_KEY
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

if not SUPABASE_URL or not SUPABASE_KEY:
    print("âš ï¸ Supabase í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. DB ì €ì¥ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    supabase: Client = None
else:
    supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
    print("âœ… Supabase í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì™„ë£Œ")
    
    # ì‹¤ì œ ì—°ê²° í…ŒìŠ¤íŠ¸
    try:
        # papers í…Œì´ë¸”ì— ì ‘ê·¼í•´ì„œ ì—°ê²° ìƒíƒœ í™•ì¸
        result = supabase.table('papers').select('id').limit(1).execute()
        print("âœ… Supabase ì—°ê²° ë° papers í…Œì´ë¸” ì ‘ê·¼ ì„±ê³µ")
    except Exception as e:
        print(f"âŒ Supabase ì—°ê²° ì‹¤íŒ¨: {e}")
        print("âš ï¸ DB ì €ì¥ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        supabase = None

def load_existing_papers(csv_file: str) -> set:
    """
    ê¸°ì¡´ CSV íŒŒì¼ì—ì„œ ë…¼ë¬¸ ì œëª©ê³¼ ì»¨í¼ëŸ°ìŠ¤ë¥¼ ì½ì–´ì„œ ì¤‘ë³µ ì²´í¬ìš© set ë°˜í™˜
    """
    existing_papers = set()
    if os.path.exists(csv_file):
        try:
            with open(csv_file, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    # ì œëª©ê³¼ ì»¨í¼ëŸ°ìŠ¤ ì¡°í•©ìœ¼ë¡œ ì¤‘ë³µ ì²´í¬
                    paper_key = (row.get('title', '').strip(), row.get('conference', '').strip())
                    existing_papers.add(paper_key)
            print(f"ğŸ“– ê¸°ì¡´ CSV íŒŒì¼ì—ì„œ {len(existing_papers)}ê°œ ë…¼ë¬¸ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ ê¸°ì¡´ CSV íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
    return existing_papers

def check_paper_exists(title: str, conference: str, existing_papers: set) -> bool:
    """
    ë…¼ë¬¸ì´ ì´ë¯¸ CSVì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    """
    paper_key = (title.strip(), conference.strip())
    return paper_key in existing_papers

def save_paper_to_csv(paper: Dict, field_name: str, conf_name: str, csv_file: str, existing_papers: set) -> bool:
    """
    ë‹¨ì¼ ë…¼ë¬¸ì„ CSV íŒŒì¼ì— ì €ì¥ (ì¤‘ë³µ ì²´í¬ í¬í•¨)
    """
    try:
        # ì¤‘ë³µ ì²´í¬
        if check_paper_exists(paper.get('title', ''), conf_name, existing_papers):
            print(f"â­ï¸ ì¤‘ë³µ ë…¼ë¬¸ íŒ¨ìŠ¤: {paper.get('title', '')[:50]}...")
            return False
        
        # CSV ì €ì¥ìš© ë°ì´í„° ì¤€ë¹„
        csv_paper = {
            'title': paper.get('title', ''),
            'abstract': paper.get('abstract', ''),
            'authors': paper.get('authors', ''),
            'conference': conf_name,
            'year': paper.get('year', 2024),
            'field': field_name,
            'url': paper.get('url', '')
        }
        
        # CSV íŒŒì¼ì— ì¶”ê°€ (append ëª¨ë“œ)
        file_exists = os.path.exists(csv_file)
        with open(csv_file, 'a', newline='', encoding='utf-8') as f:
            fieldnames = ['title', 'abstract', 'authors', 'conference', 'year', 'field', 'url']
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            
            # í—¤ë”ê°€ ì—†ìœ¼ë©´ ì¶”ê°€
            if not file_exists:
                writer.writeheader()
            
            writer.writerow(csv_paper)
        
        # ì¤‘ë³µ ì²´í¬ìš© setì— ì¶”ê°€
        paper_key = (paper.get('title', '').strip(), conf_name.strip())
        existing_papers.add(paper_key)
        
        print(f"âœ… CSV ì €ì¥ ì™„ë£Œ: {paper.get('title', '')[:50]}...")
        return True
        
    except Exception as e:
        print(f"âŒ CSV ì €ì¥ ì‹¤íŒ¨: {e}")
        return False

def main():
    base_dir = os.path.dirname(__file__)
    csv_file = os.path.join(base_dir, "all_papers.csv")
    
    # ê¸°ì¡´ CSV íŒŒì¼ì—ì„œ ì¤‘ë³µ ì²´í¬ìš© ë°ì´í„° ë¡œë“œ
    existing_papers = load_existing_papers(csv_file)
    
    with open(os.path.join(base_dir, "conference_list.json"), "r", encoding="utf-8") as f:
        conf_data = json.load(f)

    all_results = []
    total_crawled = 0
    total_saved = 0
    total_skipped = 0

    # ì´ë¯¸ í¬ë¡¤ë§ ì™„ë£Œëœ í•™íšŒë“¤ (ì œì™¸í•  í•™íšŒ ëª©ë¡)
    completed_conferences = {
        "ACL Anthology (ACL, EMNLP, NAACL, COLING)",
        "EMNLP (Empirical Methods in NLP)",
        "NAACL (North American Chapter of ACL)",
        "CVPR (IEEE Conference on Computer Vision and Pattern Recognition)",
        "WACV (Winter Conference on Applications of Computer Vision)",
        "NeurIPS",
        "ECCV (European Conference on Computer Vision)",
        "ICML",
        # ì§€ê¸ˆ jmlr, icml, iclr í¬ë¡¤ë§ ì¤‘
    }
    
    for field in conf_data["fields"]:
        field_name = field["field"]
        print(f"\n=== {field_name} ë¶„ì•¼ í¬ë¡¤ë§ ì‹œì‘ ===")
        
        for conf in field["conferences"]:
            conf_name = conf["name"]
            conf_url = conf["site"]
            crawler_module = conf.get("crawler")
            
            # ì´ë¯¸ ì™„ë£Œëœ í•™íšŒëŠ” ê±´ë„ˆë›°ê¸°
            if conf_name in completed_conferences:
                print(f"[SKIP] {conf_name} (ì´ë¯¸ í¬ë¡¤ë§ ì™„ë£Œ)")
                continue
            
            if not crawler_module:
                print(f"[SKIP] {conf_name} (crawler ë¯¸ì§€ì •)")
                continue
                
            try:
                # ë™ì  ëª¨ë“ˆ import (ìƒëŒ€ ê²½ë¡œ ì‚¬ìš©)
                module_path = f"crawlers.{crawler_module}"
                crawler_module_obj = importlib.import_module(module_path)
                
                # í¬ë¡¤ëŸ¬ í•¨ìˆ˜ í˜¸ì¶œ (ëª¨ë“ˆì—ì„œ crawl_all_papers í•¨ìˆ˜ ì°¾ê¸°)
                if hasattr(crawler_module_obj, 'crawl_all_papers'):
                    crawl_function = getattr(crawler_module_obj, 'crawl_all_papers')
                elif hasattr(crawler_module_obj, f'{crawler_module}_crawler'):
                    crawl_function = getattr(crawler_module_obj, f'{crawler_module}_crawler')
                else:
                    print(f"[ERROR] {conf_name}: crawl_all_papers í•¨ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                    continue
                
                print(f"[INFO] {field_name} - {conf_name} í¬ë¡¤ë§ ì‹œì‘")
                
                # ì‹¤ì‹œê°„ í¬ë¡¤ë§ ë° CSV ì €ì¥
                conf_crawled = 0
                conf_saved = 0
                conf_skipped = 0
                
                # í¬ë¡¤ëŸ¬ì—ì„œ ë…¼ë¬¸ì„ í•˜ë‚˜ì”© ë°›ì•„ì„œ ì‹¤ì‹œê°„ ì²˜ë¦¬
                for paper in crawl_function(conf_url):
                    paper['field'] = field_name
                    paper['conference'] = conf_name
                    all_results.append(paper)
                    conf_crawled += 1
                    
                    # ì‹¤ì‹œê°„ CSV ì €ì¥
                    if save_paper_to_csv(paper, field_name, conf_name, csv_file, existing_papers):
                        conf_saved += 1
                        total_saved += 1
                    else:
                        conf_skipped += 1
                        total_skipped += 1
                    
                    total_crawled += 1
                
                print(f"[SUCCESS] {conf_name}: {conf_crawled}ê°œ í¬ë¡¤ë§, {conf_saved}ê°œ ì €ì¥, {conf_skipped}ê°œ íŒ¨ìŠ¤")
                
            except ImportError as e:
                print(f"[ERROR] {conf_name}: í¬ë¡¤ëŸ¬ ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ - {e}")
            except Exception as e:
                print(f"[ERROR] {conf_name} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

    # ê²°ê³¼ ì €ì¥ (ë¡œì»¬ ë°±ì—…ìš©)
    output_file = os.path.join(base_dir, "all_papers.json")
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    
    print(f"\n=== í¬ë¡¤ë§ ì™„ë£Œ ===")
    print(f"ì´ {total_crawled}ê°œ ë…¼ë¬¸ í¬ë¡¤ë§ ì™„ë£Œ!")
    print(f"ì´ {total_saved}ê°œ ë…¼ë¬¸ CSV ì €ì¥ ì™„ë£Œ!")
    print(f"ì´ {total_skipped}ê°œ ì¤‘ë³µ ë…¼ë¬¸ íŒ¨ìŠ¤!")
    print(f"CSV íŒŒì¼ ì €ì¥ ìœ„ì¹˜: {csv_file}")
    print(f"JSON ë°±ì—… ì €ì¥ ìœ„ì¹˜: {output_file}")

if __name__ == "__main__":
    print("SUPABASE_URL: ", os.getenv("SUPABASE_URL"))
    print("SUPABASE_KEY: ", os.getenv("SUPABASE_SERVICE_ROLE_KEY"))

    main()