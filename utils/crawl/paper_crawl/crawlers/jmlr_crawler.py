import requests
from bs4 import BeautifulSoup
import json
import re
import os

def get_preview_sentences(text, num_sentences=2):
    sentences = re.split(r'(?<=[.!?])\s+', text)  # ë¬¸ì¥ ë¶„ë¦¬
    return " ".join(sentences[:num_sentences])

def fetch_paper_titles_and_links(url: str):
    response = requests.get(url)
    response.raise_for_status()

    soup = BeautifulSoup(response.text, 'html.parser')
    
    papers = []
    
    # dl íƒœê·¸ ë‚´ì˜ dt íƒœê·¸ë“¤ì—ì„œ ì œëª© ì°¾ê¸° (ì´ë¯¸ì§€ì—ì„œ í™•ì¸ëœ êµ¬ì¡°)
    dl_tags = soup.find_all("dl")
    for dl in dl_tags:
        dt_tags = dl.find_all("dt")
        for dt in dt_tags:
            title = dt.get_text(strip=True)
            if title and len(title) > 10:  # ì˜ë¯¸ìˆëŠ” ì œëª©ì¸ì§€ í™•ì¸
                # í•´ë‹¹ dtì˜ ë‹¤ìŒ dd íƒœê·¸ì—ì„œ ë§í¬ ì°¾ê¸°
                dd = dt.find_next_sibling("dd")
                if dd:
                    # dd ë‚´ì˜ ë§í¬ë“¤ ì°¾ê¸°
                    links = dd.find_all("a", href=True)
                    abs_url = None
                    pdf_url = None
                    
                    for link in links:
                        href = link.get('href')
                        link_text = link.get_text(strip=True).lower()
                        
                        # abs ë§í¬ ì°¾ê¸° (ìƒì„¸ í˜ì´ì§€ URL)
                        if link_text == "abs" and href:
                            if href.startswith("http"):
                                abs_url = href
                            else:
                                abs_url = "https://jmlr.org" + href
                        
                        # pdf ë§í¬ ì°¾ê¸° (ë…¼ë¬¸ URL)
                        elif link_text == "pdf" and href:
                            if href.startswith("http"):
                                pdf_url = href
                            else:
                                pdf_url = "https://jmlr.org" + href
                    
                    # abs URLì´ ìˆìœ¼ë©´ ì €ì¥ (ì €ì, ì œëª©, ì´ˆë¡ì„ ê°€ì ¸ì˜¬ URL)
                    if abs_url:
                        papers.append({
                            "title": title, 
                            "abs_url": abs_url,  # ìƒì„¸ í˜ì´ì§€ URL
                            "pdf_url": pdf_url   # ë…¼ë¬¸ PDF URL
                        })
    
    return papers

def get_detailed_title(abs_url: str):
    """
    ìƒì„¸ í˜ì´ì§€ì—ì„œ ì •í™•í•œ ì œëª©ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
    """
    try:
        response = requests.get(abs_url)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # id="content" ë‚´ì˜ h2 íƒœê·¸ì—ì„œ ì œëª© ì°¾ê¸°
        content_div = soup.find("div", id="content")
        if content_div:
            h2_tag = content_div.find("h2")
            if h2_tag:
                title = h2_tag.get_text(strip=True)
                return title
        
        return "Title not found"
    except Exception as e:
        return "Title not found"

def fetch_abstract_and_authors(abs_url: str):
    response = requests.get(abs_url)
    response.raise_for_status()
    soup = BeautifulSoup(response.text, 'html.parser')

    # Abstract ì¶”ì¶œ: class="abstract"ì¸ p íƒœê·¸ì—ì„œ ì°¾ê¸°
    abstract = "Abstract not found"
    abstract_elem = soup.find("p", class_="abstract")
    if abstract_elem:
        abstract = abstract_elem.get_text(strip=True)
    
    # ë°±ì—… ë°©ë²•: ë‹¤ë¥¸ ì´ˆë¡ ê´€ë ¨ ìš”ì†Œë“¤ ì°¾ê¸°
    if abstract == "Abstract not found":
        abstract_selectors = [
            "div.abstract",
            "div#abstract", 
            "p.abstract",
            "div.card-body",
            "div.lead"
        ]
        
        for selector in abstract_selectors:
            abstract_elem = soup.select_one(selector)
            if abstract_elem:
                text = abstract_elem.get_text(strip=True)
                if len(text) > 50:  # ì˜ë¯¸ìˆëŠ” ê¸¸ì´ì¸ì§€ í™•ì¸
                    abstract = text
                    break

    # Authors ì¶”ì¶œ: id="content" ë‚´ì˜ p>b>i êµ¬ì¡°ì—ì„œ ì°¾ê¸°
    authors = "Authors not found"
    content_div = soup.find("div", id="content")
    if content_div:
        # p íƒœê·¸ ë‚´ì˜ i íƒœê·¸ì—ì„œ ì €ì ì°¾ê¸°
        p_tags = content_div.find_all("p")
        for p in p_tags:
            i_tag = p.find("i")
            if i_tag:
                text = i_tag.get_text(strip=True)
                if len(text) > 5 and "," in text:  # ì €ì êµ¬ë¶„ìê°€ ìˆëŠ”ì§€ í™•ì¸
                    authors = text
                    break
    
    # ë°±ì—… ë°©ë²•: ë‹¤ë¥¸ ì €ì ê´€ë ¨ ìš”ì†Œë“¤ ì°¾ê¸°
    if authors == "Authors not found":
        author_selectors = [
            "p.lead",
            "div.authors",
            "span.authors",
            "div.author",
            "p.author"
        ]
        
        for selector in author_selectors:
            author_elem = soup.select_one(selector)
            if author_elem:
                text = author_elem.get_text(strip=True)
                if len(text) > 5 and any(char in text for char in [",", "and", "&"]):
                    authors = text
                    break

    return abstract, authors

def crawl_all_papers(url: str):
    """
    ì£¼ì–´ì§„ URLì—ì„œ ëª¨ë“  ë…¼ë¬¸ì„ í¬ë¡¤ë§í•˜ì—¬ í•˜ë‚˜ì”© ì‹¤ì‹œê°„ìœ¼ë¡œ ë°˜í™˜
    """
    papers = fetch_paper_titles_and_links(url)

    for i, paper in enumerate(papers, 1):
        try:
            # ì œëª©ì´ ë¹„ì–´ìˆëŠ” ê²½ìš° ê±´ë„ˆëœ€
            if not paper['title'].strip():
                continue

            # abs_urlì—ì„œ ì´ˆë¡ê³¼ ì €ì ê°€ì ¸ì˜¤ê¸°
            abstract, authors = fetch_abstract_and_authors(paper['abs_url'])
            
            # ìƒì„¸ í˜ì´ì§€ì—ì„œ ì •í™•í•œ ì œëª© ê°€ì ¸ì˜¤ê¸° (id="content" ë‚´ì˜ h2 íƒœê·¸)
            detailed_title = get_detailed_title(paper['abs_url'])
            if detailed_title and detailed_title != "Title not found":
                paper['title'] = detailed_title
            
            paper_data = {
                'title': paper['title'],
                'abstract': abstract,
                'authors': authors,
                'url': paper['pdf_url'],  # PDF URLì„ ë…¼ë¬¸ URLë¡œ ì €ì¥
                'abs_url': paper['abs_url'],  # ìƒì„¸ í˜ì´ì§€ URLë„ ì €ì¥
                'year': 2024  # ê¸°ë³¸ê°’
            }
            
            yield paper_data  # ì‹¤ì‹œê°„ìœ¼ë¡œ í•˜ë‚˜ì”© ë°˜í™˜

        except Exception as e:
            print(f"âŒ ë…¼ë¬¸ {i} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

    pass

def jmlr_crawler():
    """
    ê¸°ì¡´ í•¨ìˆ˜ - conference_list.jsonì—ì„œ JMLR ê´€ë ¨ ì»¨í¼ëŸ°ìŠ¤ë“¤ì„ ëª¨ë‘ í¬ë¡¤ë§
    """
    # conference_list.json íŒŒì¼ ê²½ë¡œ
    base_dir = os.path.dirname(os.path.dirname(__file__))
    config_path = os.path.join(base_dir, "conference_list.json")
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = json.load(f)

    conference_urls = {}
    for field in config['fields']:
        for conf in field['conferences']:
            if conf.get('crawler') == 'jmlr_crawler':
                conference_urls[conf['name']] = conf['site']

    all_results = []

    for conf_name, url in conference_urls.items():
        print(f"ğŸ” [{conf_name}] í¬ë¡¤ë§ ì¤‘...")
        for paper in crawl_all_papers(url):
            paper['conference'] = conf_name
            all_results.append(paper)

    # ì „ì²´ ìµœì¢… ì €ì¥
    output_path = os.path.join(base_dir, "jmlr_papers_2024.json")
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    
    print(f"ì´ {len(all_results)}ê°œ ë…¼ë¬¸ì´ {output_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return all_results

if __name__ == "__main__":
    jmlr_crawler()
