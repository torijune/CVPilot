import requests
from bs4 import BeautifulSoup
import json
import re
import os
import time

def get_preview_sentences(text, num_sentences=2):
    sentences = re.split(r'(?<=[.!?])\s+', text)  # ë¬¸ì¥ ë¶„ë¦¬
    return " ".join(sentences[:num_sentences])

def fetch_paper_titles_and_links(url: str):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    response = requests.get(url, headers=headers)
    response.raise_for_status()

    soup = BeautifulSoup(response.text, 'html.parser')
    
    # ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ ë…¼ë¬¸ ì°¾ê¸° ì‹œë„
    papers = []
    
    # ë°©ë²• 1: class="text-muted"ì¸ a íƒœê·¸ë“¤ì—ì„œ ë…¼ë¬¸ URL ì°¾ê¸°
    paper_links = soup.find_all("a", class_="text-muted")
    
    for link in paper_links:
        href = link.get('href')
        if href:
            # ë…¼ë¬¸ ì œëª© ì°¾ê¸° (ê°™ì€ ì»¨í…Œì´ë„ˆ ë‚´ì˜ card-title)
            title_element = link.find_next("div", class_="card-title")
            if title_element:
                title = title_element.get_text(strip=True)
                
                # ì œëª©ì´ ë¹„ì–´ ìˆê±°ë‚˜ íŠ¹ì • í‚¤ì›Œë“œë©´ ê±´ë„ˆëœ€
                if not title or title.lower() in {"pdf", "bib", "abs", "download"}:
                    continue
                
                # URL êµ¬ì„±
                if href.startswith("http"):
                    paper_url = href
                else:
                    # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                    if href.startswith('/'):
                        paper_url = "https://icml.cc" + href
                    else:
                        # ì´ë¯¸ virtual/2025/poster/ë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
                        if href.startswith('virtual/2025/poster/'):
                            paper_url = "https://icml.cc/" + href
                        else:
                            paper_url = "https://icml.cc/" + href
                
                papers.append({"title": title, "url": paper_url})
    
    # ë°©ë²• 2: ëª¨ë“  a íƒœê·¸ì—ì„œ ë…¼ë¬¸ ë§í¬ ì°¾ê¸°
    if not papers:
        all_links = soup.find_all("a", href=True)
        
        for link in all_links:
            href = link.get('href', '')
            link_text = link.get_text(strip=True)
            
            # ë…¼ë¬¸ ê´€ë ¨ ë§í¬ì¸ì§€ í™•ì¸ (poster/ë¡œ ì‹œì‘í•˜ëŠ” URLë§Œ)
            if (href and len(link_text) > 10 and 
                'poster/' in href and  # poster/ê°€ í¬í•¨ëœ URLë§Œ
                not any(keyword in link_text.lower() for keyword in ["pdf", "bib", "abs", "download", "home", "schedule", "tutorials", "skip", "select", "main", "help", "contact", "privacy", "code", "diversity", "future", "archives"])):
                
                # URL êµ¬ì„±
                if href.startswith("http"):
                    paper_url = href
                else:
                    # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                    if href.startswith('/'):
                        paper_url = "https://icml.cc" + href
                    else:
                        # ì´ë¯¸ virtual/2025/poster/ë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
                        if href.startswith('virtual/2025/poster/'):
                            paper_url = "https://icml.cc/" + href
                        else:
                            paper_url = "https://icml.cc/" + href
                
                # ì œëª© ì°¾ê¸° (ë§í¬ í…ìŠ¤íŠ¸ ë˜ëŠ” ì£¼ë³€ ìš”ì†Œì—ì„œ)
                title = link_text
                if len(title) < 5:  # ë§í¬ í…ìŠ¤íŠ¸ê°€ ì§§ìœ¼ë©´ ì£¼ë³€ì—ì„œ ì°¾ê¸°
                    parent = link.parent
                    if parent:
                        title_elem = parent.find("div", class_="card-title") or parent.find("h3") or parent.find("h4")
                        if title_elem:
                            title = title_elem.get_text(strip=True)
                
                if title and len(title) > 5:
                    papers.append({"title": title, "url": paper_url})
    
    # ë°©ë²• 3: poster/ ë§í¬ë§Œ ì§ì ‘ ì°¾ê¸°
    if not papers:
        poster_links = soup.find_all("a", href=re.compile(r'poster/\d+'))
        
        for link in poster_links:
            href = link.get('href', '')
            link_text = link.get_text(strip=True)
            
            if href and len(link_text) > 10:
                # URL êµ¬ì„±
                if href.startswith("http"):
                    paper_url = href
                else:
                    # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                    if href.startswith('/'):
                        paper_url = "https://icml.cc" + href
                    else:
                        # ì´ë¯¸ virtual/2025/poster/ë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
                        if href.startswith('virtual/2025/poster/'):
                            paper_url = "https://icml.cc/" + href
                        else:
                            paper_url = "https://icml.cc/" + href
                
                # ì œëª© ì°¾ê¸°
                title = link_text
                if len(title) < 5:
                    parent = link.parent
                    if parent:
                        title_elem = parent.find("div", class_="card-title") or parent.find("h3") or parent.find("h4")
                        if title_elem:
                            title = title_elem.get_text(strip=True)
                
                if title and len(title) > 5:
                    papers.append({"title": title, "url": paper_url})
    
    # ë°©ë²• 4: card-title í´ë˜ìŠ¤ë¥¼ ê°€ì§„ ìš”ì†Œë“¤ ì°¾ê¸°
    if not papers:
        title_elements = soup.find_all("div", class_="card-title")
        
        for title_elem in title_elements:
            title = title_elem.get_text(strip=True)
            if title and len(title) > 10:
                # í•´ë‹¹ ìš”ì†Œ ê·¼ì²˜ì˜ ë§í¬ ì°¾ê¸°
                parent = title_elem.parent
                if parent:
                    link_elem = parent.find("a", href=True)
                    if link_elem:
                        href = link_elem.get('href')
                        if href.startswith("http"):
                            paper_url = href
                        else:
                            # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                            if href.startswith('/'):
                                paper_url = "https://icml.cc" + href
                            else:
                                # ì´ë¯¸ virtual/2025/poster/ë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
                                if href.startswith('virtual/2025/poster/'):
                                    paper_url = "https://icml.cc/" + href
                                else:
                                    paper_url = "https://icml.cc/" + href
                        
                        papers.append({"title": title, "url": paper_url})
    
    # ë°©ë²• 5: JavaScript ë°ì´í„°ì—ì„œ ë…¼ë¬¸ ì •ë³´ ì°¾ê¸°
    if not papers:
        scripts = soup.find_all("script")
        for script in scripts:
            if script.string and "paper" in script.string.lower():
                # ë…¼ë¬¸ ê´€ë ¨ ë°ì´í„° íŒ¨í„´ ì°¾ê¸°
                paper_matches = re.findall(r'"title":\s*"([^"]+)"[^}]*"url":\s*"([^"]+)"', script.string)
                for title, paper_url in paper_matches:
                    if title and len(title) > 10:
                        papers.append({"title": title, "url": paper_url})
    
    return papers

def fetch_abstract_and_authors(paper_url: str):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    time.sleep(1) # ì ì‹œ ëŒ€ê¸° (ìš”ì²­ ì œí•œ ë°©ì§€)
    
    try:
        response = requests.get(paper_url, headers=headers)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Abstract ì¶”ì¶œ: ë™ì ìœ¼ë¡œ í‘œì‹œë˜ëŠ” ì´ˆë¡ ì²˜ë¦¬
        abstract = "Abstract not found"
        
        # ë°©ë²• 1: ì´ë¯¸ í‘œì‹œëœ ì´ˆë¡ ì°¾ê¸° (collapse.show í´ë˜ìŠ¤ê°€ ìˆëŠ” ê²½ìš°)
        abstract_div = soup.find("div", class_="collapse show")
        if abstract_div:
            abstract_text = abstract_div.find("span", class_="font-weight-bold")
            if abstract_text:
                abstract = abstract_text.get_text(strip=True)
                print(f"ì´ˆë¡ ì°¾ìŒ (ì´ë¯¸ í‘œì‹œë¨): {len(abstract)}ì")
        
        # ë°©ë²• 2: ìˆ¨ê²¨ì§„ ì´ˆë¡ ì°¾ê¸° (collapse í´ë˜ìŠ¤ë§Œ ìˆëŠ” ê²½ìš°)
        if abstract == "Abstract not found":
            abstract_div = soup.find("div", class_="collapse")
            if abstract_div:
                abstract_text = abstract_div.find("span", class_="font-weight-bold")
                if abstract_text:
                    abstract = abstract_text.get_text(strip=True)
                    print(f"ì´ˆë¡ ì°¾ìŒ (ìˆ¨ê²¨ì§„ ìƒíƒœ): {len(abstract)}ì")
        
        # ë°©ë²• 3: ë‹¤ë¥¸ ì´ˆë¡ ìš”ì†Œë“¤ ì°¾ê¸°
        if abstract == "Abstract not found":
            # ë‹¤ì–‘í•œ ì´ˆë¡ ê´€ë ¨ í´ë˜ìŠ¤ë“¤ ì‹œë„
            abstract_selectors = [
                "div.font-weight-bold",
                "span.font-weight-bold", 
                "div#abstractExample",
                "div.abstract",
                "p.abstract"
            ]
            
            for selector in abstract_selectors:
                abstract_elem = soup.select_one(selector)
                if abstract_elem:
                    text = abstract_elem.get_text(strip=True)
                    if len(text) > 50:  # ì˜ë¯¸ìˆëŠ” ê¸¸ì´ì¸ì§€ í™•ì¸
                        abstract = text
                        print(f"ì´ˆë¡ ì°¾ìŒ ({selector}): {len(abstract)}ì")
                        break
        
        # Authors ì¶”ì¶œ: h3 íƒœê·¸ì—ì„œ ì°¾ê¸° (ì´ë¯¸ì§€ì—ì„œ í™•ì¸ëœ êµ¬ì¡°)
        authors = "Authors not found"
        authors_h3 = soup.find("h3", class_="card-subtitle mb-2 text-muted text-center")
        if authors_h3:
            authors = authors_h3.get_text(strip=True)
            print(f"ì €ì ì°¾ìŒ: {authors}")
        
        # ë°©ë²• 2: ë‹¤ë¥¸ ì €ì ê´€ë ¨ ìš”ì†Œë“¤ ì°¾ê¸°
        if authors == "Authors not found":
            authors_selectors = [
                "div.card-subtitle.mb-2.text-muted.text-center",
                "h3.card-subtitle",
                "div.authors",
                "span.authors"
            ]
            
            for selector in authors_selectors:
                authors_elem = soup.select_one(selector)
                if authors_elem:
                    text = authors_elem.get_text(strip=True)
                    if "Â·" in text:  # ì €ì êµ¬ë¶„ìê°€ ìˆëŠ”ì§€ í™•ì¸
                        authors = text
                        print(f"ì €ì ì°¾ìŒ ({selector}): {authors}")
                        break
        
        return abstract, authors
        
    except requests.exceptions.RequestException as e:
        print(f"ìš”ì²­ ì˜¤ë¥˜: {e}")
        return "Abstract not found", "Authors not found"

def crawl_all_papers(url: str):
    """
    ì£¼ì–´ì§„ URLì—ì„œ ëª¨ë“  ë…¼ë¬¸ì„ í¬ë¡¤ë§í•˜ì—¬ í•˜ë‚˜ì”© ì‹¤ì‹œê°„ìœ¼ë¡œ ë°˜í™˜
    """
    papers = fetch_paper_titles_and_links(url)

    for i, paper in enumerate(papers, 1):
        try:
            # ì œëª©ì´ ë¹„ì–´ìˆëŠ” ê²½ìš° ê±´ë„ˆëœ€
            if not paper['title'].strip():
                continue

            abstract, authors = fetch_abstract_and_authors(paper['url'])
            
            paper_data = {
                'title': paper['title'],
                'abstract': abstract,
                'authors': authors,
                'url': paper['url'],
                'year': 2025  # ICML 2025
            }
            print("Title: ", paper['title'])
            print("Abstract: ", abstract)
            print("Authors: ", authors)
            print("URL: ", paper['url'])
            print("Year: ", 2025)
            print("-" * 100)
            
            yield paper_data  # ì‹¤ì‹œê°„ìœ¼ë¡œ í•˜ë‚˜ì”© ë°˜í™˜

        except Exception as e:
            print(f"âŒ ë…¼ë¬¸ {i} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

    print(f"ì´ {len(papers)}ê°œ ë…¼ë¬¸ í¬ë¡¤ë§ ì™„ë£Œ")

def icml_crawler():
    """
    ê¸°ì¡´ í•¨ìˆ˜ - conference_list.jsonì—ì„œ ICML ê´€ë ¨ ì»¨í¼ëŸ°ìŠ¤ë“¤ì„ ëª¨ë‘ í¬ë¡¤ë§
    """
    # conference_list.json íŒŒì¼ ê²½ë¡œ
    base_dir = os.path.dirname(os.path.dirname(__file__))
    config_path = os.path.join(base_dir, "conference_list.json")
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = json.load(f)

    conference_urls = {}
    for field in config['fields']:
        for conf in field['conferences']:
            if conf.get('crawler') == 'icml_crawler':
                conference_urls[conf['name']] = conf['site']

    all_results = []

    for conf_name, url in conference_urls.items():
        print(f"ğŸ” [{conf_name}] í¬ë¡¤ë§ ì¤‘...")
        for paper in crawl_all_papers(url):
            paper['conference'] = conf_name
            all_results.append(paper)

    # ì „ì²´ ìµœì¢… ì €ì¥
    output_path = os.path.join(base_dir, "icml_papers_2025.json")
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    
    print(f"ì´ {len(all_results)}ê°œ ë…¼ë¬¸ì´ {output_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return all_results

if __name__ == "__main__":
    icml_crawler()
