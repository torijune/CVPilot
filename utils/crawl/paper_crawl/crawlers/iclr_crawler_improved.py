import requests
from bs4 import BeautifulSoup
import json
import re
import os
import time
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def setup_selenium_driver():
    """Selenium WebDriver ì„¤ì •"""
    try:
        chrome_options = Options()
        chrome_options.add_argument("--headless")  # ë¸Œë¼ìš°ì € ì°½ ì•ˆ ë„ìš°ê¸°
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--window-size=1920,1080")
        chrome_options.add_argument("--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")
        
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
        
        logger.info("âœ… Selenium WebDriver ì„¤ì • ì™„ë£Œ")
        return driver
    except Exception as e:
        logger.error(f"âŒ Selenium WebDriver ì„¤ì • ì‹¤íŒ¨: {e}")
        return None

def get_iclr_papers_with_selenium(url: str):
    """Seleniumì„ ì‚¬ìš©í•˜ì—¬ ICLR ë…¼ë¬¸ ëª©ë¡ ìˆ˜ì§‘"""
    driver = setup_selenium_driver()
    if not driver:
        return []
    
    try:
        logger.info(f"ğŸ” ICLR í˜ì´ì§€ ë¡œë”© ì¤‘: {url}")
        driver.get(url)
        
        # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
        WebDriverWait(driver, 30).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )
        
        # ì¶”ê°€ ë¡œë”© ì‹œê°„
        time.sleep(5)
        
        # ìŠ¤í¬ë¡¤ì„ í†µí•œ ë™ì  ë¡œë”© íŠ¸ë¦¬ê±°
        logger.info("ğŸ“œ í˜ì´ì§€ ìŠ¤í¬ë¡¤ì„ í†µí•œ ë™ì  ì½˜í…ì¸  ë¡œë”©...")
        for i in range(5):
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
        
        # í˜ì´ì§€ ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸°
        page_source = driver.page_source
        logger.info(f"ğŸ“„ í˜ì´ì§€ ì†ŒìŠ¤ ê¸¸ì´: {len(page_source)}")
        
        # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
        soup = BeautifulSoup(page_source, 'html.parser')
        
        # ë…¼ë¬¸ ë§í¬ ì°¾ê¸° - ë‹¤ì–‘í•œ ë°©ë²• ì‹œë„
        papers = []
        
        # ë°©ë²• 1: forum ë§í¬ ì§ì ‘ ì°¾ê¸°
        forum_links = soup.find_all('a', href=re.compile(r'/forum\?id='))
        logger.info(f"ë°©ë²• 1: {len(forum_links)}ê°œ forum ë§í¬ ë°œê²¬")
        
        for link in forum_links:
            href = link.get('href')
            if href and '/forum?id=' in href:
                paper_id = href.split('id=')[1].split('&')[0]
                title = link.get_text(strip=True)
                
                if title and len(title) > 10 and 'forum' not in title.lower():
                    paper_url = f"https://openreview.net{href}" if href.startswith('/') else href
                    papers.append({
                        'title': title,
                        'url': paper_url,
                        'id': paper_id
                    })
                    logger.info(f"ë…¼ë¬¸ ë°œê²¬: {title[:50]}...")
        
        # ë°©ë²• 2: JavaScript ë‚´ìš©ì—ì„œ ë…¼ë¬¸ ID ì¶”ì¶œ
        scripts = soup.find_all('script')
        logger.info(f"ë°©ë²• 2: {len(scripts)}ê°œ ìŠ¤í¬ë¦½íŠ¸ íƒœê·¸ ê²€ì‚¬")
        
        for script in scripts:
            if script.string:
                # ë‹¤ì–‘í•œ íŒ¨í„´ìœ¼ë¡œ ë…¼ë¬¸ ID ì°¾ê¸°
                id_patterns = [
                    r'"id":\s*"([a-zA-Z0-9_-]{10,})"',
                    r'/forum\?id=([a-zA-Z0-9_-]{10,})',
                    r'"forum_id":\s*"([a-zA-Z0-9_-]{10,})"'
                ]
                
                for pattern in id_patterns:
                    matches = re.findall(pattern, script.string)
                    for match in matches:
                        if len(match) > 10 and match not in [p['id'] for p in papers]:
                            # ì œëª© ì°¾ê¸° ì‹œë„
                            title_patterns = [
                                rf'"id":\s*"{match}"[^}}]*"title":\s*"([^"]+)"',
                                rf'"{match}"[^}}]*"title":\s*"([^"]+)"'
                            ]
                            
                            title = None
                            for title_pattern in title_patterns:
                                title_match = re.search(title_pattern, script.string)
                                if title_match:
                                    title = title_match.group(1)
                                    break
                            
                            if title and len(title) > 10:
                                paper_url = f"https://openreview.net/forum?id={match}"
                                papers.append({
                                    'title': title,
                                    'url': paper_url,
                                    'id': match
                                })
                                logger.info(f"ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ë…¼ë¬¸ ë°œê²¬: {title[:50]}...")
        
        # ë°©ë²• 3: note í´ë˜ìŠ¤ ì°¾ê¸°
        note_divs = soup.find_all('div', class_=re.compile(r'note'))
        logger.info(f"ë°©ë²• 3: {len(note_divs)}ê°œ note div ê²€ì‚¬")
        
        for note in note_divs:
            # ì œëª© ì°¾ê¸°
            title_elem = note.find(['h1', 'h2', 'h3', 'h4', 'a'])
            if title_elem:
                title = title_elem.get_text(strip=True)
                if len(title) > 10 and title not in [p['title'] for p in papers]:
                    # URL ì°¾ê¸°
                    link_elem = note.find('a', href=re.compile(r'/forum\?id='))
                    if link_elem:
                        href = link_elem.get('href')
                        paper_id = href.split('id=')[1].split('&')[0]
                        paper_url = f"https://openreview.net{href}" if href.startswith('/') else href
                        papers.append({
                            'title': title,
                            'url': paper_url,
                            'id': paper_id
                        })
                        logger.info(f"noteì—ì„œ ë…¼ë¬¸ ë°œê²¬: {title[:50]}...")
        
        # ì¤‘ë³µ ì œê±°
        unique_papers = []
        seen_ids = set()
        for paper in papers:
            if paper['id'] not in seen_ids and len(paper['title']) > 10:
                unique_papers.append(paper)
                seen_ids.add(paper['id'])
        
        logger.info(f"âœ… ì´ {len(unique_papers)}ê°œì˜ ê³ ìœ  ë…¼ë¬¸ ë°œê²¬")
        return unique_papers
        
    except Exception as e:
        logger.error(f"âŒ Selenium í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return []
    finally:
        driver.quit()

def fetch_paper_details_with_requests(paper_url: str):
    """requestsë¥¼ ì‚¬ìš©í•˜ì—¬ ë…¼ë¬¸ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    try:
        time.sleep(1)  # ìš”ì²­ ì œí•œ ë°©ì§€
        response = requests.get(paper_url, headers=headers)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')

        # Abstract ì¶”ì¶œ
        abstract = "Abstract not found"
        abstract_selectors = [
            "div.note-content-value.markdown-rendered",
            "div.note-content",
            "div[data-testid='abstract']",
            "div.abstract"
        ]
        
        for selector in abstract_selectors:
            abstract_elem = soup.select_one(selector)
            if abstract_elem:
                abstract = abstract_elem.get_text(strip=True)
                if len(abstract) > 50:
                    logger.info(f"ì´ˆë¡ ë°œê²¬: {len(abstract)}ì")
                    break

        # Authors ì¶”ì¶œ
        authors = "Authors not found"
        
        # ë°©ë²• 1: forum-authors
        authors_div = soup.find("div", class_="forum-authors")
        if authors_div:
            author_links = authors_div.find_all("a")
            if author_links:
                author_names = [link.get_text(strip=True) for link in author_links if link.get_text(strip=True)]
                if author_names:
                    authors = ", ".join(author_names)
                    logger.info(f"ì €ì ë°œê²¬: {len(author_names)}ëª…")

        # ë°©ë²• 2: profile ë§í¬ ì°¾ê¸°
        if authors == "Authors not found":
            profile_links = soup.find_all("a", href=re.compile(r'/profile\?id='))
            if profile_links:
                author_names = [link.get_text(strip=True) for link in profile_links[:10] if link.get_text(strip=True)]
                if author_names:
                    authors = ", ".join(author_names)
                    logger.info(f"í”„ë¡œí•„ì—ì„œ ì €ì ë°œê²¬: {len(author_names)}ëª…")

        return abstract, authors
        
    except Exception as e:
        logger.error(f"âŒ ë…¼ë¬¸ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return "Abstract not found", "Authors not found"

def crawl_all_papers(url: str):
    """ê°œì„ ëœ ICLR í¬ë¡¤ëŸ¬ ë©”ì¸ í•¨ìˆ˜"""
    logger.info(f"ğŸ” ê°œì„ ëœ ICLR í¬ë¡¤ë§ ì‹œì‘: {url}")
    
    # 1. Seleniumìœ¼ë¡œ ë…¼ë¬¸ ëª©ë¡ ìˆ˜ì§‘
    papers = get_iclr_papers_with_selenium(url)
    
    if not papers:
        logger.warning("âš ï¸ ë…¼ë¬¸ ëª©ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ í¬ë¡¤ëŸ¬ë¡œ ì‹œë„...")
        # ê¸°ë³¸ í¬ë¡¤ëŸ¬ë¡œ í´ë°±
        from . import iclr_crawler
        papers = iclr_crawler.fetch_paper_titles_and_links(url)
    
    logger.info(f"ğŸ“Š ìˆ˜ì§‘ëœ ë…¼ë¬¸ ìˆ˜: {len(papers)}")
    
    # 2. ê° ë…¼ë¬¸ì˜ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
    for i, paper in enumerate(papers, 1):
        try:
            logger.info(f"ğŸ“„ ë…¼ë¬¸ {i}/{len(papers)} ì²˜ë¦¬ ì¤‘: {paper['title'][:50]}...")
            
            abstract, authors = fetch_paper_details_with_requests(paper['url'])
            
            paper_data = {
                'title': paper['title'],
                'abstract': abstract,
                'authors': authors,
                'url': paper['url'],
                'year': 2025,
                'conference': 'ICLR',
                'field': 'Machine Learning / Deep Learning (ML/DL)'
            }
            
            logger.info(f"âœ… ë…¼ë¬¸ {i} ì™„ë£Œ: {paper['title'][:30]}...")
            yield paper_data
            
        except Exception as e:
            logger.error(f"âŒ ë…¼ë¬¸ {i} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            continue
    
    logger.info(f"ğŸ‰ ì´ {len(papers)}ê°œ ë…¼ë¬¸ í¬ë¡¤ë§ ì™„ë£Œ")

def iclr_crawler_improved():
    """ê°œì„ ëœ ICLR í¬ë¡¤ëŸ¬ ì§„ì…ì """
    url = "https://openreview.net/group?id=ICLR.cc/2025/Conference#tab-accept-oral"
    
    all_results = []
    for paper in crawl_all_papers(url):
        all_results.append(paper)
    
    # ê²°ê³¼ ì €ì¥
    base_dir = os.path.dirname(os.path.dirname(__file__))
    output_path = os.path.join(base_dir, "iclr_papers_2025_improved.json")
    
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    
    logger.info(f"ğŸ’¾ {len(all_results)}ê°œ ë…¼ë¬¸ì´ {output_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return all_results

if __name__ == "__main__":
    iclr_crawler_improved() 