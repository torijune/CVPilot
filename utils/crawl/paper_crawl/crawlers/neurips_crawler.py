import requests
from bs4 import BeautifulSoup
import json
import re
import os

def get_preview_sentences(text, num_sentences=2):
    sentences = re.split(r'(?<=[.!?])\s+', text)  # ë¬¸ìž¥ ë¶„ë¦¬
    return " ".join(sentences[:num_sentences])

def fetch_paper_titles_and_links(url: str):
    response = requests.get(url)
    response.raise_for_status()

    soup = BeautifulSoup(response.text, 'html.parser')
    # NeurIPS ì‚¬ì´íŠ¸ì— ë§žëŠ” ì„ íƒìž: li class="conference" ì•ˆì˜ a íƒœê·¸
    paper_tags = soup.find_all("li", class_="conference")

    papers = []
    for tag in paper_tags:
        # li íƒœê·¸ ì•ˆì˜ a íƒœê·¸ ì°¾ê¸°
        link_tag = tag.find("a", title="paper title")
        if not link_tag:
            continue
            
        title = link_tag.text.strip()
        # ì œëª©ì´ ë¹„ì–´ ìžˆê±°ë‚˜ íŠ¹ì • í‚¤ì›Œë“œë©´ ê±´ë„ˆëœ€
        if not title or title.lower() in {"pdf", "bib", "abs"}:
            continue

        href = link_tag.get('href')
        if not href:
            continue
            
        # NeurIPS ì‚¬ì´íŠ¸ì˜ ì˜¬ë°”ë¥¸ URL êµ¬ì¡°ë¡œ ìˆ˜ì •
        if href.startswith("http"):
            link = href
        else:
            # ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° papers.nips.cc ë„ë©”ì¸ ì‚¬ìš©
            link = "https://papers.nips.cc" + href

        papers.append({"title": title, "url": link})
    return papers

def fetch_abstract_and_authors(paper_url: str):
    response = requests.get(paper_url)
    response.raise_for_status()
    soup = BeautifulSoup(response.text, 'html.parser')

    # NeurIPS ì‚¬ì´íŠ¸ì˜ ì •í™•í•œ êµ¬ì¡°ì— ë§žê²Œ ìˆ˜ì •
    # Authors - class="container-fluid" í•˜ìœ„ì˜ h4 "Authors" ë‹¤ìŒì— ì˜¤ëŠ” ë‚´ìš©
    authors = "Authors not found"
    authors_h4 = soup.find("h4", string="Authors")
    if authors_h4:
        # h4 ë‹¤ìŒì— ì˜¤ëŠ” p íƒœê·¸ì—ì„œ ì €ìž ì •ë³´ ì¶”ì¶œ
        authors_p = authors_h4.find_next_sibling("p")
        if authors_p:
            # p íƒœê·¸ ì•ˆì˜ i íƒœê·¸ì—ì„œ ì €ìžëª… ì¶”ì¶œ
            author_i_tag = authors_p.find("i")
            if author_i_tag:
                authors = author_i_tag.get_text(strip=True)
            else:
                # i íƒœê·¸ê°€ ì—†ìœ¼ë©´ p íƒœê·¸ ì „ì²´ í…ìŠ¤íŠ¸ ì‚¬ìš©
                authors = authors_p.get_text(strip=True)

    # Abstract - class="container-fluid" í•˜ìœ„ì˜ h4 "Abstract" ë‹¤ìŒì— ì˜¤ëŠ” ë‚´ìš©
    abstract = "Abstract not found"
    abstract_h4 = soup.find("h4", string="Abstract")
    if abstract_h4:
        # h4 ë‹¤ìŒì— ì˜¤ëŠ” p íƒœê·¸ë“¤ì—ì„œ ì´ˆë¡ ì¶”ì¶œ
        abstract_p = abstract_h4.find_next_sibling("p")
        if abstract_p:
            abstract = abstract_p.get_text(strip=True)

    return abstract, authors

def crawl_all_papers(url: str):
    """
    ì£¼ì–´ì§„ URLì—ì„œ ëª¨ë“  ë…¼ë¬¸ì„ í¬ë¡¤ë§í•˜ì—¬ í•˜ë‚˜ì”© ì‹¤ì‹œê°„ìœ¼ë¡œ ë°˜í™˜
    """
    print(f"ðŸ” NeurIPS í¬ë¡¤ë§ ì‹œìž‘: {url}")
    papers = fetch_paper_titles_and_links(url)

    for i, paper in enumerate(papers, 1):
        try:
            # ì œëª©ì´ ë¹„ì–´ìžˆëŠ” ê²½ìš° ê±´ë„ˆëœ€
            if not paper['title'].strip():
                continue

            abstract, authors = fetch_abstract_and_authors(paper['url'])
            
            paper_data = {
                'title': paper['title'],
                'abstract': abstract,
                'authors': authors,
                'url': paper['url'],
                'year': 2024  # ê¸°ë³¸ê°’
            }
            
            print(f"âœ… ë…¼ë¬¸ {i}: {paper['title'][:50]}...")
            yield paper_data  # ì‹¤ì‹œê°„ìœ¼ë¡œ í•˜ë‚˜ì”© ë°˜í™˜

        except Exception as e:
            print(f"âŒ ë…¼ë¬¸ {i} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

    print(f"ì´ {len(papers)}ê°œ ë…¼ë¬¸ í¬ë¡¤ë§ ì™„ë£Œ")

def neurips_crawler():
    """
    ê¸°ì¡´ í•¨ìˆ˜ - conference_list.jsonì—ì„œ NeurIPS ê´€ë ¨ ì»¨í¼ëŸ°ìŠ¤ë“¤ì„ ëª¨ë‘ í¬ë¡¤ë§
    """
    # conference_list.json íŒŒì¼ ê²½ë¡œ
    base_dir = os.path.dirname(os.path.dirname(__file__))
    config_path = os.path.join(base_dir, "conference_list.json")
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = json.load(f)

    conference_urls = {}
    for field in config['fields']:
        for conf in field['conferences']:
            if conf.get('crawler') == 'neurips_crawler':
                conference_urls[conf['name']] = conf['site']

    all_results = []

    for conf_name, url in conference_urls.items():
        print(f"ðŸ” [{conf_name}] í¬ë¡¤ë§ ì¤‘...")
        for paper in crawl_all_papers(url):
            paper['conference'] = conf_name
            all_results.append(paper)

    # ì „ì²´ ìµœì¢… ì €ìž¥
    output_path = os.path.join(base_dir, "neurips_papers_2024.json")
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    
    print(f"ì´ {len(all_results)}ê°œ ë…¼ë¬¸ì´ {output_path}ì— ì €ìž¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return all_results

if __name__ == "__main__":
    neurips_crawler()
