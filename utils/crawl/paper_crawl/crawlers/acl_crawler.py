# Target URL: 
# https://aclanthology.org/volumes/2024.emnlp-main/
# https://aclanthology.org/volumes/2024.acl-long/
# https://aclanthology.org/volumes/2024.naacl-long/
# title class= "align-middle", Ìï¥Îãπ titleÏùò ÎßÅÌÅ¨ : href= ~
# Ìï¥Îãπ ÎÖºÎ¨∏ ÌéòÏù¥ÏßÄÏóê Îì§Ïñ¥ÏôÄÏÑúÎäî Î≥¥Ïù¥Îäî Abstract class= "card-body acl-abstract"Ïùò span -> Ïù¥Í±∏ Î∂ÑÏÑù
import requests
from bs4 import BeautifulSoup
import json
import re
import os

def get_preview_sentences(text, num_sentences=2):
    sentences = re.split(r'(?<=[.!?])\s+', text)  # Î¨∏Ïû• Î∂ÑÎ¶¨
    return " ".join(sentences[:num_sentences])

def fetch_paper_titles_and_links(url: str):
    response = requests.get(url)
    response.raise_for_status()

    soup = BeautifulSoup(response.text, 'html.parser')
    paper_tags = soup.find_all("a", class_=lambda c: c and "align-middle" in c.split())

    papers = []
    for tag in paper_tags:
        title = tag.text.strip()
        # Ï†úÎ™©Ïù¥ ÎπÑÏñ¥ ÏûàÍ±∞ÎÇò ÌäπÏ†ï ÌÇ§ÏõåÎìúÎ©¥ Í±¥ÎÑàÎúÄ
        if not title or title.lower() in {"pdf", "bib", "abs"}:
            continue

        href = tag['href']
        link = href if href.startswith("http") else "https://aclanthology.org" + href

        papers.append({"title": title, "url": link})
    return papers

def fetch_abstract_and_authors(paper_url: str):
    response = requests.get(paper_url)
    response.raise_for_status()
    soup = BeautifulSoup(response.text, 'html.parser')

    # Abstract
    abstract_div = soup.find("div", class_="card-body acl-abstract")
    abstract = abstract_div.find("span").text.strip() if abstract_div and abstract_div.find("span") else "Abstract not found"

    # Authors
    lead_p = soup.find("p", class_="lead")
    authors = lead_p.get_text(separator=", ").strip() if lead_p else "Authors not found"

    return abstract, authors

def crawl_all_papers(url: str):
    """
    Ï£ºÏñ¥ÏßÑ URLÏóêÏÑú Î™®Îì† ÎÖºÎ¨∏ÏùÑ ÌÅ¨Î°§ÎßÅÌïòÏó¨ ÌïòÎÇòÏî© Ïã§ÏãúÍ∞ÑÏúºÎ°ú Î∞òÌôò
    """
    print(f"üîç ACL ÌÅ¨Î°§ÎßÅ ÏãúÏûë: {url}")
    papers = fetch_paper_titles_and_links(url)

    for i, paper in enumerate(papers, 1):
        try:
            # Ï†úÎ™©Ïù¥ ÎπÑÏñ¥ÏûàÎäî Í≤ΩÏö∞ Í±¥ÎÑàÎúÄ
            if not paper['title'].strip():
                continue

            abstract, authors = fetch_abstract_and_authors(paper['url'])
            
            paper_data = {
                'title': paper['title'],
                'abstract': abstract,
                'authors': authors,
                'url': paper['url'],
                'year': 2024  # Í∏∞Î≥∏Í∞í
            }
            
            print(f"‚úÖ ÎÖºÎ¨∏ {i}: {paper['title'][:50]}...")
            yield paper_data  # Ïã§ÏãúÍ∞ÑÏúºÎ°ú ÌïòÎÇòÏî© Î∞òÌôò

        except Exception as e:
            print(f"‚ùå ÎÖºÎ¨∏ {i} Ï≤òÎ¶¨ Ïã§Ìå®: {e}")

    print(f"Ï¥ù {len(papers)}Í∞ú ÎÖºÎ¨∏ ÌÅ¨Î°§ÎßÅ ÏôÑÎ£å")

def acl_crawler():
    """
    Í∏∞Ï°¥ Ìï®Ïàò - conference_list.jsonÏóêÏÑú ACL Í¥ÄÎ†® Ïª®ÌçºÎü∞Ïä§Îì§ÏùÑ Î™®Îëê ÌÅ¨Î°§ÎßÅ
    """
    # conference_list.json ÌååÏùº Í≤ΩÎ°ú
    base_dir = os.path.dirname(os.path.dirname(__file__))
    config_path = os.path.join(base_dir, "conference_list.json")
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = json.load(f)

    conference_urls = {}
    for field in config['fields']:
        for conf in field['conferences']:
            if conf.get('crawler') == 'acl_crawler':
                conference_urls[conf['name']] = conf['site']

    all_results = []

    for conf_name, url in conference_urls.items():
        print(f"üîç [{conf_name}] ÌÅ¨Î°§ÎßÅ Ï§ë...")
        for paper in crawl_all_papers(url):
            paper['conference'] = conf_name
            all_results.append(paper)

    # Ï†ÑÏ≤¥ ÏµúÏ¢Ö Ï†ÄÏû•
    output_path = os.path.join(base_dir, "acl_papers_2024.json")
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    
    print(f"Ï¥ù {len(all_results)}Í∞ú ÎÖºÎ¨∏Ïù¥ {output_path}Ïóê Ï†ÄÏû•ÎêòÏóàÏäµÎãàÎã§.")
    return all_results

if __name__ == "__main__":
    acl_crawler()