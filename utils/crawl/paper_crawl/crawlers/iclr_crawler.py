import requests
from bs4 import BeautifulSoup
import json
import re
import os
import time

def get_preview_sentences(text, num_sentences=2):
    sentences = re.split(r'(?<=[.!?])\s+', text)  # ë¬¸ì¥ ë¶„ë¦¬
    return " ".join(sentences[:num_sentences])

def test_single_paper():
    """ë‹¨ì¼ ë…¼ë¬¸ í…ŒìŠ¤íŠ¸"""
    test_url = "https://openreview.net/forum?id=odjMSBSWRt"  # ì‚¬ì§„ì—ì„œ ë³´ì¸ ë…¼ë¬¸
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    print(f"í…ŒìŠ¤íŠ¸ ë…¼ë¬¸ URL: {test_url}")
    
    try:
        response = requests.get(test_url, headers=headers)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        print(f"í˜ì´ì§€ ì œëª©: {soup.title.text if soup.title else 'No title'}")
        print(f"HTML ê¸¸ì´: {len(response.text)}")
        
        # ì œëª© ì°¾ê¸°
        title = "Title not found"
        title_elements = soup.find_all("h1") + soup.find_all("h2") + soup.find_all("h3") + soup.find_all("h4")
        for elem in title_elements:
            if elem.get_text(strip=True) and len(elem.get_text(strip=True)) > 10:
                title = elem.get_text(strip=True)
                print(f"ì œëª© ì°¾ìŒ: {title}")
                break
        
        # ì´ˆë¡ ì°¾ê¸°
        abstract = "Abstract not found"
        
        # ë°©ë²• 1: class="note-content-value markdown-rendered"
        abstract_div = soup.find("div", class_="note-content-value markdown-rendered")
        if abstract_div:
            abstract = abstract_div.get_text(strip=True)
            print(f"ì´ˆë¡ ì°¾ìŒ (ë°©ë²• 1): {len(abstract)}ì")
        else:
            # ë°©ë²• 2: class="note-content"
            abstract_div = soup.find("div", class_="note-content")
            if abstract_div:
                abstract = abstract_div.get_text(strip=True)
                print(f"ì´ˆë¡ ì°¾ìŒ (ë°©ë²• 2): {len(abstract)}ì")
            else:
                # ë°©ë²• 3: TL;DR ì°¾ê¸°
                tldr_elements = soup.find_all(string=re.compile(r"TL;DR:", re.IGNORECASE))
                if tldr_elements:
                    for elem in tldr_elements:
                        parent = elem.parent
                        if parent:
                            abstract = parent.get_text(strip=True)
                            print(f"ì´ˆë¡ ì°¾ìŒ (ë°©ë²• 3): {len(abstract)}ì")
                            break
        
        # ì €ì ì°¾ê¸°
        authors = "Authors not found"
        
        # ë°©ë²• 1: class="forum-authors" - ì‹¤ì œ HTML êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •
        authors_div = soup.find("div", class_="forum-authors")
        if authors_div:
            print(f"forum-authors div ë°œê²¬: {authors_div}")
            author_text = authors_div.get_text(strip=True)
            if author_text and len(author_text) > 5:
                # ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ì €ìë“¤ì„ ë¶„ë¦¬
                author_names = [name.strip() for name in author_text.split(',') if name.strip()]
                if author_names:
                    authors = ", ".join(author_names)
                    print(f"ì €ì ì°¾ìŒ (ë°©ë²• 1): {len(author_names)}ëª… - {authors}")
        
        # ë°©ë²• 2: class="note-authors" - ê¸°ì¡´ ë°©ë²•ë„ ìœ ì§€
        if authors == "Authors not found":
            authors_div = soup.find("div", class_="note-authors")
            if authors_div:
                print(f"note-authors div ë°œê²¬: {authors_div}")
                author_links = authors_div.find_all("a")
                print(f"ì €ì ë§í¬ ìˆ˜: {len(author_links)}")
                if author_links:
                    author_names = []
                    for i, author_link in enumerate(author_links):
                        author_name = author_link.get_text(strip=True)
                        print(f"ì €ì {i+1}: {author_name}")
                        if author_name and len(author_name) > 1:  # ì˜ë¯¸ìˆëŠ” ì´ë¦„ì¸ì§€ í™•ì¸
                            author_names.append(author_name)
                    if author_names:
                        authors = ", ".join(author_names)
                        print(f"ì €ì ì°¾ìŒ (ë°©ë²• 2): {len(author_names)}ëª… - {authors}")
        
        # ë°©ë²• 3: ì €ì ì •ë³´ê°€ ìˆëŠ” ë‹¤ë¥¸ ìš”ì†Œë“¤
        if authors == "Authors not found":
            # ì €ì ì´ë¦„ì´ í¬í•¨ëœ í…ìŠ¤íŠ¸ ì°¾ê¸°
            author_elements = soup.find_all(string=re.compile(r"Esben Kran|Hieu Minh Nguyen|Akash Kundu|Sami Jawhar|Jinsuk Park|Mateusz Maria Jurewicz", re.IGNORECASE))
            if author_elements:
                for elem in author_elements:
                    parent = elem.parent
                    if parent:
                        author_text = parent.get_text(strip=True)
                        if len(author_text) > 10 and len(author_text) < 500:  # ì ì ˆí•œ ê¸¸ì´ì¸ì§€ í™•ì¸
                            authors = author_text
                            print(f"ì €ì ì°¾ìŒ (ë°©ë²• 3): {authors}")
                            break
        
        # ë°©ë²• 4: ëª¨ë“  a íƒœê·¸ì—ì„œ ì €ì ì°¾ê¸°
        if authors == "Authors not found":
            all_links = soup.find_all("a", href=True)
            author_links = []
            for link in all_links:
                href = link.get('href', '')
                if '/profile?id=' in href:
                    author_name = link.get_text(strip=True)
                    if author_name and len(author_name) > 1:
                        author_links.append(author_name)
            
            if author_links:
                authors = ", ".join(author_links)
                print(f"ì €ì ì°¾ìŒ (ë°©ë²• 4): {len(author_links)}ëª… - {authors}")
        
        # ë°©ë²• 5: JavaScriptì—ì„œ ì €ì ì •ë³´ ì¶”ì¶œ
        if authors == "Authors not found":
            scripts = soup.find_all("script")
            for script in scripts:
                if script.string and "authors" in script.string.lower():
                    # ë‹¤ì–‘í•œ íŒ¨í„´ ì‹œë„
                    author_patterns = [
                        r'"authors":\s*\[([^\]]+)\]',
                        r'"authors":\s*"([^"]+)"',
                        r'"author":\s*"([^"]+)"'
                    ]
                    
                    for pattern in author_patterns:
                        authors_match = re.search(pattern, script.string)
                        if authors_match:
                            authors_text = authors_match.group(1)
                            if '[' in authors_text:  # ë°°ì—´ í˜•íƒœì¸ ê²½ìš°
                                # ë”°ì˜´í‘œë¡œ ë‘˜ëŸ¬ì‹¸ì¸ ì €ì ì´ë¦„ë“¤ ì¶”ì¶œ
                                author_names = re.findall(r'"([^"]+)"', authors_text)
                                if author_names:
                                    authors = ", ".join(author_names)
                                    print(f"ì €ì ì°¾ìŒ (ë°©ë²• 5): {len(author_names)}ëª… - {authors}")
                                    break
                            else:  # ë‹¨ì¼ ë¬¸ìì—´ì¸ ê²½ìš°
                                authors = authors_text
                                print(f"ì €ì ì°¾ìŒ (ë°©ë²• 5): {authors}")
                                break
                    if authors != "Authors not found":
                        break
        
        print(f"\n=== í…ŒìŠ¤íŠ¸ ê²°ê³¼ ===")
        print(f"ì œëª©: {title}")
        print(f"ì´ˆë¡: {abstract[:100]}...")
        print(f"ì €ì: {authors}")
        
        return {
            'title': title,
            'abstract': abstract,
            'authors': authors,
            'url': test_url
        }
        
    except Exception as e:
        print(f"í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return None

def fetch_paper_titles_and_links(url: str):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    # ë¨¼ì € ë‹¨ì¼ ë…¼ë¬¸ í…ŒìŠ¤íŠ¸
    print("ë‹¨ì¼ ë…¼ë¬¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰...")
    test_result = test_single_paper()
    
    if test_result:
        print("í…ŒìŠ¤íŠ¸ ì„±ê³µ! ì´ì œ ì „ì²´ í¬ë¡¤ë§ ì‹œë„...")
    
    # OpenReview ë©”ì¸ í˜ì´ì§€ì—ì„œ ë…¼ë¬¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    print("OpenReview ë©”ì¸ í˜ì´ì§€ì—ì„œ ë…¼ë¬¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°...")
    
    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        print(f"í˜ì´ì§€ ì œëª©: {soup.title.text if soup.title else 'No title'}")
        print(f"HTML ê¸¸ì´: {len(response.text)}")
        
        # JavaScript ë°ì´í„°ì—ì„œ ë…¼ë¬¸ ì •ë³´ ì°¾ê¸°
        scripts = soup.find_all("script")
        papers = []
        
        for i, script in enumerate(scripts):
            if script.string:
                # forum ë§í¬ ì°¾ê¸°
                forum_matches = re.findall(r'/forum\?id=([^"\s&]+)', script.string)
                if forum_matches:
                    print(f"Script {i}ì—ì„œ {len(forum_matches)}ê°œì˜ forum ë§í¬ ë°œê²¬")
                    
                    # í•´ë‹¹ scriptì˜ ë‚´ìš© ì¼ë¶€ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
                    script_content = script.string[:500]
                    print(f"Script {i} ë‚´ìš© (ì²˜ìŒ 500ì): {script_content}")
                    
                    for paper_id in forum_matches:
                        print(f"ë…¼ë¬¸ ID ì²˜ë¦¬ ì¤‘: {paper_id}")
                        
                        # ì œëª© ì°¾ê¸° - ë‹¤ì–‘í•œ íŒ¨í„´ ì‹œë„
                        title_patterns = [
                            rf'"id":\s*"{paper_id}"[^}}]*"title":\s*"([^"]+)"',
                            rf'"id":\s*"{paper_id}"[^}}]*"content":\s*{{[^}}]*"title":\s*{{[^}}]*"value":\s*"([^"]+)"',
                            rf'"{paper_id}"[^}}]*"title":\s*"([^"]+)"',
                            rf'"id":\s*"{paper_id}"[^}}]*"value":\s*"([^"]+)"',
                            rf'"{paper_id}"[^}}]*"value":\s*"([^"]+)"'
                        ]
                        
                        title = None
                        for j, pattern in enumerate(title_patterns):
                            title_match = re.search(pattern, script.string)
                            if title_match:
                                title = title_match.group(1)
                                print(f"íŒ¨í„´ {j}ë¡œ ì œëª© ì°¾ìŒ: {title[:50]}...")
                                break
                        
                        if title and len(title) > 5:  # ì˜ë¯¸ìˆëŠ” ì œëª©ì¸ì§€ í™•ì¸
                            paper_url = f"https://openreview.net/forum?id={paper_id}"
                            papers.append({"title": title, "url": paper_url, "id": paper_id})
                            print(f"ë…¼ë¬¸ ì¶”ê°€: {title[:50]}...")
                        else:
                            print(f"ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {paper_id}")
        
        print(f"JavaScriptì—ì„œ ì°¾ì€ ë…¼ë¬¸ ìˆ˜: {len(papers)}")
        
        # ì¤‘ë³µ ì œê±°
        unique_papers = []
        seen_ids = set()
        for paper in papers:
            if paper['id'] not in seen_ids:
                unique_papers.append(paper)
                seen_ids.add(paper['id'])
        
        print(f"ì¤‘ë³µ ì œê±° í›„ ë…¼ë¬¸ ìˆ˜: {len(unique_papers)}")
        
        if unique_papers:
            return unique_papers
        
    except Exception as e:
        print(f"ë©”ì¸ í˜ì´ì§€ ì ‘ê·¼ ì‹¤íŒ¨: {e}")
    
    # ëª¨ë“  ë°©ë²• ì‹¤íŒ¨ ì‹œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë°˜í™˜
    print("ëª¨ë“  ë°©ë²• ì‹¤íŒ¨. í…ŒìŠ¤íŠ¸ ë°ì´í„° ì‚¬ìš©...")
    if test_result:
        return [test_result]
    
    return []

def fetch_abstract_and_authors(paper_url: str):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    # ì ì‹œ ëŒ€ê¸° (ìš”ì²­ ì œí•œ ë°©ì§€)
    time.sleep(1)
    
    response = requests.get(paper_url, headers=headers)
    response.raise_for_status()
    soup = BeautifulSoup(response.text, 'html.parser')

    # Abstract ì¶”ì¶œ: ë‹¤ì–‘í•œ ì„ íƒì ì‹œë„
    abstract = "Abstract not found"
    
    # ë°©ë²• 1: class="note-content-value markdown-rendered"
    abstract_div = soup.find("div", class_="note-content-value markdown-rendered")
    if abstract_div:
        abstract = abstract_div.get_text(strip=True)
        print(f"ë°©ë²• 1ë¡œ ì´ˆë¡ ì°¾ìŒ: {len(abstract)}ì")
    else:
        # ë°©ë²• 2: class="note-content"
        abstract_div = soup.find("div", class_="note-content")
        if abstract_div:
            abstract = abstract_div.get_text(strip=True)
            print(f"ë°©ë²• 2ë¡œ ì´ˆë¡ ì°¾ìŒ: {len(abstract)}ì")
        else:
            # ë°©ë²• 3: TL;DR ì°¾ê¸°
            tldr_div = soup.find("div", string=re.compile(r"TL;DR:", re.IGNORECASE))
            if tldr_div:
                abstract = tldr_div.get_text(strip=True)
                print(f"ë°©ë²• 3ìœ¼ë¡œ ì´ˆë¡ ì°¾ìŒ: {len(abstract)}ì")
            else:
                # ë°©ë²• 4: JavaScriptì—ì„œ ì¶”ì¶œ
                scripts = soup.find_all("script")
                for script in scripts:
                    if script.string and "abstract" in script.string.lower():
                        abstract_match = re.search(r'"abstract":\s*"([^"]+)"', script.string)
                        if abstract_match:
                            abstract = abstract_match.group(1)
                            print(f"ë°©ë²• 4ë¡œ ì´ˆë¡ ì°¾ìŒ: {len(abstract)}ì")
                            break
    
    # Authors ì¶”ì¶œ: ë‹¤ì–‘í•œ ì„ íƒì ì‹œë„
    authors = "Authors not found"
    
    # ë°©ë²• 1: class="forum-authors" - ì‹¤ì œ HTML êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •
    authors_div = soup.find("div", class_="forum-authors")
    if authors_div:
        print(f"forum-authors div ë°œê²¬: {authors_div}")
        author_text = authors_div.get_text(strip=True)
        if author_text and len(author_text) > 5:
            # ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ì €ìë“¤ì„ ë¶„ë¦¬
            author_names = [name.strip() for name in author_text.split(',') if name.strip()]
            if author_names:
                authors = ", ".join(author_names)
                print(f"ë°©ë²• 1ë¡œ ì €ì ì°¾ìŒ: {len(author_names)}ëª… - {authors}")
    
    # ë°©ë²• 2: class="note-authors" - ê¸°ì¡´ ë°©ë²•ë„ ìœ ì§€
    if authors == "Authors not found":
        authors_div = soup.find("div", class_="note-authors")
        if authors_div:
            print(f"note-authors div ë°œê²¬: {authors_div}")
            author_links = authors_div.find_all("a")
            print(f"ì €ì ë§í¬ ìˆ˜: {len(author_links)}")
            if author_links:
                author_names = []
                for i, author_link in enumerate(author_links):
                    author_name = author_link.get_text(strip=True)
                    print(f"ì €ì {i+1}: {author_name}")
                    if author_name and len(author_name) > 1:  # ì˜ë¯¸ìˆëŠ” ì´ë¦„ì¸ì§€ í™•ì¸
                        author_names.append(author_name)
                if author_names:
                    authors = ", ".join(author_names)
                    print(f"ë°©ë²• 2ë¡œ ì €ì ì°¾ìŒ: {len(author_names)}ëª… - {authors}")
    
    # ë°©ë²• 3: ì €ì ì •ë³´ê°€ ìˆëŠ” ë‹¤ë¥¸ ìš”ì†Œë“¤
    if authors == "Authors not found":
        # ì €ì ì´ë¦„ì´ í¬í•¨ëœ í…ìŠ¤íŠ¸ ì°¾ê¸°
        author_elements = soup.find_all(string=re.compile(r"Esben Kran|Hieu Minh Nguyen|Akash Kundu|Sami Jawhar|Jinsuk Park|Mateusz Maria Jurewicz", re.IGNORECASE))
        if author_elements:
            for elem in author_elements:
                parent = elem.parent
                if parent:
                    author_text = parent.get_text(strip=True)
                    if len(author_text) > 10 and len(author_text) < 500:  # ì ì ˆí•œ ê¸¸ì´ì¸ì§€ í™•ì¸
                        authors = author_text
                        print(f"ë°©ë²• 3ìœ¼ë¡œ ì €ì ì°¾ìŒ: {authors}")
                        break
    
    # ë°©ë²• 4: ëª¨ë“  a íƒœê·¸ì—ì„œ ì €ì ì°¾ê¸°
    if authors == "Authors not found":
        all_links = soup.find_all("a", href=True)
        author_links = []
        for link in all_links:
            href = link.get('href', '')
            if '/profile?id=' in href:
                author_name = link.get_text(strip=True)
                if author_name and len(author_name) > 1:
                    author_links.append(author_name)
        
        if author_links:
            authors = ", ".join(author_links)
            print(f"ë°©ë²• 4ë¡œ ì €ì ì°¾ìŒ: {len(author_links)}ëª… - {authors}")
    
    # ë°©ë²• 5: JavaScriptì—ì„œ ì €ì ì •ë³´ ì¶”ì¶œ
    if authors == "Authors not found":
        scripts = soup.find_all("script")
        for script in scripts:
            if script.string and "authors" in script.string.lower():
                # ë‹¤ì–‘í•œ íŒ¨í„´ ì‹œë„
                author_patterns = [
                    r'"authors":\s*\[([^\]]+)\]',
                    r'"authors":\s*"([^"]+)"',
                    r'"author":\s*"([^"]+)"'
                ]
                
                for pattern in author_patterns:
                    authors_match = re.search(pattern, script.string)
                    if authors_match:
                        authors_text = authors_match.group(1)
                        if '[' in authors_text:  # ë°°ì—´ í˜•íƒœì¸ ê²½ìš°
                            # ë”°ì˜´í‘œë¡œ ë‘˜ëŸ¬ì‹¸ì¸ ì €ì ì´ë¦„ë“¤ ì¶”ì¶œ
                            author_names = re.findall(r'"([^"]+)"', authors_text)
                            if author_names:
                                authors = ", ".join(author_names)
                                print(f"ë°©ë²• 5ë¡œ ì €ì ì°¾ìŒ: {len(author_names)}ëª… - {authors}")
                                break
                        else:  # ë‹¨ì¼ ë¬¸ìì—´ì¸ ê²½ìš°
                            authors = authors_text
                            print(f"ë°©ë²• 5ë¡œ ì €ì ì°¾ìŒ: {authors}")
                            break
                if authors != "Authors not found":
                    break

    return abstract, authors

def crawl_all_papers(url: str):
    """
    ì£¼ì–´ì§„ URLì—ì„œ ëª¨ë“  ë…¼ë¬¸ì„ í¬ë¡¤ë§í•˜ì—¬ í•˜ë‚˜ì”© ì‹¤ì‹œê°„ìœ¼ë¡œ ë°˜í™˜
    """
    print(f"ğŸ” ICLR í¬ë¡¤ë§ ì‹œì‘: {url}")
    papers = fetch_paper_titles_and_links(url)
    
    print(f"ì°¾ì€ ë…¼ë¬¸ ìˆ˜: {len(papers)}")
    if papers:
        print(f"ì²« ë²ˆì§¸ ë…¼ë¬¸ ì˜ˆì‹œ: {papers[0]}")

    for i, paper in enumerate(papers, 1):
        try:
            # ì œëª©ì´ ë¹„ì–´ìˆëŠ” ê²½ìš° ê±´ë„ˆëœ€
            if not paper['title'].strip():
                continue

            print(f"ğŸ“„ ë…¼ë¬¸ {i} ì²˜ë¦¬ ì¤‘: {paper['url']}")
            abstract, authors = fetch_abstract_and_authors(paper['url'])
            
            paper_data = {
                'title': paper['title'],
                'abstract': abstract,
                'authors': authors,
                'url': paper['url'],
                'year': 2025  # ICLR 2025
            }
            
            print(f"âœ… ë…¼ë¬¸ {i}: {paper['title'][:50]}...")
            yield paper_data  # ì‹¤ì‹œê°„ìœ¼ë¡œ í•˜ë‚˜ì”© ë°˜í™˜

        except Exception as e:
            print(f"âŒ ë…¼ë¬¸ {i} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

    print(f"ì´ {len(papers)}ê°œ ë…¼ë¬¸ í¬ë¡¤ë§ ì™„ë£Œ")

def iclr_crawler():
    """
    ê¸°ì¡´ í•¨ìˆ˜ - conference_list.jsonì—ì„œ ICLR ê´€ë ¨ ì»¨í¼ëŸ°ìŠ¤ë“¤ì„ ëª¨ë‘ í¬ë¡¤ë§
    """
    # conference_list.json íŒŒì¼ ê²½ë¡œ
    base_dir = os.path.dirname(os.path.dirname(__file__))
    config_path = os.path.join(base_dir, "conference_list.json")
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = json.load(f)

    conference_urls = {}
    for field in config['fields']:
        for conf in field['conferences']:
            if conf.get('crawler') == 'iclr_crawler':
                conference_urls[conf['name']] = conf['site']

    all_results = []

    for conf_name, url in conference_urls.items():
        print(f"ğŸ” [{conf_name}] í¬ë¡¤ë§ ì¤‘...")
        for paper in crawl_all_papers(url):
            paper['conference'] = conf_name
            all_results.append(paper)

    # ì „ì²´ ìµœì¢… ì €ì¥
    output_path = os.path.join(base_dir, "iclr_papers_2025.json")
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    
    print(f"ì´ {len(all_results)}ê°œ ë…¼ë¬¸ì´ {output_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return all_results

if __name__ == "__main__":
    iclr_crawler()
