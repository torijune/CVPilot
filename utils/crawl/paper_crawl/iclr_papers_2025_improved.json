[
  {
    "title": "DarkBench: Benchmarking Dark Patterns in Large Language Models",
    "abstract": "Keywords:Dark Patterns, AI Deception, Large Language ModelsTL;DR:We introduce DarkBench, a benchmark revealing that many large language models employ manipulative dark design patterns. Organizations developing LLMs should actively recognize and mitigate the impact of dark design patterns to promote ethical Al.Abstract:We introduce DarkBench, a comprehensive benchmark for detecting dark design patterns—manipulative techniques that influence user behavior—in interactions with large language models (LLMs). Our benchmark comprises 660 prompts across six categories: brand bias, user retention, sycophancy, anthropomorphism, harmful generation, and sneaking. We evaluate models from five leading companies (OpenAI, Anthropic, Meta, Mistral, Google) and find that some LLMs are explicitly designed to favor their developers' products and exhibit untruthful communication, among other manipulative behaviors. Companies developing LLMs should recognize and mitigate the impact of dark design patterns to promote more ethical Al.Primary Area:alignment, fairness, safety, privacy, and societal considerationsCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14257",
    "authors": "Esben Kran, Hieu Minh Nguyen, Akash Kundu, Sami Jawhar, Jinsuk Park, Mateusz Maria Jurewicz",
    "url": "https://openreview.net/forum?id=odjMSBSWRt",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style",
    "abstract": "Keywords:Reward Models, Language Models, Evaluation, AlignmentAbstract:Reward models are critical in techniques like Reinforcement Learning from Human Feedback (RLHF) and Inference Scaling Laws, where they guide language model alignment and select optimal responses. \nDespite their importance, existing reward model benchmarks often evaluate models by asking them to distinguish between responses generated by models of varying power. \nHowever, this approach fails to assess reward models on subtle but critical content changes and variations in style, resulting in a low correlation with policy model performance.\nTo this end, we introduce RM-Bench, a novel benchmark designed to evaluate reward models based on their sensitivity to subtle content differences and resistance to style biases. \nExtensive experiments demonstrate that RM-Bench strongly correlates with policy model performance, making it a reliable reference for selecting reward models to align language models effectively.\nWe evaluate nearly 40 reward models on RM-Bench. \nOur results reveal that even state-of-the-art models achieve an average performance of only 46.6%, which falls short of random-level accuracy (50%) when faced with style bias interference.\nThese findings highlight the significant room for improvement in current reward models.Primary Area:datasets and benchmarksCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:13985",
    "authors": "Yantao Liu, Zijun Yao, Rui Min, Yixin Cao, Lei Hou, Juanzi Li",
    "url": "https://openreview.net/forum?id=QEHrmQPBdd",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "TopoLM: brain-like spatio-functional organization in a topographic language model",
    "abstract": "Keywords:language modeling, topography, fMRI, neuroscienceTL;DR:We develop a transformer language model with topographically organized units predicting brain-like spatio-functional organization.Abstract:Neurons in the brain are spatially organized such that neighbors on tissue often exhibit similar response profiles. In the human language system, experimental studies have observed clusters for syntactic and semantic categories, but the mechanisms underlying this functional organization remain unclear. Here, building on work from the vision literature, we develop TopoLM, a transformer language model with an explicit two-dimensional spatial representation of model units. By combining a next-token prediction objective with a spatial smoothness loss, representations in this model assemble into clusters that correspond to semantically interpretable groupings of text and closely match the functional organization in the brain's language system. TopoLM successfully predicts the emergence of a spatially organized cortical language system as well as the organization of functional clusters selective for fine-grained linguistic features empirically observed in human cortex. Our results suggest that the functional organization of the human language system is driven by a unified spatial objective, and provide a functionally and spatially aligned model of language processing in the brain.Neurons in the brain are spatially organized such that neighbors on tissue often exhibit similar response profiles. In the human language system, experimental studies have observed clusters for syntactic and semantic categories, but the mechanisms underlying this functional organization remain unclear. Here, building on work from the vision literature, we develop TopoLM, a transformer language model with an explicit two-dimensional spatial representation of model units. By combining a next-token prediction objective with a spatial smoothness loss, representations in this model assemble into clusters that correspond to semantically interpretable groupings of text and closely match the functional organization in the brain's language system. TopoLM successfully predicts the emergence of a spatially organized cortical language system as well as the organization of functional clusters selective for fine-grained linguistic features empirically observed in human cortex. Our results suggest that the functional organization of the human language system is driven by a unified spatial objective, and provide a functionally and spatially aligned model of language processing in the brain.Primary Area:applications to neuroscience & cognitive scienceCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:13712",
    "authors": "Neil Rathi, Johannes Mehrer, Badr AlKhamissi, Taha Osama A Binhuraib, Nicholas Blauch, Martin Schrimpf",
    "url": "https://openreview.net/forum?id=aWXnKanInf",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Spider 2.0: Evaluating Language Models on Real-World Enterprise Text-to-SQL Workflows",
    "abstract": "Keywords:LLM Benchmark, Data Science and Engineering, Code Generation, Text-to-SQL, LLM AgentTL;DR:A benchmark for enterprise-level Text-to-SQL involving complex databases, challenging tasks, and real-world scenarios.Abstract:Real-world enterprise text-to-SQL workflows often involve complex cloud or local data across various database systems, multiple SQL queries in various dialects, and diverse operations from data transformation to analytics.\nWe introduce Spider 2.0, an evaluation framework comprising $632$ real-world text-to-SQL workflow problems derived from enterprise-level database use cases. \nThe databases in Spider 2.0 are sourced from real data applications, often containing over 1,000 columns and stored in local or cloud database systems such as BigQuery and Snowflake.\nWe show that solving problems in Spider 2.0 frequently requires understanding and searching through database metadata, dialect documentation, and even project-level codebases. \nThis challenge calls for models to interact with complex SQL workflow environments, process extremely long contexts, perform intricate reasoning, and generate multiple SQL queries with diverse operations, often exceeding $100$ lines, which goes far beyond traditional text-to-SQL challenges.\nOur evaluations indicate that based on o1-preview, our code agent framework successfully solves only 21.3\\% of the tasks, compared with 91.2\\% on Spider 1.0 and 73.0\\% on BIRD.\nOur results on Spider 2.0 show that while language models have demonstrated remarkable performance in code generation --- especially in prior text-to-SQL benchmarks --- they require significant improvement in order to achieve adequate performance for real-world enterprise usage.\nProgress on Spider 2.0 represents crucial steps towards developing intelligent, autonomous, code agents for real-world enterprise settings.\nOur code, baseline models, and data are available at [spider2-sql.github.io](spider2-sql.github.io) .Primary Area:datasets and benchmarksCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:13657",
    "authors": "Fangyu Lei, Jixuan Chen, Yuxiao Ye, Ruisheng Cao, Dongchan Shin, Hongjin SU, ZHAOQING SUO, Hongcheng Gao, Wenjing Hu, Pengcheng Yin, Victor Zhong, Caiming Xiong, Ruoxi Sun, Qian Liu, Sida Wang, Tao Yu",
    "url": "https://openreview.net/forum?id=XmProj9cPs",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Knowledge Entropy Decay during Language Model Pretraining Hinders New Knowledge Acquisition",
    "abstract": "Keywords:knowledge entropy, knowledge acquisition and forgetting, evolving behavior during LLM pretrainingTL;DR:As pretraining progresses, models exhibit narrower integration of memory vectors, reflected by decreasing knowledge entropy, which hinders both knowledge acquisition and retention.Abstract:In this work, we investigate how a model's tendency to broadly integrate its parametric knowledge evolves throughout pretraining, and how this behavior affects overall performance, particularly in terms of knowledge acquisition and forgetting. We introduce the concept of knowledge entropy, which quantifies the range of memory sources the model engages with; high knowledge entropy indicates that the model utilizes a wide range of memory sources, while low knowledge entropy suggests reliance on specific sources with greater certainty. Our analysis reveals a consistent decline in knowledge entropy as pretraining advances. We also find that the decline is closely associated with a reduction in the model's ability to acquire and retain knowledge, leading us to conclude that diminishing knowledge entropy (smaller number of active memory sources) impairs the model's knowledge acquisition and retention capabilities. We find further support for this by demonstrating that increasing the activity of inactive memory sources enhances the model's capacity for knowledge acquisition and retention.Primary Area:foundation or frontier models, including LLMsCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:13581",
    "authors": "Jiyeon Kim, Hyunji Lee, Hyowon Cho, Joel Jang, Hyeonbin Hwang, Seungpil Won, Youbin Ahn, Dohaeng Lee, Minjoon Seo",
    "url": "https://openreview.net/forum?id=eHehzSDUFp",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Diffusion-Based Planning for Autonomous Driving with Flexible Guidance",
    "abstract": "Keywords:diffusion planning, autonomous drivingAbstract:Achieving human-like driving behaviors in complex open-world environments is a critical challenge in autonomous driving. Contemporary learning-based planning approaches such as imitation learning methods often struggle to balance competing objectives and lack of safety assurance,due to limited adaptability and inadequacy in learning complex multi-modal behaviors commonly exhibited in human planning, not to mention their strong reliance on the fallback strategy with predefined rules. We propose a novel transformer-based Diffusion Planner for closed-loop planning, which can effectively model multi-modal driving behavior and ensure trajectory quality without any rule-based refinement. Our model supports joint modeling of both prediction and planning tasks under the same architecture, enabling cooperative behaviors between vehicles. Moreover, by learning the gradient of the trajectory score function and employing a flexible classifier guidance mechanism, Diffusion Planner effectively achieves safe and adaptable planning behaviors. Evaluations on the large-scale real-world autonomous planning benchmark nuPlan and our newly collected 200-hour delivery-vehicle driving dataset demonstrate that Diffusion Planner achieves state-of-the-art closed-loop performance with robust transferability in diverse driving styles.Primary Area:applications to robotics, autonomy, planningCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:13578",
    "authors": "Yinan Zheng, Ruiming Liang, Kexin ZHENG, Jinliang Zheng, Liyuan Mao, Jianxiong Li, Weihao Gu, Rui Ai, Shengbo Eben Li, Xianyuan Zhan, Jingjing Liu",
    "url": "https://openreview.net/forum?id=wM2sfVgMDH",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Learning to Search from Demonstration Sequences",
    "abstract": "Keywords:planning, reasoning, learning to search, reinforcement learning, large language modelTL;DR:We propose a method that constructs search tree in a differetiable manner, and can be trained from just demonstration sequences.Abstract:Search and planning are essential for solving many real-world problems. However, in numerous learning scenarios, only action-observation sequences, such as demonstrations or instruction sequences, are available for learning. Relying solely on supervised learning with these sequences can lead to sub-optimal performance due to the vast, unseen search space encountered during training. In this paper, we introduce Differentiable Tree Search Network (D-TSN), a novel neural network architecture that learns to construct search trees from just sequences of demonstrations by performing gradient descent on a best-first search tree construction algorithm. D-TSN enables the joint learning of submodules, including an encoder, value function, and world model, which are essential for planning. To construct the search tree, we employ a stochastic tree expansion policy and formulate it as another decision-making task. Then, we optimize the tree expansion policy via REINFORCE with an effective variance reduction technique for the gradient computation. D-TSN can be applied to problems with a known world model or to scenarios where it needs to jointly learn a world model with a latent state space. We study problems from these two scenarios, including Game of 24, 2D grid navigation, and Procgen games, to understand when D-TSN is more helpful. Through our experiments, we show that D-TSN is effective, especially when the world model with a latent state space is jointly learned. The code is available at https://github.com/dixantmittal/differentiable-tree-search-network.Primary Area:reinforcement learningCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:13425",
    "authors": "Dixant Mittal, Liwei Kang, Wee Sun Lee",
    "url": "https://openreview.net/forum?id=v593OaNePQ",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded Attributions and Learning to Refuse",
    "abstract": "Keywords:Large Language Models, Trustworthiness, Hallucinations, Retrieval Augmented GenerationTL;DR:How to better evaluate and make LLM better for RAG taskAbstract:LLMs are an integral component of retrieval-augmented generation (RAG) systems. While many studies focus on evaluating the overall quality of end-to-end RAG systems, there is a gap in understanding the appropriateness of LLMs for the RAG task. To address this, we introduce Trust-Score, a holistic metric that evaluates the trustworthiness of LLMs within the RAG framework. Our results show that various prompting methods, such as in-context learning, fail to effectively adapt LLMs to the RAG task as measured by Trust-Score. Consequently, we propose Trust-Align, a method to align LLMs for improved Trust-Score performance. 26 out of 27 models aligned using Trust-Align substantially outperform competitive baselines on ASQA, QAMPARI, and ELI5. Specifically, in LLaMA-3-8b, Trust-Align outperforms FRONT on ASQA (↑12.56), QAMPARI (↑36.04), and ELI5 (↑17.69). Trust-Align also significantly enhances models’ ability to correctly refuse and provide quality citations. We also demonstrate the effectiveness of Trust-Align across different open-weight models, including the LLaMA series (1b to 8b), Qwen-2.5 series (0.5b to 7b), and Phi3.5 (3.8b). We release our code at https://github.com/declare-lab/trust-align.Primary Area:alignment, fairness, safety, privacy, and societal considerationsCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:13377",
    "authors": "Maojia Song, Shang Hong Sim, Rishabh Bhardwaj, Hai Leong Chieu, Navonil Majumder, Soujanya Poria",
    "url": "https://openreview.net/forum?id=Iyrtb9EJBp",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "MAP: Multi-Human-Value Alignment Palette",
    "abstract": "Keywords:Human value alignment, Generative modelTL;DR:The paper introduces Multi-Human-Value Alignment Palette (MAP), a novel approach to align generative models with multiple human values in a principled way.Abstract:Ensuring that generative AI systems align with human values is essential but challenging, especially when considering multiple human values and their potential trade-offs. Since human values can be personalized and dynamically change over time, the desirable levels of value alignment vary across different ethnic groups, industry sectors, and user cohorts. Within existing frameworks, it is hard to define human values and align AI systems accordingly across different directions simultaneously, such as harmlessness, helpfulness, and positiveness. To address this, we develop a novel, first-principle approach called Multi-Human-Value Alignment Palette (MAP), which navigates the alignment across multiple human values in a structured and reliable way. MAP formulates the alignment problem as an optimization task with user-defined constraints, which define human value targets. It can be efficiently solved via a primal-dual approach, which determines whether a user-defined alignment target is achievable and how to achieve it. We conduct a detailed theoretical analysis of MAP by quantifying the trade-offs between values, the sensitivity to constraints, the fundamental connection between multi-value alignment and sequential alignment, and proving that linear weighted rewards are sufficient for multi-value alignment. Extensive experiments demonstrate MAP's ability to align multiple values in a principled manner while delivering strong empirical performance across various tasks.Primary Area:alignment, fairness, safety, privacy, and societal considerationsCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:13248",
    "authors": "Xinran Wang, Qi Le, Ammar Ahmed, Enmao Diao, Yi Zhou, Nathalie Baracaldo, Jie Ding, Ali Anwar",
    "url": "https://openreview.net/forum?id=NN6QHwgRrQ",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Can Neural Networks Achieve Optimal Computational-statistical Tradeoff? An Analysis on Single-Index Model",
    "abstract": "Keywords:single-index model, feature learning, gradient-based method, computational-statistical tradeoffTL;DR:We propose a unified gradient-based algorithm for feature learning in Gaussian single-index model with sample complexity matching the SQ lower boundAbstract:In this work, we tackle the following question: Can neural networks trained with gradient-based methods achieve the optimal statistical-computational tradeoff in learning Gaussian single-index models? \nPrior research has shown that any polynomial-time algorithm under the statistical query (SQ) framework requires $\\Omega(d^{s^\\star/2}\\lor d)$ samples, where $s^\\star$ is the generative exponent representing the intrinsic difficulty of learning the underlying model.\nHowever, it remains unknown whether neural networks can achieve this sample complexity. \nInspired by prior techniques such as label transformation and landscape smoothing for learning single-index models, we propose a unified gradient-based algorithm for training a two-layer neural network in polynomial time.\nOur method is adaptable to a variety of loss and activation functions, covering a broad class of existing approaches.\nWe show that our algorithm learns a feature representation that strongly aligns with the unknown signal $\\theta^\\star$, with sample complexity $\\tilde O (d^{s^\\star/2} \\lor d)$, matching the SQ lower bound up to a polylogarithmic factor for all generative exponents $s^\\star\\geq 1$.\nFurthermore, we extend our approach to the setting where $\\theta^\\star$ is $k$-sparse for $k = o(\\sqrt{d})$ by introducing a novel weight perturbation technique that leverages the sparsity structure. \nWe derive a corresponding SQ lower bound \nof order $\\tilde\\Omega(k^{s^\\star})$, matched by our method up to a polylogarithmic factor.\nOur framework, especially the weight perturbation technique, is of independent interest, and suggests potential gradient-based solutions to other problems such as sparse tensor PCA.Primary Area:learning theoryCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:13084",
    "authors": "Siyu Chen, Beining Wu, Miao Lu, Zhuoran Yang, Tianhao Wang",
    "url": "https://openreview.net/forum?id=is4nCVkSFA",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Consistency Checks for Language Model Forecasters",
    "abstract": "Keywords:forecasting, markets, trading, LLM, evaluation, eval, consistency, robustnessTL;DR:It is difficult to evaluate AI forecasters instantaneously; we propose market-based consistency evals on LLM forecasters and show plenty of inconsistency.Abstract:Forecasting is a task that is difficult to evaluate: the ground truth can only be known in the future. Recent work showing LLM forecasters rapidly approaching human-level performance begs the question: how can we benchmark and evaluate these forecasters *instantaneously*? Following the consistency check framework, we measure the performance of forecasters in terms of the consistency of their predictions on different logically-related questions. We propose a new, general consistency metric based on *arbitrage*: for example, if a forecasting AI illogically predicts that both the Democratic and Republican parties have 60\\% probability of winning the 2024 US presidential election, an arbitrageur could trade against the forecaster's predictions and make a profit. We build an automated evaluation system that generates a set of base questions, instantiates consistency checks from these questions, elicits the predictions of the forecaster, and measures the consistency of the predictions. We then build a standard, proper-scoring-rule forecasting benchmark, and show that our (instantaneous) consistency metrics correlate strongly with LLM forecasters' ground truth Brier scores (which are only known in the future). We also release a consistency benchmark that resolves in 2028, providing a long-term evaluation tool for forecasting.Supplementary Material:zipPrimary Area:datasets and benchmarksCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:13065",
    "authors": "Daniel Paleka, Abhimanyu Pallavi Sudhir, Alejandro Alvarez, Vineeth Bhat, Adam Shen, Evan Wang, Florian Tramèr",
    "url": "https://openreview.net/forum?id=r5IXBlTCGc",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Spread Preference Annotation: Direct Preference Judgment for Efficient LLM Alignment",
    "abstract": "Keywords:large language model, alignment, preferenceAbstract:Aligning large language models (LLMs) with human preferences becomes a key component to obtaining state-of-the-art performance, but it yields a huge cost to construct a large human-annotated preference dataset. To tackle this problem, we propose a new framework, Spread Preference Annotation with direct preference judgment (SPA), that boosts the alignment of LLMs using only a very small amount of human-annotated preference data.\nOur key idea is leveraging the human prior knowledge within the small (seed) data and progressively improving the alignment of LLM, by iteratively generating the responses and learning from them with the self-annotated preference data.\nTo be specific, we propose to derive the preference label from the logits of LLM to explicitly extract the model's inherent preference. \nCompared to the previous approaches using external reward models or implicit in-context learning, we observe that the proposed approach is significantly more effective.\nIn addition, we introduce a noise-aware preference learning algorithm to mitigate the risk of low quality within generated preference data.\nOur experimental results demonstrate that the proposed framework significantly boosts the alignment of LLMs.\nFor example, we achieve superior alignment performance on AlpacaEval 2.0 with only 3.3% of the ground-truth preference labels in the Ultrafeedback data compared to the cases using the entire data or state-of-the-art baselines.Primary Area:generative modelsCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:12928",
    "authors": "Dongyoung Kim, Kimin Lee, Jinwoo Shin, Jaehyung Kim",
    "url": "https://openreview.net/forum?id=BPgK5XW1Nb",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Brain Bandit: A Biologically Grounded Neural Network for Efficient Control of Exploration",
    "abstract": "Keywords:explore-exploit, stochastic Hopfield network, Thompson sampling, decision under uncertainty, brain-inspired algorithm, reinforcement learningTL;DR:We demonstrate that a brain-inspired stochastic Hopfield network can achieve efficient, human-like, uncertainty-aware exploration in bandit and MDP tasks.Abstract:How to balance between exploration and exploitation in an uncertain environment is a central challenge in reinforcement learning. In contrast, humans and animals have demonstrated superior exploration efficiency in novel environments. To understand how the brain’s neural network controls exploration under uncertainty, we analyzed the dynamical systems model of a biological neural network that controls explore-exploit decisions during foraging. Mathematically, this model (named the Brain Bandit Net, or BBN) is a special type of stochastic continuous Hopfield network. We show through theory and simulation that BBN can perform posterior sampling of action values with a tunable bias towards or against uncertain options. We then demonstrate that, in multi-armed bandit (MAB) tasks, BBN can generate probabilistic choice behavior with a flexible uncertainty bias resembling human and animal choice patterns. In addition to its high efficiency in MAB tasks, BBN can also be embedded with reinforcement learning algorithms to accelerate learning in MDP tasks. Altogether, our findings reveal the theoretical foundation for efficient exploration in biological neural networks and propose a general, brain-inspired algorithm for enhancing exploration in RL.Primary Area:applications to neuroscience & cognitive scienceCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:12774",
    "authors": "Chen Jiang, Jiahui An, Yating Liu, Ni Ji",
    "url": "https://openreview.net/forum?id=RWJX5F5I9g",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "MaestroMotif: Skill Design from Artificial Intelligence Feedback",
    "abstract": "Keywords:Hierarchical RL, Reinforcement Learning, LLMsTL;DR:A method for AI-assisted skill design via Motif and LLM code generation, solving tasks zero-shot from language descriptions on NetHack.Abstract:Describing skills in natural language has the potential to provide an accessible way to inject human knowledge about decision-making into an AI system. We present MaestroMotif, a method for AI-assisted skill design, which yields high-performing and adaptable agents. MaestroMotif leverages the capabilities of Large Language Models (LLMs) to effectively create and reuse skills. It first uses an LLM's feedback to automatically design rewards corresponding to each skill, starting from their natural language description. Then, it employs an LLM's code generation abilities, together with reinforcement learning, for training the skills and combining them to implement complex behaviors specified in language. We evaluate MaestroMotif using a suite of complex tasks in the NetHack Learning Environment (NLE), demonstrating that it surpasses existing approaches in both performance and usability.Primary Area:reinforcement learningCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:12735",
    "authors": "Martin Klissarov, Mikael Henaff, Roberta Raileanu, Shagun Sodhani, Pascal Vincent, Amy Zhang, Pierre-Luc Bacon, Doina Precup, Marlos C. Machado, Pierluca D'Oro",
    "url": "https://openreview.net/forum?id=or8mMhmyRV",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Learning to Discover Regulatory Elements for Gene Expression Prediction",
    "abstract": "Keywords:Gene Expression, Deep Learning, Sequence ModelingAbstract:We consider the problem of predicting gene expressions from DNA sequences. A key challenge of this task is to find the regulatory elements that control gene expressions. Here, we introduce Seq2Exp, a Sequence to Expression network explicitly designed to discover and extract regulatory elements that drive target gene expression, enhancing the accuracy of the gene expression prediction. Our approach captures the causal relationship between epigenomic signals, DNA sequences and their associated regulatory elements. Specifically, we propose to decompose the epigenomic signals and the DNA sequence conditioned on the causal active regulatory elements, and apply an information bottleneck with the Beta distribution to combine their effects while filtering out non-causal components. Our experiments demonstrate that Seq2Exp outperforms existing baselines in gene expression prediction tasks and discovers influential regions compared to commonly used statistical methods for peak detection such as MACS3. The source code is released as part of the AIRS library (https://github.com/divelab/AIRS/).Primary Area:applications to physical sciences (physics, chemistry, biology, etc.)Code Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:12644",
    "authors": "Xingyu Su, Haiyang Yu, Degui Zhi, Shuiwang Ji",
    "url": "https://openreview.net/forum?id=Mfnh1Sqdwf",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models",
    "abstract": "Keywords:Diffusion Models, Text Diffusion, Generative ModelsAbstract:Diffusion language models offer unique benefits over autoregressive models due to their potential for parallelized generation and controllability, yet they lag in likelihood modeling and are limited to fixed-length generation. In this work, we introduce a class of block diffusion language models that interpolate between discrete denoising diffusion and autoregressive models. Block diffusion overcomes key limitations of both approaches by supporting flexible-length generation and improving inference efficiency with KV caching and parallel token sampling. We propose a recipe for building effective block diffusion models that includes an efficient training algorithm, estimators of gradient variance, and data-driven noise schedules to minimize the variance. Block diffusion sets a new state-of-the-art performance among diffusion models on language modeling benchmarks and enables generation of arbitrary-length sequences. We provide the code, along with the model weights and blog post on the project page: https://m-arriola.com/bd3lms/Primary Area:generative modelsCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:12566",
    "authors": "Marianne Arriola, Aaron Gokaslan, Justin T Chiu, Zhihan Yang, Zhixuan Qi, Jiaqi Han, Subham Sekhar Sahoo, Volodymyr Kuleshov",
    "url": "https://openreview.net/forum?id=tyEyYT267x",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Syntactic and Semantic Control of Large Language Models via Sequential Monte Carlo",
    "abstract": "Keywords:Sequential Monte Carlo, Language Models, Semantic parsing, Bayesian inference, Probabilistic programming, SMCTL;DR:We introduce a sequential Monte Carlo framework for controlling LMs at inference time via both syntactic and semantic constraints.Abstract:A wide range of LM applications require generating text that conforms to syntactic or semantic constraints. Imposing such constraints can be naturally framed as _probabilistic conditioning_, but exact generation from the resulting distribution—which can differ substantially from the LM’s base distribution—is generally intractable. In this work,\nwe develop an architecture for controlled LM generation based on sequential Monte Carlo (SMC). Our SMC framework allows us to flexibly incorporate domain- and problem-specific constraints at inference time, and efficiently reallocate computational resources in light of new information during the course of generation. By comparing to a number of alternatives and ablations on four challenging domains---Python code generation for data science, text-to-SQL, goal inference, and molecule synthesis—we demonstrate that, with little overhead, our approach allows small open-source language models to outperform models over 8$\\times$ larger, as well as closed-source, fine-tuned ones. \nIn support of the probabilistic perspective, we show that these performance improvements are driven by better approximation to the posterior distribution. \n[Our system](https://github.com/probcomp/genlm-control) builds on the framework of Lew et al. (2023) and integrates with its _language model probabilistic programming language_, giving users a simple, programmable way to apply SMC to a broad variety of controlled generation problems.Primary Area:probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)Code Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:12536",
    "authors": "João Loula, Benjamin LeBrun, Li Du, Ben Lipkin, Clemente Pasti, Gabriel Grand, Tianyu Liu, Yahya Emara, Marjorie Freedman, Jason Eisner, Ryan Cotterell, Vikash Mansinghka, Alexander K. Lew, Tim Vieira, Timothy J. O'Donnell",
    "url": "https://openreview.net/forum?id=xoXn62FzD0",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Scaling Laws for Precision",
    "abstract": "Keywords:quantization, scaling laws, precision, language modelsTL;DR:We model the effects of precision on language model loss scaling, both during and after training. We find that overtrained models degrade more when quantized at inference time, and that training larger models in lower precision can be optimal.Abstract:Low precision training and inference affect both the quality and cost of language models, but current scaling laws do not account for this. In this work, we devise \"precision-aware\" scaling laws for both training and inference. We propose that training in lower precision reduces the model's \"effective parameter count,\" allowing us to predict the additional loss incurred from training in low precision and post-train quantization. For inference, we find that the degradation introduced by post-training quantization increases as models are trained on more data, eventually making additional pretraining data actively harmful. For training, our scaling laws allow us to predict the loss of a model with different parts in different precisions, and suggest that training larger models in lower precision can be compute optimal.  We unify the scaling laws for post and pretraining quantization to arrive at a single functional form that predicts degradation from training and inference in varied precisions. We fit on over 465 pretraining runs and validate our predictions on model sizes up to 1.7B parameters trained on up to 26B tokens.Primary Area:foundation or frontier models, including LLMsCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:12529",
    "authors": "Tanishq Kumar, Zachary Ankner, Benjamin Frederick Spector, Blake Bordelon, Niklas Muennighoff, Mansheej Paul, Cengiz Pehlevan, Christopher Re, Aditi Raghunathan",
    "url": "https://openreview.net/forum?id=wg1PCg3CUP",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Context-Parametric Inversion: Why Instruction Finetuning May Not Actually Improve Context Reliance",
    "abstract": "Keywords:Instruction finetuning, context-vs-parametric relianceTL;DR:We highlight a surprising phenomenon, where the context reliance of the model decreases unexpectedly, with instruction finetuning, despite an initial increase.Abstract:Large Language Model's are instruction-finetuned to enhance their ability to follow user instructions and better comprehend input context. Still, they often struggle to follow the input context, especially when it contradicts model's parametric knowledge. This manifests as various failures, such as hallucinations where a model inserts outdated or unwarranted facts into its response. In this work, we observe an intriguing phenomenon: the context reliance of the model decreases as instruction finetuning progresses, $\\textit{despite an initial expected increase}$. We call this phenomenon as the $\\textbf{context-parametric inversion}$. This is surprising, as one would expect instruction tuning to improve the model's ability to follow input instructions.  We observe this behavior on multiple general purpose instruction tuning datasets such as TULU, Alpaca and Ultrachat, across multiple model families like Llama, Mistral and Pythia.  We perform various controlled studies to eliminate some simple hypothesis for this observed behavior and isolate what datapoints cause this counter-intuitive behavior. We then analyze the phenomenon theoretically, to explain why context reliance varies across the trajectory of finetuning. \nWe tie the observed context-parametric inversion to the properties of the finetuning data, which provides us with some potential mitigation strategies that provide limited but insightful gains.Supplementary Material:zipPrimary Area:foundation or frontier models, including LLMsCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:12499",
    "authors": "Sachin Goyal, Christina Baek, J Zico Kolter, Aditi Raghunathan",
    "url": "https://openreview.net/forum?id=SPS6HzVzyt",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Inference Scaling for Long-Context Retrieval Augmented Generation",
    "abstract": "Keywords:inference scaling, long-context LLM, retrieval augmented generationAbstract:The scaling of inference computation has unlocked the potential of long-context large language models (LLMs) across diverse settings. For knowledge-intensive tasks, the increased compute is often allocated to incorporate more external knowledge.  However, without effectively utilizing such knowledge, solely expanding context does not always enhance performance. In this work, we investigate inference scaling for retrieval augmented generation (RAG), exploring the combination of multiple strategies beyond simply increasing the quantity of knowledge, including in-context learning and iterative prompting. These strategies provide additional flexibility to scale test-time computation (e.g., by increasing retrieved documents or generation steps), thereby enhancing LLMs’ ability to effectively acquire and utilize contextual information. We address two key questions: (1) How does RAG performance benefit from the scaling of inference computation when optimally configured? (2) Can we predict the optimal test-time compute allocation for a given budget by modeling the relationship between RAG performance and inference parameters? Our observations reveal that increasing inference computation leads to nearly linear gains in RAG performance when optimally allocated, a relationship we describe as the inference scaling laws for RAG. Building on this,  we further develop the computation allocation model to estimate RAG performance across different inference configurations.  The model predicts optimal inference parameters under various computation constraints, which align closely with the experimental results. By applying these optimal configurations, we demonstrate that scaling inference compute on long-context LLMs achieves up to 58.9% gains on benchmark datasets compared to standard RAG.Primary Area:other topics in machine learning (i.e., none of the above)Code Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:12199",
    "authors": "Zhenrui Yue, Honglei Zhuang, Aijun Bai, Kai Hui, Rolf Jagerman, Hansi Zeng, Zhen Qin, Dong Wang, Xuanhui Wang, Michael Bendersky",
    "url": "https://openreview.net/forum?id=FSjIrOm1vz",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Scaling LLM Test-Time Compute Optimally Can be More Effective than Scaling Parameters for Reasoning",
    "abstract": "Keywords:test-time compute, LLMs, scaling, language modelsTL;DR:We find that by optimally scaling test-time compute we can outperform much larger models in a FLOPs matched evaluation.Abstract:Enabling LLMs to improve their outputs by using more test-time compute is a critical step towards building self-improving agents that can operate on open-ended natural language. In this paper, we scale up inference-time computation in LLMs, with a focus on answering: if an LLM is allowed to use a fixed but non-trivial amount of inference-time compute, how much can it improve its performance on a challenging prompt? Answering this question has implications not only on performance, but also on the future of LLM pretraining and how to tradeoff inference-time and pre-training compute. Little research has attempted to understand the scaling behaviors of test-time inference methods, with current work largely providing negative results for a number of these strategies. In this work, we analyze two primary mechanisms to scale test-time computation: (1) searching against dense, process-based verifier reward models (PRMs); and (2) updating the model's distribution over a response adaptively, given the prompt at test time. We find that in both cases, the effectiveness of different approaches to scaling test-time compute critically varies depending on the difficulty of the prompt. This observation motivates applying a \"compute-optimal\" scaling strategy, which acts to, as effectively as possible, allocate test-time compute per prompt in an adaptive manner. Using this compute-optimal strategy, we can improve the efficiency of test-time compute scaling for math reasoning problems by more than 4x compared to a best-of-N baseline. Additionally, in a FLOPs-matched evaluation, we find that on problems where a smaller base model attains somewhat non-trivial success rates, test-time compute  can be used to outperform a 14x larger model.Primary Area:foundation or frontier models, including LLMsCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:12182",
    "authors": "Charlie Victor Snell, Jaehoon Lee, Kelvin Xu, Aviral Kumar",
    "url": "https://openreview.net/forum?id=4FWAwZtd2n",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Capturing the Temporal Dependence of Training Data Influence",
    "abstract": "Keywords:data attributionTL;DR:We introduce data value embedding, a novel framework for real-time data attribution technique that approximates trajectory-specific leave-one-out (LOO) error.Abstract:Traditional data influence estimation methods, like influence function, assume that learning algorithms are permutation-invariant with respect to training data. However, modern training paradigms—especially for foundation models using stochastic algorithms and non-convergent, multi-stage curricula—are sensitive to data ordering, thus violating this assumption. This mismatch renders influence functions inadequate for answering some critical questions in current machine learning: How can we differentiate the influence of the same data contributing at different stages of training? More generally, how can we capture the dependence of data influence on the optimization trajectory during training? To address this gap, we formalize the concept of \\emph{trajectory-specific leave-one-out (LOO) influence}, which quantifies the impact of removing a data point from a specific iteration during training, accounting for the exact sequence of data encountered and the model's optimization trajectory. However, exactly evaluating the trajectory-specific LOO presents a significant computational challenge. To address this, we propose \\emph{data value embedding}, a novel technique enabling efficient approximation of trajectory-specific LOO. Specifically, we compute a training data embedding that encapsulates the cumulative interactions between data and the evolving model parameters. The LOO can then be efficiently approximated through a simple dot-product between the data value embedding and the gradient of the given test data. As data value embedding captures training data ordering, it offers valuable insights into model training dynamics. In particular, we uncover distinct phases of data influence, revealing that data points in the early and late stages of training exert a greater impact on the final model. These insights translate into actionable strategies for managing the computational overhead of data selection by strategically timing the selection process, potentially opening new avenues in data curation research.Primary Area:interpretability and explainable AICode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:12172",
    "authors": "Jiachen T. Wang, Dawn Song, James Zou, Prateek Mittal, Ruoxi Jia",
    "url": "https://openreview.net/forum?id=uHLgDEgiS5",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Self-Improvement in Language Models: The Sharpening Mechanism",
    "abstract": "Keywords:Learning theory, Sample complexity, Self-Improvement, Language ModelsTL;DR:We offer a new theoretical perspective on the possibility of self-improvement in language models.Abstract:Recent work in language modeling has raised the possibility of “self-improvement,” where an LLM evaluates and refines its own generations to achieve higher performance without external feedback. It is impossible for this self-improvement to create information that is not already in the model, so why should we expect that this will lead to improved capabilities? We offer a new theoretical perspective on the capabilities of self-improvement through a lens we refer to as “sharpening.” Motivated by the observation that language models are often better at verifying response quality than they are at generating correct responses, we formalize self-improvement as using the model itself as a verifier during post-training in order to ‘sharpen’ the model to one placing large mass on high-quality sequences, thereby amortizing the expensive inference-time computation of generating good sequences. We begin by introducing a new statistical framework for sharpening in which the learner has sample access to a pre-trained base policy. Then, we analyze two natural families of self improvement algorithms based on SFT and RLHF. We find that (i) the SFT-based approach is minimax optimal whenever the initial model has sufficient coverage, but (ii) the RLHF-based approach can improve over SFT-based self- improvement by leveraging online exploration, bypassing the need for coverage. We view these findings as a starting point toward a foundational understanding that can guide the design and evaluation of self-improvement algorithms.Primary Area:learning theoryCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:12101",
    "authors": "Audrey Huang, Adam Block, Dylan J Foster, Dhruv Rohatgi, Cyril Zhang, Max Simchowitz, Jordan T. Ash, Akshay Krishnamurthy",
    "url": "https://openreview.net/forum?id=WJaUkwci9o",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Data Shapley in One Training Run",
    "abstract": "Keywords:Shapley value, data valuation.TL;DR:We develop a new notion of Data Shapley that requires only one model training run.Abstract:Data Shapley offers a principled framework for attributing the contribution of data within machine learning contexts. However, the traditional notion of Data Shapley requires re-training models on various data subsets, which becomes computationally infeasible for large-scale models. Additionally, this retraining-based definition cannot evaluate the contribution of data for a specific model training run, which may often be of interest in practice. This paper introduces a novel concept, In-Run Data Shapley, which eliminates the need for model retraining and is specifically designed for assessing data contribution for a particular model of interest. In-Run Data Shapley calculates the Shapley value for each gradient update iteration and accumulates these values throughout the training process. We present several techniques that allow the efficient scaling of In-Run Data Shapley to the size of foundation models. In its most optimized implementation, our method adds negligible runtime overhead compared to standard model training. This dramatic efficiency improvement makes it possible to perform data attribution for the foundation model pretraining stage. We present several case studies that offer fresh insights into pretraining data's contribution and discuss their implications for copyright in generative AI and pretraining data curation.Primary Area:interpretability and explainable AICode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:12092",
    "authors": "Jiachen T. Wang, Prateek Mittal, Dawn Song, Ruoxi Jia",
    "url": "https://openreview.net/forum?id=HD6bWcj87Y",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Computationally Efficient RL under Linear Bellman Completeness for Deterministic Dynamics",
    "abstract": "Keywords:reinforcement learning theory, linear function approximationAbstract:We study computationally and statistically efficient Reinforcement Learning algorithms for the *linear Bellman Complete* setting. This setting uses linear function approximation to capture value functions and unifies existing models like linear Markov Decision Processes (MDP) and Linear Quadratic Regulators (LQR).  While it is known from the prior works that this setting is statistically tractable, it remained open whether a computationally efficient algorithm exists. Our work provides a computationally efficient algorithm for the linear Bellman complete setting that works for MDPs with large action spaces, random initial states, and random rewards but relies on the underlying dynamics to be deterministic. Our approach is based on randomization: we inject random noise into least squares regression problems to perform optimistic value iteration. Our key technical contribution is to carefully design the noise to only act in the null space of the training data to ensure optimism while circumventing a subtle error amplification issue.Supplementary Material:pdfPrimary Area:reinforcement learningCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:12025",
    "authors": "Runzhe Wu, Ayush Sekhari, Akshay Krishnamurthy, Wen Sun",
    "url": "https://openreview.net/forum?id=hyfe5q5TD0",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "How new data permeates LLM knowledge and how to dilute it",
    "abstract": "Keywords:fine-tuning, hallucinations, knowledge injection, memory, LLMsAbstract:Large language models continually learn through the accumulation of gradient-based updates, but how individual pieces of new information affect existing knowledge, leading to both beneficial generalization and problematic hallucination, remains poorly understood. We demonstrate that when learning new information, LLMs exhibit a \"priming\" effect: learning a new fact can cause the model to inappropriately apply that knowledge in unrelated contexts.\nTo systematically study this phenomenon, we introduce \"Outlandish,\" a carefully curated dataset of 1320 diverse text samples designed to probe how new knowledge permeates through an LLM's existing knowledge base. Using this dataset, we show that the degree of priming after learning new information can be predicted by measuring the token probability of key words before training. This relationship holds robustly across different model architectures (PALM-2, Gemma, Llama), sizes, and training stages.\nFinally, we develop two novel techniques to modulate how new knowledge affects existing model behavior: (1) a ``stepping-stone'' text augmentation strategy and (2) an ``ignore-k'' update pruning method. These approaches reduce undesirable priming effects by 50-95% while preserving the model's ability to learn new information. Our findings provide both empirical insights into how LLMs learn and practical tools for improving the specificity of knowledge insertion in language models. Further materials: https://sunchipsster1.github.io/projects/outlandish/Primary Area:interpretability and explainable AICode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:13888",
    "authors": "Chen Sun, Renat Aksitov, Andrey Zhmoginov, Nolan Andrew Miller, Max Vladymyrov, Ulrich Rueckert, Been Kim, Mark Sandler",
    "url": "https://openreview.net/forum?id=NGKQoaqLpo",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "UniCBE: An Uniformity-driven Comparing Based Evaluation Framework with Unified Multi-Objective Optimization",
    "abstract": "Keywords:evaluation, efficient, scalability, accuracy, convergenceTL;DR:We propose UniCBE, a comparing-based evaluation framework with better scalability, accuracy and convergence.Abstract:Human preference plays a significant role in measuring large language models and guiding them to align with human values. Unfortunately, current comparing-based evaluation (CBE) methods typically focus on a single optimization objective, failing to effectively utilize scarce yet valuable preference signals. To address this, we delve into key factors that can enhance the accuracy, convergence, and scalability of CBE: suppressing sampling bias, balancing descending process of uncertainty, and mitigating updating uncertainty.\nFollowing the derived guidelines, we propose UniCBE, a unified uniformity-driven CBE framework which simultaneously optimize these core objectives by constructing and integrating three decoupled sampling probability matrices, each designed to ensure uniformity in specific aspects. We further ablate the optimal tuple sampling and preference aggregation strategies to achieve efficient CBE.\nOn the AlpacaEval benchmark, UniCBE saves over 17% of evaluation budgets while achieving a Pearson correlation with ground truth exceeding 0.995, demonstrating excellent accuracy and convergence. In scenarios where new models are continuously introduced, UniCBE can even save over 50% of evaluation costs, highlighting its improved scalability.Supplementary Material:zipPrimary Area:applications to computer vision, audio, language, and other modalitiesCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:13837",
    "authors": "Peiwen Yuan, Shaoxiong Feng, Yiwei Li, Xinglin Wang, Yueqi Zhang, Jiayi Shi, Chuyi Tan, Boyuan Pan, Yao Hu, Kan Li",
    "url": "https://openreview.net/forum?id=rpwGUtTeA5",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "NetMoE: Accelerating MoE Training through Dynamic Sample Placement",
    "abstract": "Keywords:Mixture of Experts, All-to-All communication, Distributed trainingAbstract:Mixture of Experts (MoE) is a widely used technique to expand model sizes for better model quality while maintaining the computation cost constant. In a nutshell, an MoE model consists of multiple experts in each model layer and routes the training tokens to only a fixed number of experts rather than all. In distributed training, as experts are distributed among different GPUs, All-to-All communication is necessary to exchange the training tokens among the GPUs after each time of expert routing. Due to the frequent and voluminous data exchanges, All-to-All communication has become a notable challenge to training efficiency.\n\nIn this paper, we manage to accelerate All-to-All communication in MoE models from the training sample perspective, which is unexplored so far. In particular, we put forward the observation that tokens in the same training sample have certain levels of locality in expert routing. Motivated by this, we develop NetMoE, which takes such locality into account and dynamically rearranges the placement of training samples to minimize All-to-All communication costs. Specifically, we model the All-to-All communication given the sample placement and formulate an integer programming problem to deduce the optimal placement in polynomial time. Experiments with 32 GPUs show that NetMoE achieves a maximum efficiency improvement of $1.67 \\times$ compared with current MoE training frameworks.Primary Area:infrastructure, software libraries, hardware, systems, etc.Code Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:13820",
    "authors": "Xinyi Liu, Yujie Wang, Fangcheng Fu, Xupeng Miao, Shenhan Zhu, Xiaonan Nie, Bin CUI",
    "url": "https://openreview.net/forum?id=1qP3lsatCR",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "TabReD: Analyzing Pitfalls and Filling the Gaps in Tabular Deep Learning Benchmarks",
    "abstract": "Keywords:Tabular Data, Benchmarks, Reality Check, Tabular Deep Learning, ApplicationsTL;DR:We introduce TabReD, a collection of industry-grade tabular datasets, filling the gaps in academic benchmarks. Our evaluation reveals performance differences for various models and techniques in a new setting.Abstract:Advances in machine learning research drive progress in real-world applications. \nTo ensure this progress, it is important to understand the potential pitfalls on the way from a novel method's success on academic benchmarks to its practical deployment. In this work, we analyze existing tabular deep learning benchmarks and find two common characteristics of tabular data in typical industrial applications that are underrepresented in the datasets usually used for evaluation in the literature.\nFirst, in real-world deployment scenarios, distribution of data often changes over time. To account for this distribution drift, time-based train/test splits should be used in evaluation. However, existing academic tabular datasets often lack timestamp metadata to enable such evaluation.\nSecond, a considerable portion of datasets in production settings stem from extensive data acquisition and feature engineering pipelines. This can have an impact on the absolute and relative number of predictive, uninformative, and correlated features compared to academic datasets.\nIn this work, we aim to understand how recent research advances in tabular deep learning transfer to these underrepresented conditions.\nTo this end, we introduce TabReD -- a collection of eight industry-grade tabular datasets. \nWe reassess a large number of tabular ML models and techniques on TabReD. We demonstrate that evaluation on both time-based data splits and richer feature sets leads to different methods ranking, compared to evaluation on random splits and smaller number of features, which are common in academic benchmarks. Furthermore, simple MLP-like architectures and GBDT show the best results on the TabReD datasets, while other methods are less effective in the new setting.Supplementary Material:zipPrimary Area:datasets and benchmarksCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:13784",
    "authors": "Ivan Rubachev, Nikolay Kartashev, Yury Gorishniy, Artem Babenko",
    "url": "https://openreview.net/forum?id=L14sqcrUC3",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Improving the Sparse Structure Learning of Spiking Neural Networks from the View of Compression Efficiency",
    "abstract": "Keywords:spiking neural networksAbstract:The human brain utilizes spikes for information transmission and dynamically reorganizes its network structure to boost energy efficiency and cognitive capabilities throughout its lifespan. Drawing inspiration from this spike-based computation, Spiking Neural Networks (SNNs) have been developed to construct event-driven models that emulate this efficiency. Despite these advances, deep SNNs continue to suffer from over-parameterization during training and inference, a stark contrast to the brain’s ability to self-organize. Furthermore, existing sparse SNNs are challenged by maintaining optimal pruning levels due to a static pruning ratio, resulting in either under or over-pruning.\nIn this paper, we propose a novel two-stage dynamic structure learning approach for deep SNNs, aimed at maintaining effective sparse training from scratch while optimizing compression efficiency. \nThe first stage evaluates the compressibility of existing sparse subnetworks within SNNs using the PQ index, which facilitates an adaptive determination of the rewiring ratio for synaptic connections based on data compression insights. In the second stage, this rewiring ratio critically informs the dynamic synaptic connection rewiring process, including both pruning and regrowth. This approach significantly improves the exploration of sparse structures training in deep SNNs, adapting sparsity dynamically from the point view of compression efficiency.\nOur experiments demonstrate that this sparse training approach not only aligns with the performance of current deep SNNs models but also significantly improves the efficiency of compressing sparse SNNs. Crucially, it preserves the advantages of initiating training with sparse models and offers a promising solution for implementing Edge AI on neuromorphic hardware.Supplementary Material:pdfPrimary Area:applications to neuroscience & cognitive scienceCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:13685",
    "authors": "Jiangrong Shen, Qi Xu, Gang Pan, Badong Chen",
    "url": "https://openreview.net/forum?id=gcouwCx7dG",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "JudgeLM: Fine-tuned Large Language Models are Scalable Judges",
    "abstract": "Keywords:LLM JudgingTL;DR:We propose a high-quality LLM judge dataset and a series of strong LLM judges to address the biases in LLM judging.Abstract:Evaluating Large Language Models (LLMs) in open-ended scenarios is challenging because existing benchmarks and metrics can not measure them comprehensively. To address this problem, we propose to fine-tune LLMs as scalable judges (JudgeLM) to evaluate LLMs efficiently and effectively in open-ended benchmarks. We first propose a comprehensive, large-scale, high-quality dataset containing task seeds, LLMs-generated answers, and GPT-4-generated judgments for fine-tuning high-performance judges, as well as a new benchmark for evaluating the judges. We train JudgeLM at different scales from 7B, 13B, to 33B parameters, and conduct a systematic analysis of its capabilities and behaviors. We then analyze the key biases in fine-tuning LLM as a judge and consider them as position bias, knowledge bias, and format bias. To address these issues, JudgeLM introduces a bag of techniques including swap augmentation, reference support, and reference drop, which clearly enhance the judge's performance. JudgeLM obtains the state-of-the-art judge performance on both the existing PandaLM benchmark and our proposed new benchmark. Our JudgeLM is efficient and the JudgeLM-7B only needs 3 minutes to judge 5K samples with 8 A100 GPUs. JudgeLM obtains high agreement with the teacher judge, achieving an agreement exceeding 90% that even surpasses human-to-human agreement. JudgeLM also demonstrates extended capabilities in being judges of the single answer, multimodal models, multiple answers, multi-turn chat, etc.Primary Area:alignment, fairness, safety, privacy, and societal considerationsCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:13655",
    "authors": "Lianghui Zhu, Xinggang Wang, Xinlong Wang",
    "url": "https://openreview.net/forum?id=xsELpEPn4A",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Direct Post-Training Preference Alignment for Multi-Agent Motion Generation Model Using Implicit Feedback from Pre-training Demonstrations",
    "abstract": "Keywords:Efficient Post-training Preference Alignment, Alignment from demonstrations, Multi-agent Motion GenerationTL;DR:We propose an efficient post-training alignment approach that significantly improves the pre-trained motion generation model’s quality without requiring additional post-training human preference annotation or expansive compute.Abstract:Recent advancements in Large Language Models (LLMs) have revolutionized motion generation models in embodied applications such as autonomous driving and robotic manipulation. While LLM-type auto-regressive motion generation models benefit from training scalability, there remains a discrepancy between their token prediction objectives and human preferences. As a result, models pre-trained solely with token-prediction objectives often generate behaviors that deviate from what humans would prefer, making post-training preference alignment crucial for producing human-preferred motions. Unfortunately, post-training alignment requires extensive preference rankings of motions generated by the pre-trained model, which are costly and time-consuming to annotate, especially in multi-agent motion generation settings. Recently, there has been growing interest in leveraging expert demonstrations previously used during pre-training to scalably generate preference data for post-training alignment. However, these methods often adopt an adversarial assumption, treating all pre-trained model-generated samples as unpreferred examples and relying solely on pre-training expert demonstrations to construct preferred examples. This adversarial approach overlooks the valuable signal provided by preference rankings among the model's own generations, ultimately reducing alignment effectiveness and potentially leading to misaligned behaviors. In this work, instead of treating all generated samples as equally bad, we propose a principled approach that leverages implicit preferences encoded in pre-training expert demonstrations to construct preference rankings among the pre-trained model's generations, offering more nuanced preference alignment guidance with zero human cost. We apply our approach to large-scale traffic simulation (more than 100 agents) and demonstrate its effectiveness in improving the realism of pre-trained model's generated behaviors, making a lightweight 1M motion generation model comparable to state-of-the-art large imitation-based models by relying solely on implicit feedback from pre-training demonstrations, without requiring additional post-training human preference annotations or incurring high computational costs. Furthermore, we provide an in-depth analysis of preference data scaling laws and their effects on over-optimization, offering valuable insights for future studies.Supplementary Material:zipPrimary Area:applications to robotics, autonomy, planningCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:13515",
    "authors": "Thomas Tian, Kratarth Goel",
    "url": "https://openreview.net/forum?id=8UFG9D8xeU",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Online Preference Alignment for Language Models via Count-based Exploration",
    "abstract": "Keywords:Reinforcement Learning from Human Feedback, RLHF, Preference Alignment, Exploration, LLMsTL;DR:We propose count-based online preference optimization for LLM alignment that leverages coin-flip counting to encourage exploration in online RLHF.Abstract:Reinforcement Learning from Human Feedback (RLHF) has shown great potential in fine-tuning Large Language Models (LLMs) to align with human preferences. Existing methods perform preference alignment from a fixed dataset, which can be limited in data coverage and the resulting reward model is hard to generalize in out-of-distribution responses. Thus, online RLHF is more desirable to empower the LLM to explore outside the support of the initial dataset by iteratively collecting the prompt-response pairs. In this paper, we study the fundamental problem in online RLHF, i.e., how to explore for LLM. We give a theoretical motivation in linear reward assumption to show that an optimistic reward with an upper confidence bound (UCB) term leads to a provably efficient RLHF policy. Then, we reformulate our objective to direct preference optimization with an exploration term, where the UCB-term can be converted to a count-based exploration bonus. We further propose a practical algorithm, named Count-based Online Preference Optimization (COPO), which leverages a simple coin-flip counting module to estimate the pseudo-count of a prompt-response pair in previously collected data. COPO encourages LLMs to balance exploration and preference optimization in an iterative manner, which enlarges the exploration space and the entire data coverage of iterative LLM policies. We conduct online RLHF experiments on Zephyr and Llama-3 models. The results on instruction-following and standard academic benchmarks show that COPO significantly increases performance.Primary Area:foundation or frontier models, including LLMsCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:13482",
    "authors": "Chenjia Bai, Yang Zhang, Shuang Qiu, Qiaosheng Zhang, Kang Xu, Xuelong Li",
    "url": "https://openreview.net/forum?id=cfKZ5VrhXt",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Internet of Agents: Weaving a Web of Heterogeneous Agents for Collaborative Intelligence",
    "abstract": "Keywords:llm agent, multi-agentTL;DR:We propose IoA, a novel framework inspired by the Internet for effective collaboration among diverse LLM agents. IoA enables autonomous conversation flow, integration of heterogeneous agents, etc. It outperforms SoTA baselines in various tasks.Abstract:The rapid advancement of large language models (LLMs) has paved the way for the development of highly capable autonomous agents. However, existing multi-agent frameworks often struggle with integrating diverse capable third-party agents due to reliance on agents defined within their own ecosystems. They also face challenges in simulating distributed environments, as most frameworks are limited to single-device setups. Furthermore, these frameworks often rely on hard-coded communication pipelines, limiting their adaptability to dynamic task requirements. Inspired by the concept of the Internet, we propose the Internet of Agents (IoA), a novel framework that addresses these limitations by providing a flexible and scalable platform for LLM-based multi-agent collaboration. IoA introduces an agent integration protocol, an instant-messaging-like architecture design, and dynamic mechanisms for agent teaming and conversation flow control. Through extensive experiments on general assistant tasks, embodied AI tasks, and retrieval-augmented generation benchmarks, we demonstrate that IoA consistently outperforms state-of-the-art baselines, showcasing its ability to facilitate effective collaboration among heterogeneous agents. IoA represents a step towards linking diverse agents in an Internet-like environment, where agents can seamlessly collaborate to achieve greater intelligence and capabilities. We will release our code to facilitate further research.Primary Area:applications to computer vision, audio, language, and other modalitiesCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:13450",
    "authors": "Weize Chen, Ziming You, Ran Li, yitong guan, Chen Qian, Chenyang Zhao, Cheng Yang, Ruobing Xie, Zhiyuan Liu, Maosong Sun",
    "url": "https://openreview.net/forum?id=o1Et3MogPw",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning",
    "abstract": "Keywords:LLM, Math Reasoning, Process Supervision, Reward Models, RL, SearchAbstract:A promising approach for improving reasoning in large language models is to use process reward models (PRMs). PRMs provide feedback at each step of a multi-step reasoning trace, improving credit assignment over outcome reward models (ORMs) that only provide feedback at the final step. However, collecting dense, per-step human labels is not scalable, and training PRMs from automatically-labeled data has thus far led to limited gains. With the goal of using PRMs to improve a *base* policy via test-time search and reinforcement learning (RL), we ask: ``How should we design process rewards?'' Our key insight is that, to be effective, the process reward for a step should measure \n *progress*: a change in the likelihood of producing a correct response in the future, before and after taking the step, as measured under a *prover* policy distinct from the base policy. Such progress values can {distinguish} good and bad steps generated by the base policy, even though the base policy itself cannot.  Theoretically, we show that even weaker provers can improve the base policy, as long as they distinguish steps without being too misaligned with the base policy. Our results show that process rewards defined as progress under such provers improve the efficiency of exploration during test-time search and online RL. We empirically validate our claims by training  **process advantage verifiers (PAVs)** to measure progress under such provers and show that compared to ORM, they are >8% more accurate, and 1.5-5x more compute-efficient. Equipped with these insights, our PAVs enable **one of the first results** showing a 6x gain in sample efficiency for a policy trained using online RL with PRMs vs. ORMs.Supplementary Material:zipPrimary Area:foundation or frontier models, including LLMsCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:13389",
    "authors": "Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh Agarwal, Jonathan Berant, Aviral Kumar",
    "url": "https://openreview.net/forum?id=A6Y7AqlzLW",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "MQuAKE-Remastered: Multi-Hop Knowledge Editing Can Only Be Advanced with Reliable Evaluations",
    "abstract": "Keywords:knowledge edit, model edit, multi-hop, question answering, natural language processing, dataset auditTL;DR:Updating one knowledge fact will produce a ripple effect, making multi-hop knowledge editing (MHKE) a desired capability for reliable LLMs. We reveal many unknown errors of MQuAKE — the most popular MHKE dataset — though an audit and fix everything.Abstract:Large language models (LLMs) can give out erroneous answers to factually rooted questions either as a result of undesired training outcomes or simply because the world has moved on after a certain knowledge cutoff date. Under such scenarios, *knowledge editing* often comes to the rescue by delivering efficient patches for such erroneous answers without significantly altering the rest, where many editing methods have seen reasonable success when the editing targets are simple and direct (e.g., *``what club does Lionel Messi currently play for?''*).\n\nHowever, knowledge fragments like this are often deeply intertwined in the real world, making effectively propagating the editing effect to non-directly related questions a practical challenge (to entertain an extreme example: [*\"What car did the wife of the owner of the club that Messi currently plays for used to get to school in the 80s?\"*](youtube.com/watch?v=DbwiHC1Fu-E\\&t=132s)). Prior arts have coined this task as *multi-hop knowledge editing* with the most popular dataset being MQuAKE, serving as the sole evaluation benchmark for many later proposed editing methods due to the expensive nature of constructing knowledge editing datasets at scale. \n\nIn this work, we reveal that **up to 33\\% or 76\\% of \\mquake{}'s questions and ground truth labels are, in fact, corrupted in various fashions due to some unintentional clerical or procedural oversights**. Our work provides a detailed audit of MQuAKE's error pattern and a comprehensive fix without sacrificing its dataset capacity. Additionally, we benchmarked almost all proposed MQuAKE-evaluated editing methods on our post-fix dataset, **MQuAKE-Remastered**. We observe that many methods try to overfit the original MQuAKE by exploiting some dataset idiosyncrasies of MQuAKE. We provide a guideline on how to approach such datasets faithfully and show that a simple, minimally invasive approach — **GWalk** — can offer beyond SOTA editing performance without such exploitation. The MQuAKE-Remastered datasets and utilities are available at [huggingface.co/datasets/henryzhongsc/MQuAKE-Remastered](https://huggingface.co/datasets/henryzhongsc/MQuAKE-Remastered) and [github.com/henryzhongsc/MQuAKE-Remastered](https://github.com/henryzhongsc/MQuAKE-Remastered), respectively.Primary Area:datasets and benchmarksCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:13349",
    "authors": "Shaochen Zhong, Yifan Lu, Lize Shao, Bhargav Bhushanam, Xiaocong Du, Yixin Wan, Yucheng Shi, Daochen Zha, Yiwei Wang, Ninghao Liu, Kaixiong Zhou, Shuai Xu, Kai-Wei Chang, Louis Feng, Vipin Chaudhary, Xia Hu",
    "url": "https://openreview.net/forum?id=m9wG6ai2Xk",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Competition Dynamics Shape Algorithmic Phases of In-Context Learning",
    "abstract": "Keywords:In-Context Learning, Circuit Competition, Markov Chains, Training Dynamics, GeneralizationTL;DR:In-context learning consists of phases of multiple algorithmic solutions, many phenomena are explained by this decomposition.Abstract:In-Context Learning (ICL) has significantly expanded the general-purpose nature of large language models, allowing them to adapt to novel tasks using merely the inputted context. This has motivated a series of papers that analyze tractable synthetic domains and postulate precise mechanisms that may underlie ICL. However, the use of relatively distinct setups that often lack a sequence modeling nature to them makes it unclear how general the reported insights from such studies are. Motivated by this, we propose a synthetic sequence modeling task that involves learning to simulate a finite mixture of Markov chains. As we show, models trained on this task reproduce most well-known results on ICL, hence offering a unified setting for studying the concept. Building on this setup, we demonstrate we can explain a model’s behavior by decomposing it into four broad algorithms that combine a fuzzy retrieval vs. inference approach with either unigram or bigram statistics of the context. These algorithms engage in a competitive dynamics to dominate model behavior, with the precise experimental conditions dictating which algorithm ends up superseding others: e.g., we find merely varying context size or amount of training yields (at times sharp) transitions between which algorithm dictates the model behavior, revealing a mechanism that explains the transient nature of ICL. In this sense, we argue ICL is best thought of as a mixture of different algorithms, each with its own peculiarities, instead of a monolithic capability. This also implies that making general claims about ICL that hold universally across all settings may be infeasible.Primary Area:interpretability and explainable AICode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:13272",
    "authors": "Core Francisco Park, Ekdeep Singh Lubana, Hidenori Tanaka",
    "url": "https://openreview.net/forum?id=XgH1wfHSX8",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "In vivo cell-type and brain region classification via multimodal contrastive learning",
    "abstract": "Keywords:contrastive learning, electrophysiology, extracellular, multimodal, neuroscience, cell type, brain region, Neuropixels, deep learningAbstract:Current electrophysiological approaches can track the activity of many neurons, yet it is usually unknown which cell-types or brain areas are being recorded without further molecular or histological analysis. Developing accurate and scalable algorithms for identifying the cell-type and brain region of recorded neurons is thus crucial for improving our understanding of neural computation. In this work, we develop a multimodal contrastive learning approach for neural data that can be fine-tuned for different downstream tasks, including inference of cell-type and brain location. We utilize multimodal contrastive learning to jointly embed the activity autocorrelations and extracellular waveforms of individual neurons. We demonstrate that our embedding approach, Neuronal Embeddings via MultimOdal Contrastive Learning (NEMO), paired with supervised fine-tuning, achieves state-of-the-art cell-type classification for two opto-tagged datasets and brain region classification for the public International Brain Laboratory Brain-wide Map dataset. Our method represents a promising step towards accurate cell-type and brain region classification from electrophysiological recordings.Primary Area:applications to neuroscience & cognitive scienceCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:13258",
    "authors": "Han Yu, Hanrui Lyu, YiXun Xu, Charlie Windolf, Eric Kenji Lee, Fan Yang, Andrew M Shelton, Olivier Winter, International Brain Laboratory, Eva L Dyer, Chandramouli Chandrasekaran, Nicholas A. Steinmetz, Liam Paninski, Cole Lincoln Hurwitz",
    "url": "https://openreview.net/forum?id=10JOlFIPjt",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Physics of Language Models: Part 3.3, Knowledge Capacity Scaling Laws",
    "abstract": "Keywords:scaling laws, knowledge capacity, language modelsAbstract:Scaling laws describe the relationship between the size of language models and their capabilities. Unlike prior studies that evaluate a model's capability via loss or benchmarks, we estimate information-theoretically the number of knowledge \\emph{bits} a model stores. We focus on factual knowledge represented as tuples, such as (USA, capital, Washington D.C.) from a Wikipedia page. Through multiple controlled datasets, we establish that language models can and only can store \\emph{2 bits of knowledge per parameter, even when quantized to int8}, and such knowledge can be flexibly extracted for downstream applications. \n\nMore broadly, we present 12 results on how (1) training duration, (2) model architecture, (3) quantization, (4) sparsity constraints such as MoE, and (5) data signal-to-noise ratio affect a model's knowledge storage capacity.Primary Area:foundation or frontier models, including LLMsCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:13207",
    "authors": "Zeyuan Allen-Zhu, Yuanzhi Li",
    "url": "https://openreview.net/forum?id=FxNNiUgtfa",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Scalable Decision-Making in Stochastic Environments through Learned Temporal Abstraction",
    "abstract": "Keywords:Sequential Decision-Making, Monte Carlo Tree Search, Temporal Abstraction, Planning, Model-based Reinforcement Learning, Offline Reinforcement LearningTL;DR:A scalable approach for sequential decision-making in high-dimensional continuous action spaces by learning macro actions and using MCTS.Abstract:Sequential decision-making in high-dimensional continuous action spaces, particularly in stochastic environments, faces significant computational challenges. We explore this challenge in the traditional offline RL setting, where an agent must learn how to make decisions based on data collected through a stochastic behavior policy. We present \\textit{Latent Macro Action Planner} (L-MAP), which addresses this challenge by learning a set of temporally extended macro-actions through a state-conditional Vector Quantized Variational Autoencoder (VQ-VAE), effectively reducing action dimensionality. L-MAP employs a (separate) learned prior model that acts as a latent transition model and allows efficient sampling of plausible actions. During planning, our approach accounts for stochasticity in both the environment and the behavior policy by using Monte Carlo tree search (MCTS). In offline RL settings, including stochastic continuous control tasks, L-MAP efficiently searches over discrete latent actions to yield high expected returns.\nEmpirical results demonstrate that L-MAP maintains low decision latency despite increased action dimensionality. Notably, across tasks ranging from continuous control with inherently stochastic dynamics to high-dimensional robotic hand manipulation, L-MAP significantly outperforms existing model-based methods and performs on par with strong model-free actor-critic baselines, highlighting the effectiveness of the proposed approach in planning in complex and stochastic environments with high-dimensional action spaces.Supplementary Material:zipPrimary Area:reinforcement learningCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:13144",
    "authors": "Baiting Luo, Ava Pettet, Aron Laszka, Abhishek Dubey, Ayan Mukhopadhyay",
    "url": "https://openreview.net/forum?id=pQsllTesiE",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Modeling Complex System Dynamics with Flow Matching Across Time and Conditions",
    "abstract": "Keywords:Flow Matching, dynamical systemsAbstract:Modeling the dynamics of complex real-world systems from temporal snapshot data is crucial for understanding phenomena such as gene regulation, climate change, and financial market fluctuations. Researchers have recently proposed a few methods based either on the Schroedinger Bridge or Flow Matching to tackle this problem, but these approaches remain limited in their ability to effectively combine data from multiple time points and different experimental settings. This integration is essential in real-world scenarios where observations from certain combinations of time points and experimental conditions are missing, either because of experimental costs or sensory failure. To address this challenge, we propose a novel method named Multi-Marginal Flow Matching (MMFM). MMFM first constructs a flow using smooth spline-based interpolation across time points and conditions and regresses it with a neural network using the classifier-free guided Flow Matching framework. This framework allows for the sharing of contextual information about the dynamics across multiple trajectories. We demonstrate the effectiveness of our method on both synthetic and real-world datasets, including a recent single-cell genomics data set with around a hundred chemical perturbations across time points. Our results show that MMFM significantly outperforms existing methods at imputing data at missing time points.Primary Area:learning on time series and dynamical systemsCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:12998",
    "authors": "Martin Rohbeck, Edward De Brouwer, Charlotte Bunne, Jan-Christian Huetter, Anne Biton, Kelvin Y. Chen, Aviv Regev, Romain Lopez",
    "url": "https://openreview.net/forum?id=hwnObmOTrV",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Joint Reward and Policy Learning with Demonstrations and Human Feedback Improves Alignment",
    "abstract": "Keywords:Alignment, Inverse Reinforcement Learning, Reinforment Learning from Human FeedbackAbstract:Aligning to human preferences and/or intentions is an important requirement for contemporary foundation models. To ensure alignment, popular approaches such as reinforcement learning with human feedback (RLHF) break down the task into three stages: (i) a model is computed with supervised fine-tuning (SFT) based upon large demonstrations data, (ii) a reward model (RM) is estimated based upon human feedback data, and (iii) reinforcement learning (RL) is used to further refine the SFT model by optimizing the estimated reward model.  Demonstrations and human feedback data reflect human user preferences in different ways. As a result, the reward model estimate obtained from only human feedback data is likely not as accurate as a reward model estimate obtained from both demonstration and human feedback data. A policy model that optimizes the reward model estimate obtained from both demonstration and human feedback data will likely exhibit better alignment performance. We introduce a tractable algorithm for finding the reward and policy models and provide a finite-time performance guarantee. Additionally, we demonstrate the efficiency of the proposed solution with extensive experiments including alignment problems in LLMs and robotic control problems in MuJoCo. We observe that the proposed solutions outperform the existing alignment algorithm by large margins, especially when the amounts of demonstration and preference data are unbalanced.Primary Area:foundation or frontier models, including LLMsCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:12997",
    "authors": "Chenliang Li, Siliang Zeng, Zeyi Liao, Jiaxiang Li, Dongyeop Kang, Alfredo Garcia, Mingyi Hong",
    "url": "https://openreview.net/forum?id=VCbqXtS5YY",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Counterfactual Realizability",
    "abstract": "Keywords:causal inference, experiment design, causal reinforcement learning, counterfactual reasoningTL;DR:A complete algorithm for which counterfactual (Layer 3) distributions can be experimentally realized; and its implications for optimal decision-makingAbstract:It is commonly believed that, in a real-world environment, samples can only be drawn from observational and interventional distributions, corresponding to Layers 1 and 2 of the *Pearl Causal Hierarchy*. Layer 3, representing counterfactual distributions, is believed to be inaccessible by definition. However, Bareinboim, Forney, and Pearl (2015) introduced a procedure that allows an agent to sample directly from a counterfactual distribution, leaving open the question of what other counterfactual quantities can be estimated directly via physical experimentation. We resolve this by introducing a formal definition of realizability, the ability to draw samples from a distribution, and then developing a complete algorithm to determine whether an arbitrary counterfactual distribution is realizable given fundamental physical constraints, such as the inability to go back in time and subject the same unit to a different experimental condition. We illustrate the implications of this new framework for counterfactual data collection using motivating examples from causal fairness and causal reinforcement learning. While the baseline approach in these motivating settings typically follows an interventional or observational strategy, we show that a counterfactual strategy provably dominates both.Primary Area:causal reasoningCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:12927",
    "authors": "Arvind Raghavan, Elias Bareinboim",
    "url": "https://openreview.net/forum?id=uuriavczkL",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Robustness Reprogramming for Representation Learning",
    "abstract": "Keywords:Adversarial Robustness, Robustness Reprogramming, Robust Representation LearningAbstract:This work tackles an intriguing and fundamental open challenge in representation learning: Given a well-trained deep learning model, can it be reprogrammed to enhance its robustness against adversarial or noisy input perturbations without altering its parameters?\nTo explore this, we revisit the core feature transformation mechanism in representation learning and propose a novel non-linear robust pattern matching technique as a robust alternative. Furthermore, we introduce three model reprogramming paradigms to offer flexible control of robustness under different efficiency requirements. Comprehensive experiments and ablation studies across diverse learning models ranging from basic linear model and MLPs to shallow and modern deep ConvNets demonstrate the effectiveness \nof our approaches.\nThis work not only opens a promising and orthogonal direction for improving adversarial defenses in deep learning beyond existing methods but also provides new insights into designing more resilient AI systems with robust statistics. \nOur implementation is available at https://github.com/chris-hzc/Robustness-Reprogramming.Primary Area:other topics in machine learning (i.e., none of the above)Code Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:12910",
    "authors": "Zhichao Hou, MohamadAli Torkamani, Hamid Krim, Xiaorui Liu",
    "url": "https://openreview.net/forum?id=SuH5SdOXpe",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "LayerDAG: A Layerwise Autoregressive Diffusion Model for Directed Acyclic Graph Generation",
    "abstract": "Keywords:directed acyclic graphs, graph generation, discrete diffusion, autoregressive modelAbstract:Directed acyclic graphs (DAGs) serve as crucial data representations in domains such as hardware synthesis and compiler/program optimization for computing systems. DAG generative models facilitate the creation of synthetic DAGs, which can be used for benchmarking computing systems while preserving intellectual property. However, generating realistic DAGs is challenging due to their inherent directional and logical dependencies. This paper introduces LayerDAG, an autoregressive diffusion model, to address these challenges. LayerDAG decouples the strong node dependencies into manageable units that can be processed sequentially. By interpreting the partial order of nodes as a sequence of bipartite graphs, LayerDAG leverages autoregressive generation to model directional dependencies and employs diffusion models to capture logical dependencies within each bipartite graph. Comparative analyses demonstrate that LayerDAG outperforms existing DAG generative models in both expressiveness and generalization, particularly for generating large-scale DAGs with up to 400 nodes—a critical scenario for system benchmarking. Extensive experiments on both synthetic and real-world flow graphs from various computing platforms show that LayerDAG generates valid DAGs with superior statistical properties and benchmarking performance. The synthetic DAGs generated by LayerDAG enhance the training of ML-based surrogate models, resulting in improved accuracy in predicting performance metrics of real-world DAGs across diverse computing platforms.Supplementary Material:zipPrimary Area:learning on graphs and other geometries & topologiesCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:12908",
    "authors": "Mufei Li, Viraj Shitole, Eli Chien, Changhai Man, Zhaodong Wang, Srinivas, Ying Zhang, Tushar Krishna, Pan Li",
    "url": "https://openreview.net/forum?id=kam84eEmub",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "DailyDilemmas: Revealing Value Preferences of LLMs with Quandaries of Daily Life",
    "abstract": "Keywords:language model, moral dilemma, model alignment, machine ethics, value alignmentAbstract:As users increasingly seek guidance from LLMs for decision-making in daily life, many of these decisions are not clear-cut and depend significantly on the personal values and ethical standards of people. We present DailyDilemmas, a dataset of 1,360 moral dilemmas encountered in everyday life. Each dilemma presents two possible actions, along with affected parties and relevant human values for each action. Based on these dilemmas, we gather a repository of human values covering diverse everyday topics, such as interpersonal relationships, workplace, and environmental issues. With DailyDilemmas, we evaluate LLMs on these dilemmas to determine what action they will choose and the values represented by these action choices. Then, we analyze values through the lens of five theoretical frameworks inspired by sociology, psychology, and philosophy, including the World Values Survey, Moral Foundations Theory, Maslow's Hierarchy of Needs, Aristotle's Virtues, and Plutchik's Wheel of Emotions. For instance, we find LLMs are most aligned with self-expression over survival in World Values Survey and care over loyalty in Moral Foundations Theory. Interestingly, we find substantial preference differences in models for some core values. For example, for truthfulness, Mixtral-8x7B neglects it by 9.7% while GPT-4-turbo selects it by 9.4%. We also study the recent guidance released by OpenAI (ModelSpec), and Anthropic (Constitutional AI) to understand how their designated principles reflect their models' actual value prioritization when facing nuanced moral reasoning in daily-life settings. Finally, we find that end users cannot effectively steer such prioritization using system prompts.Supplementary Material:zipPrimary Area:datasets and benchmarksCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:12857",
    "authors": "Yu Ying Chiu, Liwei Jiang, Yejin Choi",
    "url": "https://openreview.net/forum?id=PGhiPGBf47",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Learning-Augmented Frequent Directions",
    "abstract": "Keywords:learning-augmented algorithms, algorithms with predictions, data streams, streaming algorithms, frequency estimation, heavy hitters, frequent directions, low-rank approximationTL;DR:We simplify and generalize existing work on frequency estimation with learned predictions.Abstract:An influential paper of Hsu et al. (ICLR'19) introduced the study of learning-augmented streaming algorithms in the context of frequency estimation. A fundamental problem in the streaming literature, the goal of frequency estimation is to approximate the number of occurrences of items appearing in a long stream of data using only a small amount of memory. Hsu et al. develop a natural framework to combine the worst-case guarantees of popular solutions such as CountMin and CountSketch with learned predictions of high frequency elements. They demonstrate that learning the underlying structure of data can be used to yield better streaming algorithms, both in theory and practice.\n\nWe simplify and generalize past work on learning-augmented frequency estimation. Our first contribution is a learning-augmented variant of the Misra-Gries algorithm which improves upon the error of learned CountMin and learned CountSketch and achieves the state-of-the-art performance of randomized algorithms (Aamand et al., NeurIPS'23) with a simpler, deterministic algorithm. Our second contribution is to adapt learning-augmentation to a high-dimensional generalization of frequency estimation corresponding to finding important directions (top singular vectors) of a matrix given its rows one-by-one in a stream. We analyze a learning-augmented variant of the Frequent Directions algorithm, extending the theoretical and empirical understanding of learned predictions to matrix streaming.Supplementary Material:zipPrimary Area:other topics in machine learning (i.e., none of the above)Code Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:12778",
    "authors": "Anders Aamand, Justin Y. Chen, Siddharth Gollapudi, Sandeep Silwal, Hao WU",
    "url": "https://openreview.net/forum?id=WcZLG8XxhD",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "No Need to Talk: Asynchronous Mixture of Language Models",
    "abstract": "Keywords:language models, distributed learning, divide and conquer, efficient inferenceAbstract:We introduce SMALLTALK LM, an innovative method for training a mixture of language models in an almost asynchronous manner. Each\nmodel of the mixture specializes in distinct parts of the data distribution, without the need of high-bandwidth communication between the nodes training each model. At inference, a lightweight router directs a given sequence to a single expert, according to a short prefix. This inference scheme naturally uses a fraction of the parameters from the overall mixture model. Unlike prior works on asynchronous LLM training, our routing method does not rely on full corpus clustering or access to metadata, making it more suitable for real-world applications.  Our experiments on language modeling demonstrate that SMALLTALK LM achieves significantly lower perplexity than dense model baselines for the same total training FLOPs and an almost identical inference cost. Finally, in our downstream evaluations we outperform the dense baseline on 75% of the tasks.Primary Area:foundation or frontier models, including LLMsCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:12727",
    "authors": "Anastasiia Filippova, Angelos Katharopoulos, David Grangier, Ronan Collobert",
    "url": "https://openreview.net/forum?id=pHOH8FVrTp",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Estimating the Probabilities of Rare Outputs in Language Models",
    "abstract": "Keywords:low probabilities, adversarial training, importance samplingTL;DR:We present methods for estimating the probability that a language model outputs a rare token on a given input distribution.Abstract:We consider the problem of *low probability estimation*: given a machine learning model and a formally-specified input distribution, how can we estimate the probability of a binary property of the model's output, even when that probability is too small to estimate by random sampling? This problem is motivated by the need to improve worst-case performance, which distribution shift can make much more likely. We study low probability estimation in the context of argmax sampling from small transformer language models. We compare two types of methods: importance sampling, which involves searching for inputs giving rise to the rare output, and activation extrapolation, which involves extrapolating a probability distribution fit to the model's logits. We find that importance sampling outperforms activation extrapolation, but both outperform naive sampling. Finally, we explain how minimizing the probability estimate of an undesirable behavior generalizes adversarial training, and argue that new methods for low probability estimation are needed to provide stronger guarantees about worst-case performance.Primary Area:probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)Code Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:12679",
    "authors": "Gabriel Wu, Jacob Hilton",
    "url": "https://openreview.net/forum?id=DC8bsa9bzY",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Quality Measures for Dynamic Graph Generative Models",
    "abstract": "Keywords:generative models, dynamic graphs, evaluation metricsTL;DR:We introduce a novel metric leveraging random projections to evaluate generative models for dynamic graphs.Abstract:Deep generative models have recently achieved significant success in modeling graph data, including dynamic graphs, where topology and features evolve over time. However, unlike in vision and natural language domains, evaluating generative models for dynamic graphs is challenging due to the difficulty of visualizing their output, making quantitative metrics essential. In this work, we develop a new quality metric for evaluating generative models of dynamic graphs. Current metrics for dynamic graphs typically involve discretizing the continuous-evolution of graphs into static snapshots and then applying conventional graph similarity measures. This approach has several limitations: (a) it models temporally related events as i.i.d. samples, failing to capture the non-uniform evolution of dynamic graphs; (b) it lacks a unified measure that is sensitive to both features and topology; (c) it fails to provide a scalar metric, requiring multiple metrics without clear superiority; and (d) it requires explicitly instantiating each static snapshot, leading to impractical runtime demands that hinder evaluation at scale. We propose a novel metric based on the Johnson-Lindenstrauss lemma, applying random projections directly to dynamic graph data. This results in an expressive, scalar, and application-agnostic measure of dynamic graph similarity that overcomes the limitations of traditional methods. We also provide a comprehensive empirical evaluation of metrics for continuous-time dynamic graphs, demonstrating the effectiveness of our approach compared to existing methods. Our implementation is available at https://github.com/ryienh/jl-metric.Primary Area:learning on graphs and other geometries & topologiesCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:12628",
    "authors": "Ryien Hosseini, Filippo Simini, Venkatram Vishwanath, Rebecca Willett, Henry Hoffmann",
    "url": "https://openreview.net/forum?id=8bjspmAMBk",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Beyond Random Masking: When Dropout meets Graph Convolutional Networks",
    "abstract": "Keywords:Graph neural networks, DropoutAbstract:Graph Convolutional Networks (GCNs) have emerged as powerful tools for learning on graph-structured data, yet the behavior of dropout in these models remains poorly understood. This paper presents a comprehensive theoretical analysis of dropout in GCNs, revealing that its primary role differs fundamentally from standard neural networks - preventing oversmoothing rather than co-adaptation. We demonstrate that dropout in GCNs creates dimension-specific stochastic sub-graphs, leading to a form of structural regularization not present in standard neural networks. Our analysis shows that dropout effects are inherently degree-dependent, resulting in adaptive regularization that considers the topological importance of nodes. We provide new insights into dropout's role in mitigating oversmoothing and derive novel generalization bounds that account for graph-specific dropout effects. Furthermore, we analyze the synergistic interaction between dropout and batch normalization in GCNs, uncovering a mechanism that enhances overall regularization. Our theoretical findings are validated through extensive experiments on both node-level and graph-level tasks across 14 datasets. Notably, GCN with dropout and batch normalization outperforms state-of-the-art methods on several benchmarks, demonstrating the practical impact of our theoretical insights.Primary Area:learning on graphs and other geometries & topologiesCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14284",
    "authors": "Yuankai Luo, Xiao-Ming Wu, Hao Zhu",
    "url": "https://openreview.net/forum?id=PwxYoMvmvy",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Self-supervised contrastive learning performs non-linear system identification",
    "abstract": "Keywords:system identification, dynamics learning, identifiability, self-supervised learningAbstract:Self-supervised learning (SSL) approaches have brought tremendous success across many tasks and domains. It has been argued that these successes can be attributed to a link between SSL and identifiable representation learning: Temporal structure and auxiliary variables ensure that latent representations are related to the true underlying generative factors of the data. Here, we deepen this connection and show that SSL can perform system identification in latent space. We propose dynamics contrastive learning, a framework to uncover linear, switching linear and non-linear dynamics under a non-linear observation model, give theoretical guarantees and validate them empirically.Primary Area:learning on time series and dynamical systemsCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14280",
    "authors": "Rodrigo González Laiz, Tobias Schmidt, Steffen Schneider",
    "url": "https://openreview.net/forum?id=ONfWFluZBI",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Sparse autoencoders reveal selective remapping of visual concepts during adaptation",
    "abstract": "Keywords:interpretability, vision-language models, sparse autoencoder, adaptationAbstract:Adapting foundation models for specific purposes has become a standard approach to build machine learning systems for downstream applications. Yet, it is an open question which mechanisms take place during adaptation. Here we develop a new Sparse Autoencoder (SAE) for the CLIP vision transformer, named PatchSAE, to extract interpretable concepts at granular levels (e.g., shape, color, or semantics of an object) and their patch-wise spatial attributions. We explore how these concepts influence the model output in downstream image classification tasks and investigate how recent state-of-the-art prompt-based adaptation techniques change the association of model inputs to these concepts. While activations of concepts slightly change between adapted and non-adapted models, we find that the majority of gains on common adaptation tasks can be explained with the existing concepts already present in the non-adapted foundation model. This work provides a concrete framework to train and use SAEs for Vision Transformers and provides insights into explaining adaptation mechanisms.Primary Area:interpretability and explainable AICode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14240",
    "authors": "Hyesu Lim, Jinho Choi, Jaegul Choo, Steffen Schneider",
    "url": "https://openreview.net/forum?id=imT03YXlG2",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "PIED: Physics-Informed Experimental Design for Inverse Problems",
    "abstract": "Keywords:Physics-Informed Neural Network, PINNs, Experimental Design, AI For Science, Active Learning, Data SelectionTL;DR:An experimental design framework for PDE-based inverse problems that uses PINNs and its training dynamics, in a fully differentiable architecture to perform continuous optimization of design parameters.Abstract:In many science and engineering settings, system dynamics are characterized by governing partial differential equations (PDEs), and a major challenge is to solve inverse problems (IPs) where unknown PDE parameters are inferred based on observational data gathered under limited budget. \nDue to the high costs of setting up and running experiments, experimental design (ED) is often done with the help of PDE simulations to optimize for the most informative design parameters (e.g., sensor placements) to solve such IPs, prior to actual data collection. This process of optimizing design parameters is especially critical when the budget and other practical constraints make it infeasible to adjust the design parameters between trials during the experiments.\nHowever, existing experimental design (ED) methods tend to require sequential and frequent design parameter adjustments between trials. Furthermore, they also have significant computational bottlenecks due to the need for complex numerical simulations for PDEs, and do not exploit the advantages provided by physics informed neural networks (PINNs) in solving IPs for PDE-governed systems, such as its meshless solutions, differentiability, and amortized training. \nThis work presents Physics-Informed Experimental Design (PIED), the first ED framework that makes use of PINNs in a fully differentiable architecture to perform continuous optimization of design parameters for IPs for one-shot deployments. \nPIED overcomes existing methods' computational bottlenecks through parallelized computation and meta-learning of PINN parameter initialization, and proposes novel methods to effectively take into account PINN training dynamics in optimizing the ED parameters. \nThrough experiments based on noisy simulated data and even real world experimental data, we empirically show that given limited observation budget, PIED significantly outperforms existing ED methods in solving IPs, including for challenging settings where the inverse parameters are unknown functions rather than just finite-dimensional.Supplementary Material:zipPrimary Area:applications to physical sciences (physics, chemistry, biology, etc.)Code Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14224",
    "authors": "Apivich Hemachandra, Gregory Kang Ruey Lau, See-Kiong Ng, Bryan Kian Hsiang Low",
    "url": "https://openreview.net/forum?id=w7P92BEsb2",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "AgentRefine: Enhancing Agent Generalization through Refinement Tuning",
    "abstract": "Keywords:agent, self-refine, diversity, generalization, data synthesisTL;DR:The self-refine data can expand the search space of LLM agent and improve the reason quality, leading a generalized performance in agent tasks.Abstract:Large Language Model (LLM) based agents have proved their ability to perform complex tasks like humans. However, there is still a large gap between open-sourced LLMs and commercial models like the GPT series. In this paper, we focus on improving the agent generalization capabilities of LLMs via instruction tuning. We first observe that the existing agent training corpus exhibits satisfactory results on held-in evaluation sets but fails to generalize to held-out sets. These agent-tuning works face severe formatting errors and are frequently stuck in the same mistake for a long while. We analyze that the poor generalization ability comes from overfitting to several manual agent environments and a lack of adaptation to new situations. They struggle with the wrong action steps and can not learn from the experience but just memorize existing observation-action relations. Inspired by the insight, we propose a novel AgentRefine framework for agent-tuning. The core idea is to enable the model to learn to correct its mistakes via observation in the trajectory. Specifically, we propose an agent synthesis framework to encompass a diverse array of environments and tasks and prompt a strong LLM to refine its error action according to the environment feedback. AgentRefine significantly outperforms state-of-the-art agent-tuning work in terms of generalization ability on diverse agent tasks. It also has better robustness facing perturbation and can generate diversified thought in inference. Our findings establish the correlation between agent generalization and self-refinement and provide a new paradigm for future research.Supplementary Material:zipPrimary Area:other topics in machine learning (i.e., none of the above)Code Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14212",
    "authors": "Dayuan Fu, Keqing He, Yejie Wang, Wentao Hong, Zhuoma GongQue, Weihao Zeng, Wei Wang, Jingang Wang, Xunliang Cai, Weiran Xu",
    "url": "https://openreview.net/forum?id=FDimWzmcWn",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "TabM: Advancing tabular deep learning with parameter-efficient ensembling",
    "abstract": "Keywords:tabular, tabular data, deep learning, architectureTL;DR:Parameter-efficient ensembling has a massive positive impact on tabular MLPs, and TabM is a new SOTA architecture illustrating that.Abstract:Deep learning architectures for supervised learning on tabular data range from simple multilayer perceptrons (MLP) to sophisticated Transformers and retrieval-augmented methods.\nThis study highlights a major, yet so far overlooked opportunity for substantially improving tabular MLPs; namely, parameter-efficient ensembling -- a paradigm for imitating an ensemble of models with just one model.\nWe start by describing TabM -- a simple model based on MLP and BatchEnsemble (an existing technique), improved with our custom modifications.\nThen, we perform a large scale evaluation of tabular DL architectures on public benchmarks in terms of both task performance and efficiency, which renders the landscape of tabular DL in a new light.\nIn particular, we find that TabM outperforms prior tabular DL models, while the complexity of attention- and retrieval-based methods does not pay off.\nLastly, we conduct a detailed empirical analysis, that sheds some light on the high performance of TabM.\nFor example, we show that parameter-efficient ensembling is not an arbitrary trick, but rather a highly effective way to reduce overfitting and improve optimization dynamics of tabular MLPs.\nOverall, our work brings an impactful technique to tabular DL, analyses its behaviour, and advances the performance-efficiency tradeoff with TabM -- a simple and powerful baseline for researchers and practitioners.Supplementary Material:zipPrimary Area:other topics in machine learning (i.e., none of the above)Code Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14197",
    "authors": "Yury Gorishniy, Akim Kotelnikov, Artem Babenko",
    "url": "https://openreview.net/forum?id=Sd4wYYOhmY",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Multi-Label Test-Time Adaptation with Bound Entropy Minimization",
    "abstract": "Keywords:Vision-Language Models, Zero-Shot Multi-Label Generalization, Test-Time AdaptationTL;DR:A Multi-Label Test-Time Adaptation method with Bound Entropy Minimization objective.Abstract:Mainstream test-time adaptation (TTA) techniques endeavor to mitigate distribution shifts via entropy minimization for multi-class classification, inherently increasing the probability of the most confident class. However, when encountering multi-label instances, the primary challenge stems from the varying number of labels per image, and prioritizing only the highest probability class inevitably undermines the adaptation of other positive labels. To address this issue, we investigate TTA within multi-label scenario (ML--TTA), developing Bound Entropy Minimization (BEM) objective to simultaneously increase the confidence of multiple top predicted labels. Specifically, to determine the number of labels for each augmented view, we retrieve a paired caption with yielded textual labels for that view. These labels are allocated to both the view and caption, called weak label set and strong label set with the same size k. Following this, the proposed BEM considers the highest top-k predicted labels from view and caption as a single entity, respectively, learning both view and caption prompts concurrently. By binding top-k predicted labels, BEM overcomes the limitation of vanilla entropy minimization, which exclusively optimizes the most confident class. Across the MSCOCO, VOC, and NUSWIDE multi-label datasets, our ML--TTA framework equipped with BEM exhibits superior performance compared to the latest SOTA methods, across various model architectures, prompt initialization, and varying label scenarios. The code is available at https://github.com/Jinx630/ML-TTA.Primary Area:unsupervised, self-supervised, semi-supervised, and supervised representation learningCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14187",
    "authors": "Xiangyu Wu, Feng Yu, Yang Yang, Qing-Guo Chen, Jianfeng Lu",
    "url": "https://openreview.net/forum?id=75PhjtbBdr",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "ToolGen: Unified Tool Retrieval and Calling via Generation",
    "abstract": "Keywords:Agent, Tool Learning, Virtual TokenTL;DR:Unified tool retrieval and calling by transforming tools into virtual tokensAbstract:As large language models (LLMs) advance, their inability to autonomously execute tasks by directly interacting with external tools remains a critical limitation. Traditional methods rely on inputting tool descriptions as context, which is constrained by context length and requires separate, often inefficient, retrieval mechanisms. We introduce ToolGen, a paradigm shift that integrates tool knowledge directly into the LLM’s parameters by representing each tool as a unique token. This enables the LLM to generate tool calls and arguments as part of its next token prediction capabilities, seamlessly blending tool invocation with language generation.  Our framework allows the LLM to access and utilize a vast amount of tools with no additional retrieval step, significantly enhancing both performance and scalability. Experimental results with over 47,000 tools show that ToolGen not only achieves superior results in both tool retrieval and autonomous task completion but also sets the stage for a new era of AI agents that can adapt to tools across diverse domains.  By fundamentally transforming tool retrieval into a generative process, ToolGen paves the way for more versatile, efficient, and autonomous AI systems. ToolGen enables end-to-end tool learning and opens opportunities for integration with other advanced techniques such as chain-of-thought and reinforcement learning, thereby expanding the practical capabilities of LLMsPrimary Area:applications to computer vision, audio, language, and other modalitiesCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14183",
    "authors": "Renxi Wang, Xudong Han, Lei Ji, Shu Wang, Timothy Baldwin, Haonan Li",
    "url": "https://openreview.net/forum?id=XLMAMmowdY",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Activation Gradient based Poisoned Sample Detection Against Backdoor Attacks",
    "abstract": "Keywords:Backdoor Defense, Poisoned Sample Detection, AI securityAbstract:This work studies the task of poisoned sample detection for defending against data poisoning based backdoor attacks. Its core challenge is finding a generalizable and discriminative metric to distinguish between clean and various types of poisoned samples (e.g., various triggers, various poisoning ratios). Inspired by a common phenomenon in backdoor attacks that the backdoored model tend to map significantly different poisoned and clean samples within the target class to similar activation areas, we introduce a novel perspective of the circular distribution of the gradients w.r.t. sample activation, dubbed gradient circular distribution (GCD). And, we find two interesting observations based on GCD. One is that the GCD of samples in the target class is much more dispersed than that in the clean class. The other is that in the GCD of target class, poisoned and clean samples are clearly separated. Inspired by above two observations, we develop an innovative three-stage poisoned sample detection approach, called Activation Gradient based Poisoned sample Detection (AGPD). First, we calculate GCDs of all classes from the model trained on the untrustworthy dataset. Then, we identify the target class(es) based on the difference on GCD dispersion between target and clean classes. Last, we filter out poisoned samples within the identified target class(es) based on the clear separation between poisoned and clean samples. Extensive experiments under various settings of backdoor attacks demonstrate the superior detection performance of the proposed method to existing poisoned detection approaches according to sample activation-based metrics.Supplementary Material:zipPrimary Area:alignment, fairness, safety, privacy, and societal considerationsCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14155",
    "authors": "Danni Yuan, Mingda Zhang, Shaokui Wei, Li Liu, Baoyuan Wu",
    "url": "https://openreview.net/forum?id=VNMJfBBUd5",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Causally Motivated Sycophancy Mitigation for Large Language Models",
    "abstract": "Keywords:Large Language Model; Sycophancy; Causal ModelingAbstract:Incorporating user preferences into large language models (LLMs) can enhance the personalization and reliability of model outputs and facilitate the application of LLMs to real-world scenarios. However, leveraging user preferences can be a double-edged sword. Recent studies have found that improper utilization can incur sycophancy, where LLMs prioritize alignment with user preferences over the correctness of their outputs. To address sycophancy in LLMs, we analyze and model the problem through the lens of structured causal models (SCMs). We attribute sycophancy to LLMs' reliance on spurious correlations between user preferences and model outputs in this paper. Based on the proposed SCMs, we develop a novel framework, termed **CAUSM**, to mitigate sycophancy in LLMs by exploiting a significant causal signature. Specifically, we eliminate the spurious correlations embedded in the intermediate layers of LLMs through causally motivated head reweighting, and then calibrate the intra-head knowledge along the causal representation direction. Extensive experiments are conducted across diverse language tasks to demonstrate the superiority of our method over state-of-the-art competitors in mitigating sycophancy in LLMs.Primary Area:foundation or frontier models, including LLMsCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14154",
    "authors": "Haoxi Li, Xueyang Tang, Jie ZHANG, Song Guo, Sikai Bai, Peiran Dong, Yue Yu",
    "url": "https://openreview.net/forum?id=yRKelogz5i",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Compositional simulation-based inference for time series",
    "abstract": "Keywords:Simulation-based inference, Bayesian inference, time series, markovian simulators, Amortized Bayesian inferenceTL;DR:Simulation-based inference for Markovian simulators leveraging the factorizationAbstract:Amortized simulation-based inference (SBI) methods train neural networks on simulated data to perform Bayesian inference. While this strategy avoids the need for tractable likelihoods, it often requires a large number of simulations and has been challenging to scale to time series data. Scientific simulators frequently emulate real-world dynamics through thousands of single-state transitions over time. We propose an SBI approach that can exploit such Markovian simulators by locally identifying parameters consistent with individual state transitions. We then compose these local results to obtain a posterior over parameters that align with the entire time series observation. We focus on applying this approach to neural posterior score estimation but also show how it can be applied, e.g., to neural likelihood (ratio) estimation. We demonstrate that our approach is more simulation-efficient than directly estimating the global posterior on several synthetic benchmark tasks and simulators used in ecology and epidemiology. Finally, we validate scalability and simulation efficiency of our approach by applying it to a high-dimensional Kolmogorov flow simulator with around one million data dimensions.Supplementary Material:zipPrimary Area:probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)Code Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14141",
    "authors": "Manuel Gloeckler, Shoji Toyota, Kenji Fukumizu, Jakob H. Macke",
    "url": "https://openreview.net/forum?id=uClUUJk05H",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Bayesian Treatment of the Spectrum of the Empirical Kernel in (Sub)Linear-Width Neural Networks",
    "abstract": "Keywords:infinite bayesian neural networks, kernel theory, random matrix theoryAbstract:We study Bayesian neural networks (BNNs) in the theoretical limits of infinitely increasing number of training examples, network width and input space dimension. Our findings establish new bridges between kernel-theoretic approaches and techniques derived from statistical mechanics through the correspondence between Mercer's eigenvalues and limiting spectral distributions of covariance matrices studied in random matrix theory. \n   Our theoretical contributions first consist in novel integral formulas that accurately describe the predictors of BNNs in the asymptotic linear-width and sublinear-width regimes. Moreover, we extend the recently developed renormalisation theory of deep linear neural networks, enabling a rigorous explanation of the mounting empirical evidence that hints at the theory's applicability to nonlinear BNNs with ReLU activations in the linear-width regime.\n   From a practical standpoint, our results introduce a novel technique for estimating the predictor statistics of a trained BNN that is applicable to the sublinear-width regime where the predictions of the renormalisation theory are inaccurate.Supplementary Material:zipPrimary Area:learning theoryCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14113",
    "authors": "Ouns El Harzli, Bernardo Cuenca Grau",
    "url": "https://openreview.net/forum?id=O6znYvxC1U",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "When GNNs meet symmetry in ILPs: an orbit-based feature augmentation approach",
    "abstract": "Keywords:integer linear programming, symmetry, machine learning, graph neural networksAbstract:A common characteristic in integer linear programs (ILPs) is symmetry, allowing variables to be permuted without altering the underlying problem structure. Recently, GNNs have emerged as a promising approach for solving ILPs. \nHowever, a significant challenge arises when applying GNNs to ILPs with symmetry: classic GNN architectures struggle to differentiate between symmetric variables, which limits their predictive accuracy. In this work, we investigate the properties of permutation equivalence and invariance in GNNs, particularly in relation to the inherent symmetry of ILP formulations. We reveal that the interaction between these two factors contributes to the difficulty of distinguishing between symmetric variables.\nTo address this challenge, we explore the potential of feature augmentation and propose several guiding principles for constructing augmented features. Building on these principles, we develop an orbit-based augmentation scheme that first groups symmetric variables and then samples augmented features for each group from a discrete uniform distribution. Empirical results demonstrate that our proposed approach significantly enhances both training efficiency and predictive performance.Primary Area:optimizationCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14111",
    "authors": "Qian Chen, Lei Li, Qian Li, Jianghua Wu, Akang Wang, Ruoyu Sun, Xiaodong Luo, Tsung-Hui Chang, Qingjiang Shi",
    "url": "https://openreview.net/forum?id=wVTJRnZ11Z",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Optimal Transport for Time Series Imputation",
    "abstract": "Keywords:Time series, ImputationAbstract:Missing data imputation through distribution alignment has demonstrated advantages for non-temporal datasets but exhibits suboptimal performance in time-series applications. The primary obstacle is crafting a discrepancy measure that simultaneously (1) captures temporal patterns—accounting for periodicity and temporal dependencies inherent in time-series—and (2) accommodates non-stationarity, ensuring robustness amidst multiple coexisting temporal patterns. In response to these challenges, we introduce the Proximal Spectrum Wasserstein (PSW) discrepancy, a novel discrepancy tailored for comparing two \\textit{sets} of time-series based on optimal transport. It incorporates a pairwise spectral distance to encapsulate temporal patterns, and a selective matching regularization to accommodate non-stationarity. Subsequently, we develop the PSW for Imputation (PSW-I) framework, which iteratively refines imputation results by minimizing the PSW discrepancy. Extensive experiments demonstrate that PSW-I effectively accommodates temporal patterns and non-stationarity, outperforming prevailing time-series imputation methods. Code is available at https://github.com/FMLYD/PSW-I.Primary Area:learning on time series and dynamical systemsCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14099",
    "authors": "Hao Wang, zhengnan li, Haoxuan Li, Xu Chen, Mingming Gong, BinChen, Zhichao Chen",
    "url": "https://openreview.net/forum?id=xPTzjpIQNp",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Video Action Differencing",
    "abstract": "Keywords:Video, Actions, Differencing, Zero-shot, benchmark, multimodal, lmm, llmTL;DR:A new task and benchmark for comparing how an action is performed between two videos, with a zero-shot methodAbstract:How do two individuals differ when performing the same action? In this work, we introduce Video Action Differencing (VidDiff), the novel task of identifying subtle differences between videos of the same action, which has numerous applications, such as coaching and skill learning. To enable development on this new task, we first create VidDiffBench, a benchmark dataset containing 549 video pairs, with human annotations of 4,469 fine-grained action differences and 2,075 timestamps indicating where these differences occur. Our experiments demonstrate that VidDiffBench poses a significant challenge for state-of-the-art large multimodal models (LMMs), such as GPT-4o and Qwen2-VL. By analyzing the failure cases of LMMs on VidDiffBench, we highlight two key challenges for this task: localizing relevant sub-actions over two videos and fine-grained frame comparison. To overcome these, we propose the VidDiff method, an agentic workflow that breaks the task into three stages: action difference proposal, keyframe localization, and frame differencing, each stage utilizing specialized foundation models. To encourage future research in this new task, we release the benchmark and code.Primary Area:datasets and benchmarksCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14085",
    "authors": "James Burgess, Xiaohan Wang, Yuhui Zhang, Anita Rau, Alejandro Lozano, Lisa Dunlap, Trevor Darrell, Serena Yeung-Levy",
    "url": "https://openreview.net/forum?id=3bcN6xlO6f",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "GANDALF: Generative AttentioN based Data Augmentation and predictive modeLing Framework for personalized cancer treatment",
    "abstract": "Keywords:personalized drug response prediction, cancer, genomic data augmentation, diffusion model, pseudolabellingTL;DR:A cancer drug response prediction model that addresses the problem of limited labelled data through a novel genomic data augmentation technique.Abstract:Effective treatment of cancer is a major challenge faced by healthcare providers, due to the highly individualized nature of patient responses to treatment. This is caused by the heterogeneity seen in cancer-causing alterations (mutations) across patient genomes. Limited availability of response data in patients makes it difficult to train personalized treatment recommendation models on mutations from clinical genomic sequencing reports. Prior methods tackle this by utilising larger, labelled pre-clinical laboratory datasets (‘cell lines’), via transfer learning. These methods augment patient data by learning a shared, domain-invariant representation, between the cell line and patient domains, which is then used to train a downstream drug response prediction (DRP) model. This approach augments data in the shared space but fails to model patient-specific characteristics, which have a strong influence on their drug response. We propose a novel generative attention-based data augmentation and predictive modeling framework, GANDALF, to tackle this crucial shortcoming of prior methods. GANDALF not only augments patient genomic data directly, but also accounts for its domain-specific characteristics. GANDALF outperforms state-of-the-art DRP models on publicly available patient datasets and emerges as the front-runner amongst SOTA cancer DRP models.Primary Area:applications to physical sciences (physics, chemistry, biology, etc.)Code Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14072",
    "authors": "Aishwarya Jayagopal, Yanrong Zhang, Robert John Walsh, Tuan Zea Tan, Anand D Jeyasekharan, Vaibhav Rajan",
    "url": "https://openreview.net/forum?id=WwmtcGr4lP",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "RaSA: Rank-Sharing Low-Rank Adaptation",
    "abstract": "Keywords:parameter-efficient fine-tuning, large language model, low-rank adaptationAbstract:Low-rank adaptation (LoRA) has been prominently employed for parameter-efficient fine-tuning of large language models (LLMs). However, the limited expressive capacity of LoRA, stemming from the low-rank constraint, has been recognized as a bottleneck, particularly in rigorous tasks like code generation and mathematical reasoning. To address this limitation, we introduce Rank-Sharing Low-Rank Adaptation (RaSA), an innovative extension that enhances the expressive capacity of LoRA by leveraging partial rank sharing across layers. By forming a shared rank pool and applying layer-specific weighting, RaSA effectively increases the number of ranks without augmenting parameter overhead. Our theoretically grounded and empirically validated approach demonstrates that RaSA not only maintains the core advantages of LoRA but also significantly boosts performance in challenging code and math tasks. Code, data and scripts are available at: https://github.com/zwhe99/RaSA.Primary Area:foundation or frontier models, including LLMsCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14067",
    "authors": "Zhiwei He, Zhaopeng Tu, Xing Wang, Xingyu Chen, Zhijie Wang, Jiahao Xu, Tian Liang, Wenxiang Jiao, Zhuosheng Zhang, Rui Wang",
    "url": "https://openreview.net/forum?id=GdXI5zCoAt",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Scaling Speech-Text Pre-training with Synthetic Interleaved Data",
    "abstract": "Keywords:large language models; speech language model; spoken chatbotsAbstract:Speech language models (SpeechLMs) accept speech input and produce speech output, allowing for more natural human-computer interaction compared to text-based large language models (LLMs).\nTraditional approaches for developing SpeechLMs are constrained by the limited availability of unsupervised speech data and parallel speech-text data, which are significantly less abundant compared to text pre-training data, thereby limiting their scalability as LLMs.\nWe propose a novel approach to scaling speech-text pre-training by leveraging large-scale synthetic interleaved data derived from text corpora, eliminating the need for parallel speech-text datasets.\nOur method efficiently constructs speech-text interleaved data by sampling text spans from existing text corpora and synthesizing corresponding speech spans using a text-to-token model, bypassing the need to generate actual speech.\nWe also employ a supervised speech tokenizer derived from an automatic speech recognition (ASR) model  by incorporating a vector-quantized bottleneck into the encoder. This supervised training approach results in discrete speech tokens with strong semantic preservation even at lower sampling rates (e.g. 12.5Hz), while still maintaining speech reconstruction quality.\nStarting from a pre-trained language model and scaling our pre-training to 1 trillion tokens (with 600B synthetic interleaved speech-text data), we achieve state-of-the-art performance in both speech language modeling and spoken question answering, improving performance on spoken questions tasks from the previous SOTA of 13\\% (Moshi) to 31\\%.\nWe further demonstrate that by fine-tuning the pre-trained model with speech dialogue data, we can develop an end-to-end spoken chatbot that achieves competitive performance comparable to existing baselines in both conversational abilities and speech quality, even operating exclusively in the speech domain.Supplementary Material:zipPrimary Area:foundation or frontier models, including LLMsCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14059",
    "authors": "Aohan Zeng, Zhengxiao Du, Mingdao Liu, Lei Zhang, shengmin jiang, Yuxiao Dong, Jie Tang",
    "url": "https://openreview.net/forum?id=3tukjsVyrE",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Offline Model-Based Optimization by Learning to Rank",
    "abstract": "Keywords:Offline model-based optimization, black-box optimization, learning to rank, learning to optimizeAbstract:Offline model-based optimization (MBO) aims to identify a design that maximizes a black-box function using only a fixed, pre-collected dataset of designs and their corresponding scores. This problem has garnered significant attention from both scientific and industrial domains. A common approach in offline MBO is to train a regression-based surrogate model by minimizing mean squared error (MSE) and then find the best design within this surrogate model by different optimizers (e.g., gradient ascent). However, a critical challenge is the risk of out-of-distribution errors, i.e., the surrogate model may typically overestimate the scores and mislead the optimizers into suboptimal regions. Prior works have attempted to address this issue in various ways, such as using regularization techniques and ensemble learning to enhance the robustness of the model, but it still remains. In this paper, we argue that regression models trained with MSE are not well-aligned with the primary goal of offline MBO, which is to \\textit{select} promising designs rather than to predict their scores precisely. Notably, if a surrogate model can maintain the order of candidate designs based on their relative score relationships, it can produce the best designs even without precise predictions. To validate it, we conduct experiments to compare the relationship between the quality of the final designs and MSE, finding that the correlation is really very weak. In contrast, a metric that measures order-maintaining quality shows a significantly stronger correlation. Based on this observation, we propose learning a ranking-based model that leverages learning to rank techniques to prioritize promising designs based on their relative scores. We show that the generalization error on ranking loss can be well bounded. Empirical results across diverse tasks demonstrate the superior performance of our proposed ranking-based method than twenty existing methods. Our implementation is available at \\url{https://github.com/lamda-bbo/Offline-RaM}.Supplementary Material:zipPrimary Area:optimizationCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14057",
    "authors": "Rong-Xi Tan, Ke Xue, Shen-Huan Lyu, Haopu Shang, yaowang, Yaoyuan Wang, Fu Sheng, Chao Qian",
    "url": "https://openreview.net/forum?id=sb1HgVDLjN",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "From Search to Sampling: Generative Models for Robust Algorithmic Recourse",
    "abstract": "Keywords:Algorithmic recourse, explainability, generative modellingTL;DR:We propose a generative modelling approach for recourse that provides a distribution over likely recourse instances.Abstract:Algorithmic Recourse provides recommendations to individuals who are adversely impacted by automated model decisions, on how to alter their profiles to achieve a favorable outcome. Effective recourse methods must balance three conflicting goals: proximity to the original profile to minimize cost, plausibility for realistic recourse, and validity to ensure the desired outcome. We show that existing methods train for these objectives separately and then search for recourse through a joint optimization over the recourse goals during inference, leading to poor recourse recommendations. We introduce GenRe, a generative recourse model designed to train the three recourse objectives jointly. Training such generative models is non-trivial due to lack of direct recourse supervision. We propose efficient ways to synthesize such supervision and further show that GenRe's training leads to a consistent estimator. Unlike most prior methods, that employ non-robust gradient descent based search during inference, GenRe simply performs a forward sampling over the generative model to produce minimum cost recourse, leading to superior performance across multiple metrics. We also demonstrate GenRe provides the best trade-off between cost, plausibility and validity, compared to state-of-art baselines. Our code is available at: https://github.com/prateekgargX/genrePrimary Area:alignment, fairness, safety, privacy, and societal considerationsCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14050",
    "authors": "Prateek Garg, Lokesh Nagalapatti, Sunita Sarawagi",
    "url": "https://openreview.net/forum?id=NtwFghsJne",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Neural Wave Equation for Irregularly Sampled Sequence Data",
    "abstract": "Keywords:Wave Equation, Neural ODE, Sequence LabellingTL;DR:Partial Differential Equations parameterised by a Neural Network (like Neural ODE) can be used to solve sequence modeling problems. We hypothesize why this might be the case and demonstrate that it outpeforms many known continuous RNN models.Abstract:Sequence labeling problems arise in several real-world applications such as healthcare and robotics. In many such applications, sequence data are irregularly sampled and are of varying complexities. Recently, efforts have been made to develop neural ODE-based architectures to model the evolution of hidden states continuously in time, to address irregularly sampled sequence data. However, they assume a fixed architectural depth and limit their flexibility to adapt to data sets with varying complexities. We propose the neural wave equation, a novel deep learning method inspired by the wave equation, to address this through continuous modeling of depth. Neural Wave Equation models the evolution of hidden states continuously across time as well as depth by using a non-homogeneous wave equation parameterized by a neural network.  Through d'Alembert's analytical solution of the wave equation, we also show that the neural wave equation provides denser connections across the hidden states, allowing for better modeling capability.  We conduct experiments on several sequence labeling problems involving irregularly sampled sequence data and demonstrate the superior performance of the proposed neural wave equation model.Supplementary Material:zipPrimary Area:neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)Code Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14044",
    "authors": "Arkaprava Majumdar, M Anand Krishna, P. K. Srijith",
    "url": "https://openreview.net/forum?id=kbeX97jExm",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "ComaDICE: Offline Cooperative Multi-Agent Reinforcement Learning with Stationary Distribution Shift Regularization",
    "abstract": "Keywords:Offline Reinforcement Learning, Multi-Agent Reinforcement Learning, Stationary Distribution Correction EstimationTL;DR:This paper introduces ComaDICE, a novel offline cooperative multi-agent reinforcement learning algorithm that uses stationary distribution shift regularization to improve performance in complex environments like MuJoCo and StarCraft II.Abstract:Offline reinforcement learning (RL) has garnered significant attention for its ability to learn effective policies from pre-collected datasets without the need for further environmental interactions. While promising results have been demonstrated in single-agent settings, offline multi-agent reinforcement learning (MARL) presents additional challenges due to the large joint state-action space and the complexity of multi-agent behaviors. A key issue in offline RL is the distributional shift, which arises when the target policy being optimized deviates from the behavior policy that generated the data. This problem is exacerbated in MARL due to the interdependence between agents' local policies and the expansive joint state-action space. Prior approaches have primarily addressed this challenge by incorporating regularization in the space of either Q-functions or policies. In this work, we propose a novel type of regularizer in the space of stationary distributions to address the distributional shift more effectively. Our algorithm, ComaDICE, provides a principled framework for offline cooperative MARL to correct the stationary distribution of the global policy, which is then leveraged to derive local policies for individual agents. Through extensive experiments on the offline multi-agent MuJoCo and StarCraft II benchmarks, we demonstrate that ComaDICE achieves superior performance compared to state-of-the-art offline MARL methods across nearly all tasks.Supplementary Material:zipPrimary Area:reinforcement learningCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14042",
    "authors": "The Viet Bui, Thanh Hong Nguyen, Tien Anh Mai",
    "url": "https://openreview.net/forum?id=5o9JJJPPm6",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Probabilistic Conformal Prediction with Approximate Conditional Validity",
    "abstract": "Keywords:Conformal Prediction, Conditional coverage, Probabilistic method, Uncertainty QuantificationTL;DR:We introduce a method that effectively integrates conformal approaches with an estimate of the conditional distribution to ensure the approximate conditional validity.Abstract:We develop a new method for generating prediction sets that combines the flexibility of conformal methods with an estimate of the conditional distribution $\\textup{P}_{Y \\mid X}$. Existing methods, such as conformalized quantile regression and probabilistic conformal prediction, usually provide only a marginal coverage guarantee. In contrast, our approach extends these frameworks to achieve approximately conditional coverage, which is crucial for many practical applications. Our prediction sets adapt to the behavior of the predictive distribution, making them effective even under high heteroscedasticity. While exact conditional guarantees are infeasible without assumptions on the underlying data distribution, we derive non-asymptotic bounds that depend on the total variation distance of the conditional distribution and its estimate. Using extensive simulations, we show that our method consistently outperforms existing approaches in terms of conditional coverage, leading to more reliable statistical inference in a variety of applications.Primary Area:probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)Code Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14036",
    "authors": "Vincent Plassier, Alexander Fishkov, Mohsen Guizani, Maxim Panov, Eric Moulines",
    "url": "https://openreview.net/forum?id=Nfd7z9d6Bb",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Rethinking Neural Multi-Objective Combinatorial Optimization via Neat Weight Embedding",
    "abstract": "Keywords:Neural Multi-Objective Combinatorial Optimization, Weight Embedding, Conditional AttentionTL;DR:We propose a neat weight embedding method for neural multi-objective combinatorial optimizationAbstract:Recent decomposition-based neural multi-objective combinatorial optimization (MOCO) methods struggle to achieve desirable performance. Even equipped with complex learning techniques, they often suffer from significant optimality gaps in weight-specific subproblems. To address this challenge, we propose a neat weight embedding method to learn weight-specific representations, which captures weight-instance interaction for the subproblems and was overlooked by most current methods. We demonstrate the potentials of our method in two instantiations. First, we introduce a succinct addition model to learn weight-specific node embeddings, which surpassed most existing neural methods. Second, we design an enhanced conditional attention model to simultaneously learn the weight embedding and node embeddings, which yielded new state-of-the-art performance. Experimental results on classic MOCO problems verified the superiority of our method. Remarkably, our method also exhibits favorable generalization performance across problem sizes, even outperforming the neural method specialized for boosting size generalization.Primary Area:other topics in machine learning (i.e., none of the above)Code Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14028",
    "authors": "Jinbiao Chen, Zhiguang Cao, Jiahai Wang, Yaoxin Wu, Hanzhang Qin, Zizhen Zhang, Yue-Jiao Gong",
    "url": "https://openreview.net/forum?id=GM7cmQfk2F",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Robust Root Cause Diagnosis using In-Distribution Interventions",
    "abstract": "Keywords:Root Cause Diagnosis, Causal Inference, Interventional RCDTL;DR:Identifying root cause of anomalies using interventions rather than counterfactuals estimated from a learned SCMAbstract:Diagnosing the root cause of an anomaly in a complex interconnected system is\na pressing problem in today’s cloud services and industrial operations. We propose In-Distribution Interventions (IDI), a novel algorithm that predicts root cause\nas nodes that meet two criteria: 1) Anomaly: root cause nodes should take on\nanomalous values; 2) Fix: had the root cause nodes assumed usual values, the\ntarget node would not have been anomalous. Prior methods of assessing the fix\ncondition rely on counterfactuals inferred from a Structural Causal Model (SCM)\ntrained on historical data. But since anomalies are rare and fall outside the training distribution, the fitted SCMs yield unreliable counterfactual estimates. IDI\novercomes this by relying on interventional estimates obtained by solely probing the fitted SCM at in-distribution inputs. We present a theoretical analysis\ncomparing and bounding the errors in assessing the fix condition using interventional and counterfactual estimates. We then conduct experiments by systematically varying the SCM’s complexity to demonstrate the cases where IDI’s interventional approach outperforms the counterfactual approach and vice versa.\nExperiments on both synthetic and PetShop RCD benchmark datasets demonstrate that IDI consistently identifies true root causes more accurately and robustly than nine existing state-of-the-art RCD baselines. Code will be released\nat https://github.com/nlokeshiisc/IDI_release.Primary Area:causal reasoningCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14022",
    "authors": "Lokesh Nagalapatti, Ashutosh Srivastava, Sunita Sarawagi, Amit Sharma",
    "url": "https://openreview.net/forum?id=l11DZY5Nxu",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Synergistic Approach for Simultaneous Optimization of Monolingual, Cross-lingual, and Multilingual Information Retrieval",
    "abstract": "Keywords:Information Retrieval, Multilingualism and Cross-Lingual NLP, Question AnsweringAbstract:Information retrieval across different languages is an increasingly important challenge in natural language processing. Recent approaches based on multilingual pre-trained language models have achieved remarkable success, yet they often optimize for either monolingual, cross-lingual, or multilingual retrieval performance at the expense of others. This paper proposes a novel hybrid batch training strategy to simultaneously improve zero-shot retrieval performance across monolingual, cross-lingual, and multilingual settings while mitigating language bias. The approach fine-tunes multilingual language models using a mix of monolingual and cross-lingual question-answer pair batches sampled based on dataset size. Experiments on XQuAD-R, MLQA-R, and MIRACL benchmark datasets show that the proposed method consistently achieves comparable or superior results in zero-shot retrieval across various languages and retrieval tasks compared to monolingual-only or cross-lingual-only training. Hybrid batch training also substantially reduces language bias in multilingual retrieval compared to monolingual training. These results demonstrate the effectiveness of the proposed approach for learning language-agnostic representations that enable strong zero-shot retrieval performance across diverse languages.Primary Area:applications to computer vision, audio, language, and other modalitiesCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14290",
    "authors": "Adel Elmahdy, Sheng-Chieh Lin, Amin Ahmad",
    "url": "https://openreview.net/forum?id=zkNCWtw2fd",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "EXecution-Eval: Can language models execute real-world code?",
    "abstract": "Keywords:large language model, evaluation, benchmark, code executionAbstract:As Large Language Models (LLMs) advance, traditional benchmarks face challenges of dataset saturation and disconnection from real-world performance, limiting our understanding of true model capabilities. We introduce EXecution-Eval (EXE), a benchmark designed to assess LLMs' ability to execute code and predict program states. EXE attempts to address key limitations in existing evaluations: difficulty scaling, task diversity, training data contamination, and cost-effective scalability.\nComprising over 30,000 tasks derived from 1,000 popular Python repositories on GitHub, EXE spans a range of context lengths and algorithmic complexities. Tasks require models to execute code, necessitating various operations including mathematical reasoning, logical inference, bit manipulation, string operations, loop execution, and maintaining multiple internal variable states during computation. Our methodology involves: (a) selecting and preprocessing GitHub repositories, (b) generating diverse inputs for functions, (c) executing code to obtain ground truth outputs, and (d) formulating tasks that require models to reason about code execution. This approach allows for continuous new task generation for as few as 1,200 tokens, significantly reducing the risk of models \"training on the test set.\"\nWe evaluate several state-of-the-art LLMs on EXE, revealing insights into their code comprehension and execution capabilities. Our results show that even the best-performing models struggle with complex, multi-step execution tasks, highlighting specific computational concepts that pose the greatest challenges for today's LLMs. Furthermore, we review EXE's potential for finding and predicting errors to aid in assessing a model's cybersecurity capabilities. We propose EXE as a sustainable and challenging testbed for evaluating frontier models, offering potential insights into their internal mechanistic advancementPrimary Area:datasets and benchmarksCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14287",
    "authors": "Rob Kopel",
    "url": "https://openreview.net/forum?id=viQ1bLqKY0",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "The Rate-Distortion-Perception Trade-Off with Algorithmic Realism",
    "abstract": "Keywords:lossy compression, perceptual quality, rate-distortion-perception trade-off, randomization, universal criticsAbstract:Realism constraints (or constraints on perceptual quality) have received considerable recent attention within the context of lossy compression, particularly of images. Theoretical studies of lossy compression indicate that high-rate common randomness between the compressor and the decompressor is a valuable resource for achieving realism. On the other hand, the utility of significant amounts of common randomness at test time has not been noted in practice. We offer an explanation for this discrepancy by considering a realism constraint that requires satisfying a universal critic that inspects realizations of individual compressed images, or batches thereof. We characterize the optimal rate-distortion-perception trade-off under such a realism constraint, and show that it is asymptotically achievable without any common randomness, unless the batch size is impractically large.Primary Area:applications to computer vision, audio, language, and other modalitiesCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14286",
    "authors": "Yassine Hamdi, Aaron B. Wagner, Deniz Gunduz",
    "url": "https://openreview.net/forum?id=vdUYa7N8Mt",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Defining Deception in Decision Making",
    "abstract": "Keywords:deception, AI safetyTL;DR:a definition for deception under the formalism of decision makingAbstract:With the growing capabilities of machine learning systems, particularly those that interact with humans, there is an increased risk of systems that can easily deceive and manipulate people. Preventing unintended behaviors therefore represents an important challenge for creating aligned AI systems. To approach this challenge in a principled way, we first need to define deception formally. In this work, we present a concrete definition of deception under the formalism of rational decision making in partially observed Markov decision processes. Specifically, we propose a general regret theory of deception under which the degree of deception can be quantified in terms of the actor's beliefs, actions, and utility. To evaluate our definition, we study the degree to which our definition aligns with human judgments about deception. We hope that our work will constitute a step toward both systems that aim to avoid deception, and detection mechanisms to identify deceptive agents.Supplementary Material:pdfPrimary Area:alignment, fairness, safety, privacy, and societal considerationsCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14282",
    "authors": "Marwa Abdulhai, Micah Carroll, Justin Svegliato, Aryansh Shrivastava, Anca Dragan, Sergey Levine",
    "url": "https://openreview.net/forum?id=YaRzuMaubS",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "MAC-CAFE: Multi-actor, Centralized Critic Architecture for Feedback-driven Editing",
    "abstract": "Keywords:Retrieval-Augmented Generation, Large Language Models, Knowledge Base Editing, Prompt OptimizationTL;DR:Textual Knowledge Base editing based on expert feedback using a multi actor centralized critic architectureAbstract:Large Language Models (LLMs) often generate incorrect or outdated information, especially in low-resource settings or when dealing with private data. To address this, Retrieval-Augmented Generation (RAG) uses external knowledge bases (KBs), but these can also suffer from inaccuracies. We introduce MAC-CAFE, a novel Multi-actor, Centralized Critic Architecture for Feedback-driven Editing approach that iteratively refines the KB based on expert feedback using a multi-actor, centralized critic reinforcement learning framework. Each document is assigned to an actor, modeled as a ReACT agent, which performs structured edits based on document-specific targeted instructions from a centralized critic. Experimental results show that MAC-CAFE significantly improves KB quality and RAG system performance, enhancing accuracy by up to 8% over baselines.Supplementary Material:pdfPrimary Area:reinforcement learningCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14279",
    "authors": "Naman Gupta, Shashank Kirtania, Priyanshu Gupta, Krishna Kariya, Sumit Gulwani, Arun Iyer, Suresh Parthasarathy Iyengar, Arjun Radhakrishna, Sriram K. Rajamani, Gustavo Soares",
    "url": "https://openreview.net/forum?id=Ql7msQBqoF",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Achieving Exact Federated Unlearning with Improved Post-Unlearning Performance",
    "abstract": "Keywords:Exact Federated Unlearning, Improved Post-Unlearning Performance, Multi-Models TrainingTL;DR:This paper proposes methods for federated learning that ensure exact federated unlearning while achieving a better performance post-unlearning than retraining from scratch.Abstract:Federated learning is a machine learning paradigm that allows multiple clients to train aggregated model via sharing model updates to a central server without sharing their data. Even though the data is not shared, it can indirectly influence the aggregated model via the shared model updates. In many real-life scenarios, we need to completely remove a client's influence (unlearning) from the aggregated model, such as competitive clients who want to remove their influence from the aggregated model after leaving the coalition to ensure other clients do not benefit from their contributions. The influence removal is also needed when the adversarial client negatively affects the aggregated model. Though the aggregated model can be retrained from scratch to ensure exact unlearning (completely removing the client's influence from the aggregated model), it performs poorly just after the unlearning, which is undesirable during deployment. To overcome this challenge, this paper proposes federated unlearning algorithms that ensure exact unlearning while achieving better performance post-unlearning. Our experimental results on different real datasets validate the performance of the proposed algorithms.Supplementary Material:zipPrimary Area:other topics in machine learning (i.e., none of the above)Code Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14278",
    "authors": "Ze Yu Zhang, Bui Thi Cam Nhung, Arun Verma, Bolin Ding, Bryan Kian Hsiang Low",
    "url": "https://openreview.net/forum?id=NHe6guO3l6",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Physics-Transfer Learning: A Framework to Address the Accuracy-Performance Dilemma in Modeling Complexity Problems in Engineering Sciences",
    "abstract": "Keywords:Physics-Transfer Learning; Accuracy-Performance Dilemma; Engineering Sciences; Complexity; Materials Strength; Brain DevelopmentAbstract:The development of theoretical sciences traditionally adheres to an observation-assumption-model paradigm, which is effective in simple systems but challenged by the `curse of complexity’ in modern engineering sciences. Advancements in artificial intelligence (AI) and machine learning (ML) offer a data-driven alternative, capable of interpolating and extrapolating scientific inference where direct solutions are intractable. Moreover, feature engineering in ML resembles dimensional analysis in classical physics, suggesting that data-driven ML methods could potentially extract new physics behind complex data. Here we propose a physics-transfer (PT) learning framework to learn physics across digital models of varying fidelities and complexities, which addresses the accuracy-performance dilemma in understanding representative multiscale problems. The capability of our approach is showcased through screening metallic alloys by their strengths and predicting the morphological development of brains. The physics of crystal plasticity is learned from low-fidelity molecular dynamics simulation and the model is then fed by material parameters from high-fidelity, electronic structures level, density functional theory calculations, offering chemically accurate strength predictions with several orders lower computational costs. The physics of bifurcation in the evolution of brain morphologies is learned from simple sphere and ellipsoid models and then applied to predict the morphological development of human brains, showing excellent agreement with longitudinal magnetic resonance imaging (MRI) data. The learned latent variables are shown to be highly relevant to uncovered physical descriptors, explaining the effectiveness of the PT framework, which holds great potential in closing the gaps in understanding complexity problems in engineering sciences.Supplementary Material:zipPrimary Area:applications to physical sciences (physics, chemistry, biology, etc.)Code Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14270",
    "authors": "Yingjie Zhao, Zhiping Xu",
    "url": "https://openreview.net/forum?id=llW4qRsF0o",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Exploring the Recall of Language Models: Case Study on Molecules",
    "abstract": "Keywords:recall, language models, molecular language models, sampling methods for language modelsTL;DR:We measure the recall of language models trained on molecular datasetsAbstract:Most of the current benchmarks evaluate Generative Language Models based on the accuracy of the generated output. However, in some scenarios, it is also important to evaluate the recall of the generations, i.e., whether a model can generate all correct outputs, such as all security vulnerabilities of a given codebase. There are two challenges in evaluating the recall: the lack of complete sets of correct outputs for any task and the existence of many distinct but similar outputs (e.g., two exploits that target the same vulnerability).\n\nIn this paper, we propose a benchmark from the domain of small organic molecules. We define several sets of molecules of varying complexity and fine-tune language models on subsets of those sets. We attempt to generate as many molecules from the target sets as possible and measure the recall, i.e., the percentage of generated molecules from the target set. We examine the impact of the training loss function and sampling strategy on the recall. We propose a sampling strategy based on beam search that avoids duplicates and maximizes recall. Finally, we show that given a small validation set, one can predict the recall of the model without actually generating many samples, which can act as a model selection strategy for maximizing generation recall.Primary Area:foundation or frontier models, including LLMsCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14268",
    "authors": "Philipp Guevorguian, Knarik Mheryan, Hasmik Mnatsakanyan, Hrant Khachatrian",
    "url": "https://openreview.net/forum?id=DlZ97cVwr0",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "SNAP-TTA: Sparse Test-Time Adaptation for Latency-Sensitive Applications",
    "abstract": "Keywords:Test-Time Adaptation, Unsupervised Domain AdaptationAbstract:Test-Time Adaptation (TTA) methods use unlabeled test data to dynamically adjust models in response to distribution changes. However, existing TTA methods are not tailored for practical use on edge devices with limited computational capacity, resulting in a latency-accuracy trade-off. To address this problem, we propose SNAP-TTA, a sparse TTA framework that significantly reduces adaptation frequency and data usage, delivering latency reductions proportional to adaptation rate. It achieves competitive accuracy even with an adaptation rate as low as 0.01, demonstrating its ability to adapt infrequently while utilizing only a small portion of the data relative to full adaptation. Our approach involves (i) Class and Domain Representative Memory (CnDRM), which identifies key samples that are both class-representative and domain-representative to facilitate adaptation with minimal data, and (ii) Inference-only Batch-aware Memory Normalization (IoBMN), which leverages representative samples to adjust normalization layers on-the-fly during inference, aligning the model effectively to changing domains. When combined with five state-of-the-art TTA algorithms, SNAP-TTA maintains the performances of these methods even with much-reduced adaptation rates from 0.01 to 0.5, making it suitable for edge devices serving latency-sensitive applications.Primary Area:transfer learning, meta learning, and lifelong learningCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14267",
    "authors": "Hyeongheon Cha, Dong Min Kim, Taesik Gong, Hye Won Chung, Sung-Ju Lee",
    "url": "https://openreview.net/forum?id=0vtftmYQGV",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Recovering Plasticity of Neural Networks via Soft Weight Rescaling",
    "abstract": "Keywords:loss of plasticity, plasticity, continual learning, online learningTL;DR:We propose a novel weight regularization method for recovering plasticity and improving generalization capability of the neural networks.Abstract:Recent studies have shown that as training progresses, neural networks gradually lose their capacity to learn new information, a phenomenon known as plasticity loss. An unbounded weight growth is one of the main causes of plasticity loss. Furthermore, it harms generalization capability and disrupts optimization dynamics. Re-initializing the network can be a solution, but it results in the loss of learned information, leading to performance drops. In this paper, we propose Soft Weight Rescaling (SWR), a novel approach that prevents unbounded weight growth without losing information. SWR recovers the plasticity of the network by simply scaling down the weight at each step of the learning process. We theoretically prove that SWR bounds weight magnitude and balances weight magnitude between layers. Our experiment shows that SWR improves performance on warm-start learning, continual learning, and single-task learning setups on standard image classification benchmarks.Primary Area:transfer learning, meta learning, and lifelong learningCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14266",
    "authors": "Seungwon Oh, Sangyeon Park, Isaac Han, Kyung-Joong Kim",
    "url": "https://openreview.net/forum?id=DnBjhWLVU1",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Shared Memory for Multi-agent Lifelong Pathfinding",
    "abstract": "Keywords:shared memory, transformers, multi-agent pathfindingAbstract:Multi-agent reinforcement learning (MARL) demonstrates significant progress in solving cooperative and competitive multi-agent problems in various environments. One of the main challenges in MARL is the need to explicitly predict other agents' behavior to achieve cooperation. As a solution to this problem, we propose the Shared Recurrent Memory Transformer (SRMT), which extends memory transformers to multi-agent settings by pooling and globally broadcasting individual working memories, enabling agents to implicitly exchange information and coordinate actions. We evaluate SRMT on the Partially Observable Multi-Agent Path Finding problem, both in a toy bottleneck navigation task requiring agents to pass through a narrow corridor and on a set of mazes from the POGEMA benchmark. In the bottleneck task, SRMT consistently outperforms a range of reinforcement learning baselines, especially under sparse rewards, and generalizes effectively to longer corridors than those seen during training. On POGEMA maps,  including Mazes, Random, and Warehouses, SRMT is competitive with a variety of recent MARL, hybrid, and planning-based algorithms. These results suggest that incorporating shared memory into transformer-based architectures can enhance coordination in decentralized multi-agent systems.Primary Area:reinforcement learningCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14258",
    "authors": "Alsu Sagirova, Yuri Kuratov, Mikhail Burtsev",
    "url": "https://openreview.net/forum?id=9DrPvYCETp",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "VeSX: A Framework Featured by Verification, Self-Correction and In-context Learning for Web Automation Tasks",
    "abstract": "Keywords:LLM agent, web automationAbstract:While large language models have achieved remarkable success in tasks such as reasoning and question answering, applying LLMs to interactive tasks like web automation remains challenging. In web automation, existing planning-execution workflow often faces limitations due to the infeasible subtasks. We propose VeSX, a framework designed to enhance subtask feasibility through verification, self-correction, and in-context learning. VeSX introduces three key improvements: (1) subgoal-guided verification, which verifies the execution results of subtasks based on the preset subgoals; (2) hierarchical self-correction, which combines reflection and replanning, targeting to self-correct mistakes in both planning and execution phases; (3) exemplar bank, which improves in-context learning by partitioning execution trajectories and heuristically generating metadata for exemplars. We evaluate VeSX on WebArena benchmark and achieve the state-of-the-art average success rate of 0.34, which significantly outperforms existing methods without human guidance on all five scenarios.Primary Area:foundation or frontier models, including LLMsCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14255",
    "authors": "Lin Li, Zhuyu Yao, Xinyi Yang, Boxun Li, Qingmin Liao, Yu Wang",
    "url": "https://openreview.net/forum?id=Of5F2GdGLA",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "OpenCity: A Scalable Platform to Simulate Urban Activities with Massive LLM Agents",
    "abstract": "Keywords:LLM Agent, Large Language Model, Urban StudyAbstract:Agent-based models (ABMs) have long been employed to explore how individual behaviors aggregate into complex societal phenomena in urban space. Unlike black-box predictive models, ABMs excel at explaining the micro-macro linkages that drive such emergent behaviors. The recent rise of Large Language Models (LLMs) has led to the development of LLM agents capable of simulating urban activities with unprecedented realism. However, scaling LLM agents to large city simulations presents significant challenges. Existing models are limited by the computational and communication costs of LLMs, compounded by the dynamic nature of urban environments that require continual updates to agent behavior. To address these limitations, we propose OpenCity, a scalable simulation platform optimized for both system and prompt efficiencies. Specifically, we propose a LLM request scheduler to reduce communication overhead by parallelizing requests through IO multiplexing. Besides, we deisgn a ``group-and-distill'' prompt optimization strategy minimizes redundancy by clustering agents with similar static attributes. Through experiments on six global cities, OpenCity achieves a 600-fold acceleration in simulation time per agent, a 70\\% reduction in LLM requests, and a 50\\% reduction in token usage. These improvements enable the simulation of 10,000 agents’ daily activities in 1 hour on commodity hardware. Additionally, OpenCity establishes a benchmark for LLM agents, comparing simulated mobility behaviors, origin-destination flows, and segregation indices against real-world data. We believe our OpenCity platform provides a critical infrastructure to harness the power of LLMs for interdisciplinary studies in urban space, fostering the collective efforts of broader research communities. Code repo is available at https://anonymous.4open.science/r/Anonymous-OpenCity-42BD.Primary Area:infrastructure, software libraries, hardware, systems, etc.Code Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14253",
    "authors": "Yuwei Yan, Qingbin Zeng, Zhiheng Zheng, Jingzhe Yuan, Jun Zhang, Jie Feng, Fengli Xu, Yong Li",
    "url": "https://openreview.net/forum?id=qK6U4Ahfms",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Structure-Aware Parameter-Efficient Machine Unlearning on Transformer Models",
    "abstract": "Keywords:Machine Unlearning, Parameter-Efficient, TransformerAbstract:Transformer has become fundamental to a vast series of pretrained large models that have achieved remarkable success across diverse applications. Machine unlearning is an emerging field focused on efficiently removing the influence of specific data from trained models, to comply with privacy regulations enforcing the right to be forgotten. The sheer size of Transformer-based models poses a significant challenge to unlearning efficiency. Existing methods find it promising to restrict unlearning updates to a small portion of influence-critical parameters. However, their parameter-efficient unlearning methods are largely devised in a structure-oblivious manner, which tends to inaccurately identify these parameters and leads to inferior unlearning performance for Transformers. In this paper, we propose {\\tt SPE-Unlearn}, a structure-aware parameter-efficient machine unlearning approach tailored for the Transformer architecture. {\\tt SPE-Unlearn} introduces a learnable pair of masks to respectively pinpoint influence-critical parameters in the heads and filters of Transformers. The learning objective of these masks is derived by jointly considering both desiderata of unlearning, i.e., sufficiency in influence removal and efficiency, and optimized through an efficient algorithm featured by a greedy search with a warm start. Equipped with the identified key parameters, {\\tt SPE-Unlearn} facilitates second-order unlearning, memory-free unlearning, and memory-aided unlearning scenarios. Extensive experiments on various transformer models and datasets demonstrate the effectiveness and efficiency of {\\tt SPE-Unlearn}~for Transformer unlearning.Primary Area:alignment, fairness, safety, privacy, and societal considerationsCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14252",
    "authors": "Wenjie Bao, Jian Lou, Yuke Hu, Xiaochen Li, Zhihao Liu, Jiaqi Liu, Zhan Qin, Kui Ren",
    "url": "https://openreview.net/forum?id=drrXhD2r8V",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Large-Scale Multi-Agent Reinforcement Learning for Traffic Signal Optimization",
    "abstract": "Keywords:Reinforcement Learning, Traffic Signal Control, Multi-Agent, TransformerAbstract:We present a novel approach to Traffic Signal Control (TSC) in a multi-agent environment by modeling communication among agents as a sequence problem, enabling intersections within road networks to communicate with one another. Taking inspiration from point cloud processing and graph neural networks, we make our architecture capable of handling variable road network topologies, including differing numbers of intersections and intersection types, and demonstrate this by successfully training on real & randomly generated road networks and traffic demands. Furthermore, we demonstrate that even utilizing minimal state information can achieve competitive performance.Primary Area:reinforcement learningCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14250",
    "authors": "Magnus Müller, Alexander Prochnow, Jonas Otten, Lionel Peer",
    "url": "https://openreview.net/forum?id=hWF0HH8Rr9",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "ESMGain: Effective and Efficient Prediction of Mutation’s functional Effect via ESM2 Transfer Learning and robust Benchmarks",
    "abstract": "Keywords:protein, language model, deep learning, biology, gain of function, enzymeTL;DR:ESMGain leverages ESM2 fine-tuning to predict functional effects of mutations, outperforming competitors by task-specific optimization, and introduces a benchmarking framework with harmonic Spearman as an accurate metric across effect types.Abstract:Functional effect prediction of mutations, especially for properties like catalytic activity, holds greater significance for clinicians and protein engineers than traditional pathogenicity predictions. Recent approaches leveraging static ESM1 embeddings or multimodal features (e.g. embeddings, structures, and evolutionary data) either (1) fall short in accuracy or (2) involve complex preprocessing pipelines. Moreover, functional effect prediction suffers from (3) a lack of standardized datasets and metrics for robust benchmarking. We address these challenges by systematically optimizing ESM2-based functional effect prediction: Through extensive ablation studies, we demonstrate that fine-tuning significantly outperforms static embeddings, scaling laws for model size are non-transferable and LoRA matches full fine-tuning performance, deviating from trends observed in natural language processing. Our framework, ESM-Effect, fine-tunes 35M ESM2 layers with an inductive bias regression head achieving state-of-the-art performance. It slightly surpasses multimodal competitor PreMode indicating redundancy in structural and evolutionary features. We further propose a benchmarking framework featuring robst test datasets and strategies, and the relative Bin-Mean Error (rBME), as a metric designed to emphasize prediction accuracy in challenging, non-clustered, and rare gain-of-function regions. rBME better reflects model performance compared to commonly used Spearman’s rho, as evidenced by improved plot-based analyses. As ESM-Effect exhibits mixed transferability to different unseen mutational regions, we identify multiple areas for improvement such as finer-grained pretraining strategies.Primary Area:applications to physical sciences (physics, chemistry, biology, etc.)Code Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14247",
    "authors": "Moritz Glaser",
    "url": "https://openreview.net/forum?id=vVlNBaiLdN",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Mitigating Privacy Risk of Adversarial Examples with Counterfactual Explanations",
    "abstract": "Keywords:Adversarial Examples, Privacy, Counterfactual ExplanationsTL;DR:We mitigated privacy risks of adversarial examples and used counterfactual explanation method to generate adversarial examples for the first time.Abstract:Robustness and privacy are two fundamental security properties that \nmachine learning models require. Without the balance between robustness and privacy leads to \nrobust models with high privacy risks. Obtaining machine learning models with high adversarial robustness and \nprivacy performance remains an open problem. In order to enhance the privacy performance of \nrobust models, we employ counterfactual explanations as a method \nto mitigate privacy risks while concurrently maintaining robust model accuracy, reducing the privacy risk of the robust model to the level of \nrandom guessing and using counterfactual explanations to generate adversarial examples for the first time. We analyze the similarities and differences between \nadversarial examples and counterfactual explanations and utilize these properties to design the \ngeneration method. We \nconduct an in-depth analysis of the advantages offered by counterfactual explanations compared \nto traditional adversarial examples. Our study indicates that the correlation between \nrobustness and privacy is strong and the ideal balance state of accuracy, robustness, and privacy is with 95\\% \nadversarial examples involved in model training.Supplementary Material:zipPrimary Area:interpretability and explainable AICode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14241",
    "authors": "Aohan Sun, Yanrong Lu, ATHANASIOS V. VASILAKOS",
    "url": "https://openreview.net/forum?id=gaa7gWPZBz",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "GenBen:A Genarative Benchmark for LLM-Aided Design",
    "abstract": "Keywords:GenBen; Benchmark; LLM-Aided Design; LLM; Hardware DesignTL;DR:An Open Source Benchmark for LLM-Aided Hardware DesignAbstract:This paper introduces GenBen, a generative benchmark designed to evaluate the capabilities of large language models (LLMs) in hardware design. With the rapid advancement of LLM-aided design (LAD), it has become crucial to assess the effectiveness of these models in automating hardware design processes.\nExisting benchmarks primarily focus on hardware code generation and often neglect critical aspects such as Quality-of-Result (QoR) metrics, design diversity, modality, and test set contamination. GenBen is the first open-source, generative benchmark tailored for LAD that encompasses a range of tasks, from high-level architecture to low-level circuit optimization, and includes diverse, silicon-proven hardware designs. \nWe have also designed a difficulty tiering mechanism to provide fine-grained insights into enhancements of LLM-aided designs. Through extensive evaluations of several state-of-the-art LLMs using GenBen, we reveal their strengths and weaknesses in hardware design automation. Our findings are based on 10,920 experiments and 2,160 hours of evaluation, underscoring the potential of this work to significantly advance the LAD research community. \nIn addition, both GenBen employs an end-to-end testing infrastructure to ensure consistent and reproducible results across different LLMs. The benchmark is available at https://anonymous.4open.science/r/GENBEN-2812.Primary Area:datasets and benchmarksCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14237",
    "authors": "Gwok-Waa Wan, Wang yubo, SamZaak Wong, jingyi zhang, Mengnv Xing, Zhe jiang, Nan Guan, ying wang, Ning Xu, Qiang Xu, Xi Wang",
    "url": "https://openreview.net/forum?id=gtVo4xcpFI",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "DySpec: Faster Speculative Decoding with Dynamic Token Tree Structure",
    "abstract": "Keywords:inference methods, efficient inference, speculative decodingAbstract:While speculative decoding has recently appeared as a promising direction for accelerating the inference of large language models (LLMs), the speedup and scalability are strongly bounded by the token acceptance rate.\nPrevalent methods usually organize predicted tokens as independent chains or fixed token trees, which fails to generalize to diverse query distributions. \nIn this paper, we propose \\textsc{DySpec}, a faster speculative decoding algorithm with a novel dynamic token tree structure. \nWe begin by bridging the draft distribution and acceptance rate from \nintuitive and empirical clues, and successfully show that the two variables are strongly correlated. Based on this, we employ a greedy strategy to dynamically expand the token tree at run time. Theoretically, we show that our method can achieve optimal results under mild assumptions. Empirically, \\textsc{DySpec} yields a higher acceptance rate and speedup than fixed trees. \\textsc{DySpec} can drastically improve the throughput and reduce the latency of token generation across various data distribution and model sizes, which significantly outperforms strong competitors, including Specinfer and Sequoia. Under low temperature setting, \\textsc{DySpec} can improve the throughput up to 9.10x and reduce the latency up to 9.4x on Llama2-70B. Under high temperature setting, \\textsc{DySpec} can also improve the throughput up to 6.21x, despite the increasing difficulty of speculating more than one token per step for draft model.Primary Area:infrastructure, software libraries, hardware, systems, etc.Code Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14236",
    "authors": "Yunfan Xiong, Ruoyu Zhang, Yanzeng Li, Tianhao Wu, Lei Zou",
    "url": "https://openreview.net/forum?id=orr5uPZY28",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "StreamingBench: Assessing the Gap for MLLMs to Achieve Streaming Video Understanding",
    "abstract": "Keywords:Benchmark, Streaming Video Understanding, Multimodal Large Language Models, Video Benchmark, EvaluationAbstract:The rapid development of Multimodal Large Language Models (MLLMs) has expanded their capabilities from image comprehension to video understanding. However, most of these MLLMs focus primarily on ofﬂine video comprehension, necessitating extensive processing of all video frames before any queries can be made. This presents a signiﬁcant gap compared to the human ability to watch, listen, think, and respond to streaming inputs in real time, highlighting the limitations of current MLLMs. In this paper, we introduce StreamingBench, the ﬁrst comprehensive benchmark designed to evaluate the streaming video understanding capabilities of MLLMs. StreamingBench assesses three core aspects of streaming video understanding: (1) real-time visual understanding, (2) omni-source understanding and (3) contextual understanding. The benchmark consists of 18 tasks, featuring 900 videos and 4,500 human-curated QA pairs. Each video features ﬁve questions presented at different time points to simulate a continuous streaming scenario. We conduct experiments on StreamingBench with 15 open-source and proprietary MLLMs and ﬁnd that even the most advanced proprietary MLLMs like Gemini 1.5 Pro and GPT-4o perform signiﬁcantly below human-level streaming video understanding capabilities. We hope our work can facilitate further advancements for MLLMs, empowering them to approach human-level video comprehension and interaction in more realistic scenarios.Primary Area:datasets and benchmarksCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14232",
    "authors": "Junming Lin, Zheng Fang, Zihao Wan, Fuwen Luo, Chi Chen, Peng Li, Yang Liu, Maosong Sun",
    "url": "https://openreview.net/forum?id=qnAZqlMGTB",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Toward Human-Interpretable Explanations in a Unified Framework for GNNs",
    "abstract": "Keywords:eXplainable AI, Graph Neural NetworksTL;DR:Toward Human-Interpretable Explanations in a Unified Framework for GNNsAbstract:As Graph Neural Networks (GNNs) are increasingly applied across various domains, explainability has become a critical factor for real-world applications. Existing post-hoc explainability methods primarily focus on estimating the importance of edges, nodes, or subgraphs in the input graph to identify substructures crucial for predictions. However, these methods often lack human interpretability and do not provide a unified framework that incorporates both model-level and instance-level explanations. In this context, we propose leveraging a set of graphlets---small, connected, non-isomorphic induced subgraphs widely used in various scientific fields---and their associated orbits as human-interpretable units to decompose GNN predictions. Domain experts can select the most relevant graphlets as interpretable units and request unified explanations based on these units. To address this problem, we introduce UO-Explainer, the Unified and Orbit-based Explainer for GNNs, which utilizes predefined orbits that are generalizable and universal across graph domains as interpretable units. Our model decomposes GNN weights into orbit units to extract class-specific graph patterns (model-level) and to identify important subgraphs within individual data instances for prediction (instance-level). Extensive experimental results demonstrate that UO-Explainer outperforms existing baselines in providing meaningful and interpretable explanations across both synthetic and real-world datasets.Primary Area:interpretability and explainable AICode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14225",
    "authors": "Kyeongrok Park, Hyunju Kang, Hogun Park",
    "url": "https://openreview.net/forum?id=N0MnPLK6r7",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "The Good, the Bad and the Ugly: Watermarks, Transferable Attacks and Adversarial Defenses",
    "abstract": "Keywords:Watermarks, Adversarial Defenses, Transferable Attacks, Interactive Proof Systems, Cryptography, Backdooring, Game Theory, Learning TheoryTL;DR:We show that for all classification tasks, at least one of the following exists: a watermark, an adversarial defense, or a transferable attack, with the latter tied to cryptography.Abstract:We formalize and extend existing definitions of backdoor-based watermarks and adversarial defenses as *interactive protocols* between two players. The existence of these schemes is inherently tied to the learning tasks for which they are designed. Our main result shows that for *almost every* discriminative learning task, at least one of the two — a watermark or an adversarial defense — exists. The \"*almost*\" refers to the fact that we also identify a third, counterintuitive but necessary option, i.e., a scheme we call a *transferable attack*. By transferable attack, we refer to an efficient algorithm computing queries that look indistinguishable from the data distribution and fool *all* efficient defenders.\n\nTo this end, we prove the necessity of a transferable attack via a construction that uses a cryptographic tool called homomorphic encryption. Furthermore, we show that any task that satisfies our notion of a transferable attack implies a *cryptographic primitive*, thus requiring the underlying task to be computationally complex. These two facts imply an \"*equivalence*\" between the existence of transferable attacks and cryptography. Finally, we show that the class of tasks of bounded VC-dimension has an adversarial defense, and a subclass of them has a watermark.Primary Area:alignment, fairness, safety, privacy, and societal considerationsCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14223",
    "authors": "Grzegorz Gluch, Berkant Turan, Sai Ganesh Nagarajan, Sebastian Pokutta",
    "url": "https://openreview.net/forum?id=wE5xp3zBaQ",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Common Feature Learning for Zero-shot Image Recognition",
    "abstract": "Keywords:Zero-shot Image Recognition；Visual-semantic Relationship；Fine-grained Alignment；Semantic Vectors Generation；Abstract:The key issue of zero-shot image recognition (ZIR)  is how to infer the relationship between visual space and semantic space from seen classes, and then effectively transfer the  relationship to unseen classes. Recently, most methods have focused on how to use images and class semantic vectors or class names to learn the relationship between visual space and semantic space. The relationship established by these two methods is class-level and coarse-grained. The differences between images of the same class are ignored, which leads to insufficiently tight relationships and affects the accurate recognition of unseen classes.To tackle such problem, we propose Common Feature learning for Zero-shot Image Recognition (CF-ZIR) method to learn fine-grained visual semantic relationships at the image-level. Based on the inter class association information provided by class semantic vectors, guide the extraction of common visual features between classes to obtain image semantic vectors. Experiments on three widely used benchmark datasets show the effectiveness of the proposed approach.Primary Area:unsupervised, self-supervised, semi-supervised, and supervised representation learningCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14222",
    "authors": "Shuang Li, Lichun WANG, Kai Xu, Jianjia Xin",
    "url": "https://openreview.net/forum?id=pcnq7fZs4t",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Flow Matching for One-Step Sampling",
    "abstract": "Keywords:Flow Matching, Generative Models, Ordinary Differential Equations, One-step generationTL;DR:The paper proposes a Flow Matching-based approach, that eliminates ODE solvers during sampling, speeding up without sacrificing performance.Abstract:Flow-based generative models have rapidly advanced as a method for mapping simple distributions to complex ones for which the distribution function is unknown. By leveraging continuous-time stochastic processes, these models offer a powerful framework for density estimation, i.e. an algorithm that samples new points based only on existing samples. However, their requirement of solving ordinary differential equations (ODEs) during sampling process incurs substantial computational costs, particularly for large amount of data and numerous time points. This paper proposes a novel solution, which is based on a theoretical analysis of Flow Matching (FM), to overcome this bottleneck, namely, we developed an algorithm to find the point prototype for a given point from the target distribution. By eliminating the need for ODE solvers, our method significantly accelerates sampling while preserving model performance. Numerical experiments validate the proposed approach, demonstrating its efficiency.Primary Area:generative modelsCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14213",
    "authors": "Svetlana Pavlova, Ivan Oseledets, Gleb Ryzhakov",
    "url": "https://openreview.net/forum?id=WxLwXyBJLw",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "StarCraft II Arena: Evaluating LLMs in Strategic Planning, Real-Time Decision Making, and Adaptability",
    "abstract": "Keywords:benchmark evaluation, large language model, LLM-based agent, strategic reasoning, real-time decision-making.TL;DR:A benchmark for evaluating large language models in StarCraft II, focusing on strategic planning, real-time decision-making, and adaptability using fine-grained capability metrics and decision trace analysis.Abstract:StarCraft II plays an important role in developing AI agents for real-time strategic reasoning due to its complex nature. However, people usually draw conclusions of how competent their agents are according to the level of the built-in agents in StarCraft II which they can win in terms of the final success rate. Little intermediate quantitative information is considered while human-in-the-loop analysis is time inefficient, which results in inadequate reflection of the true strategic reasoning ability. In this work, we propose StarCraft II Arena, a well-designed benchmark for evaluating the strategic planning, real-time decision-making, and adaptability capabilities of large language models (LLMs) agents. We introduce using fine-grained capability metrics, allowing for targeted capture and analysis of specific capability, and further propose a detailed decision trace to enhance the understanding of LLM behavior. We demonstrate the utility of such a benchmark by evaluating several state-of-the-art LLMs in various setups. Our results reveal distinct performances in long-term strategy development, real-time decision-making, and adapting to environmental changes. Such results show that the StarCraft II Arena offers a deeper insight into the decision-making process of LLMs and has the potential to become a challenging and comprehensive benchmark for strategic reasoning.Primary Area:datasets and benchmarksCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14211",
    "authors": "Wenjie Tang, Yuan Zhou, Erqiang Xu, Keyan Cheng, Minne Li, Zhiyuan Wang",
    "url": "https://openreview.net/forum?id=o3V7OuPxu4",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Neuroacoustic Patterns: Constant Q Cepstral Coefficients for the Classification of Neurodegenerative Disorders",
    "abstract": "Keywords:Neurodegenerative Disorder, Constant Q Cepstral Coefficient, Form Invariance, Random Forest, SVM.Abstract:Early identification of neurodegenerative diseases is crucial for effective diagnosis in neurological disorders. However, the quasi-periodic nature of vocal tract sampling often results in inadequate spectral resolution in traditional spectral features, such as Mel Frequency Cepstral Coefficients (MFCC), thereby limiting their classification effectiveness. In this study, we propose the use of Constant Q Cepstral Coefficients (CQCC), which leverage geometrically spaced frequency bins to provide superior spectrotemporal resolution, particularly for capturing the fundamental frequency and its harmonics in speech signals associated with neurodegenerative disorders. Our results demonstrate that CQCC, when integrated with Random Forest and Support Vector Machine classifiers, significantly outperform MFCC, achieving absolute improvements of 5.6 % and 7.7 %, respectively. Furthermore, CQCC show enhanced performance over traditional acoustic measures, such as Jitter, Shimmer, and Teager Energy. The effectiveness of CQCC is underpinned by the form-invariance property of the Constant Q Transform (CQT), which ensures consistent feature representation across varying pitch and tonal conditions, thereby enhancing classification robustness. Furthermore, the robustness of CQCC features against MFCC features are validated using LDA plots. These findings are validated using the Italian Parkinson’s database and the Minsk2019 database of Amyotrophic Lateral Sclerosis, underscoring the potential of CQCC to advance the classification of neurodegenerative disorders.Primary Area:applications to neuroscience & cognitive scienceCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Reciprocal Reviewing:I understand the reciprocal reviewing requirement as described on https://iclr.cc/Conferences/2025/CallForPapers. If none of the authors are registered as a reviewer, it may result in a desk rejection at the discretion of the program chairs. To request an exception, please complete this form at https://forms.gle/Huojr6VjkFxiQsUp6.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14296",
    "authors": "Aastha Kachhi, Shashank Ojha, Megha Pandey, Ajay Kumar Sharma, Anurag Pandey",
    "url": "https://openreview.net/forum?id=5sRnsubyAK",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "A Feature-Aware Federated Learning Framework for Unsupervised Anomaly Detection in 5G Networks",
    "abstract": "Keywords:Federated Learning, Anomaly Detection, 5G Networks, Privacy-PreservingAbstract:The expansion of 5G networks has led to remarkable data volume and complexity, introducing significant security challenges that require the implementation of robust and scalable anomaly detection mechanisms. Traditional centralized approaches pose privacy risks and scalability challenges due to the distributed nature of 5G infrastructures. Federated Learning (FL) offers a decentralized solution but often overlooks the importance of feature relevance and privacy preservation during model aggregation. This paper introduces a novel Feature-Aware Federated framework that integrates feature importance into the aggregation process while ensuring differential privacy. We employ integrated gradients to compute feature importance for each client, aggregate them globally with differential privacy noise, and use these insights to weight model parameters during aggregation. Additionally, we propose Dynamic Feature Importance Adaptation (DFIA) to update feature importance occasionally, enhancing the model's adaptability to evolving data distributions. Experimental results demonstrate that our framework outperforms traditional federated approaches like FedAvg and FedProx in unsupervised anomaly detection tasks within 5G networks, achieving higher accuracy and robustness while preserving data privacy.Primary Area:other topics in machine learning (i.e., none of the above)Code Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Reciprocal Reviewing:I understand the reciprocal reviewing requirement as described on https://iclr.cc/Conferences/2025/CallForPapers. If none of the authors are registered as a reviewer, it may result in a desk rejection at the discretion of the program chairs. To request an exception, please complete this form at https://forms.gle/Huojr6VjkFxiQsUp6.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14294",
    "authors": "Saeid Sheikhi",
    "url": "https://openreview.net/forum?id=J1SGf2lyr6",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "UnoLoRA: Single Low-Rank Adaptation for Efficient Multitask Fine-tuning",
    "abstract": "Keywords:lora, multi-task learning, peftTL;DR:Insights into using a single LoRA adapter for multi-task learning, and the actual low-rank representations and how they generalise across tasks.Abstract:Recent advances in Parameter-Efficient Fine-Tuning (PEFT) have shown Low- Rank Adaptation (LoRA) to be an effective implicit regularizer for large language models. Building on these findings, we propose UnoLoRA, a novel approach that leverages a single shared LoRA module for efficient multi-task learning. While existing methods typically use separate LoRA adaptations for each task, our approach demonstrates that a single shared adapter can effectively capture both task-specific and task-agnostic knowledge. We further introduce UnoLoRA*, an enhanced variant that employs a shared hypernetwork to generate task-specific embeddings, improving convergence and task adaptation. Our method significantly reduces trainable parameters to just 0.05% per task while maintaining competitive performance on the GLUE benchmark. Our analysis reveals that the A and B matrices in our shared LoRA adapter naturally develop complementary roles: A matrices capture generalizable features across tasks, while B matrices specialize in task-specific representations. Our results show that sharing a single LoRA adapter can achieve efficient multi-task learning while significantly reducing memory requirements, making it particularly valuable for resource-constrained applications.Supplementary Material:zipPrimary Area:other topics in machine learning (i.e., none of the above)Code Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Reciprocal Reviewing:I understand the reciprocal reviewing requirement as described on https://iclr.cc/Conferences/2025/CallForPapers. If none of the authors are registered as a reviewer, it may result in a desk rejection at the discretion of the program chairs. To request an exception, please complete this form at https://forms.gle/Huojr6VjkFxiQsUp6.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14293",
    "authors": "Anirudh Lakhotia, Akash Kamalesh, Prerana Sanjay Kulkarni, Nischal H S, Gowri Srinivasa",
    "url": "https://openreview.net/forum?id=49ti6LOUw5",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Extending Flexibility of Image Coding Enhancement Framework for IoTs",
    "abstract": "Keywords:Data Compression, IoT infrastructure, Edge Computing, Scalable DesignAbstract:Neural image compression, necessary in various edge-device scenarios, suffers from its heavy encode-decode structures and inflexible compression level switch. The primary issue is that the computational and storage capabilities of edge devices are weaker than those of servers, preventing them from handling the same amount of computation and storage. One solution is to downsample images and reconstruct them on the receiver side; however, current methods uniformly downsample the image and limit flexibility in compression levels. We take a step to break up this paradigm by proposing a conditional uniform-based sampler that allows for flexible image size reduction and reconstruction. Building on this, we introduce a lightweight transformer-based reconstruction structure to further reduce the reconstruction load on the receiver side. Extensive evaluations conducted on a real-world testbed demonstrate multiple advantages of our system over existing compression techniques, especially in terms of adaptability to different compression levels, computational efficiency, and image reconstruction quality.Primary Area:infrastructure, software libraries, hardware, systems, etc.Code Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Reciprocal Reviewing:I understand the reciprocal reviewing requirement as described on https://iclr.cc/Conferences/2025/CallForPapers. If none of the authors are registered as a reviewer, it may result in a desk rejection at the discretion of the program chairs. To request an exception, please complete this form at https://forms.gle/Huojr6VjkFxiQsUp6.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14271",
    "authors": "Yu Mao, Jingzong LI, Jun Wang, Hong Xu, Tei-Wei Kuo, Nan Guan, Chun Jason Xue",
    "url": "https://openreview.net/forum?id=LJWPYzjDz4",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Stochastic Sparse Sampling: A Framework for Variable-Length Medical Time Series Classification",
    "abstract": "Keywords:Time Series, Healthcare, Medicine, Epilepsy, NeuroscienceAbstract:ile the majority of time series classification research has focused on modeling fixed-length sequences, variable-length time series classification (VTSC) remains critical in healthcare, where sequence length may vary among patients and events. To address this challenge, we propose $\\textbf{S}$tochastic $\\textbf{S}$parse $\\textbf{S}$ampling (SSS), a novel VTSC framework developed for medical time series. SSS manages variable-length sequences by sparsely sampling fixed windows to compute local predictions, which are then aggregated and calibrated to form a global prediction. We apply SSS to the task of seizure onset zone (SOZ) localization, a critical VTSC problem requiring identification of seizure-inducing brain regions from variable-length electrophysiological time series. We evaluate our method on the Epilepsy iEEG Multicenter Dataset, a heterogeneous collection of intracranial electroencephalography (iEEG) recordings obtained from four independent medical centers. SSS demonstrates superior performance compared to state-of-the-art (SOTA) baselines across most medical centers, and superior performance on all out-of-distribution (OOD) unseen medical centers. Additionally, SSS naturally provides post-hoc insights into local signal characteristics related to the SOZ, by visualizing temporally averaged local predictions throughout the signal.Primary Area:learning on time series and dynamical systemsCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Reciprocal Reviewing:I understand the reciprocal reviewing requirement as described on https://iclr.cc/Conferences/2025/CallForPapers. If none of the authors are registered as a reviewer, it may result in a desk rejection at the discretion of the program chairs. To request an exception, please complete this form at https://forms.gle/Huojr6VjkFxiQsUp6.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14254",
    "authors": "Xavier Mootoo, Alan Arnoldo Diaz Montiel, Milad Lankarany, Hina Tabassum",
    "url": "https://openreview.net/forum?id=Y0kmI2zqqi",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "MathOdyssey: Benchmarking Mathematical Problem-Solving Skills in Large Language Models Using Odyssey Math Data",
    "abstract": "Keywords:Math, LLMsAbstract:Large language models (LLMs) have significantly advanced natural language understanding and demonstrated strong problem-solving abilities. Despite these successes, most LLMs still struggle with solving mathematical problems due to the intricate reasoning required. This paper investigates the mathematical problem-solving capabilities of LLMs using the newly developed ``MathOdyssey'' dataset. The dataset includes diverse mathematical problems at high school and university levels, created by experts from notable institutions to rigorously test LLMs in advanced problem-solving scenarios and cover a wider range of subject areas. By providing the MathOdyssey dataset as a resource to the AI community, we aim to contribute to the understanding and improvement of AI capabilities in complex mathematical problem-solving. We conduct benchmarking on open-source models, such as Llama-3, and closed-source models from the GPT series and Gemini models. Our results indicate that while LLMs perform well on routine and moderately difficult tasks, they face significant challenges with Olympiad-level problems and complex university-level questions. Our analysis shows a narrowing performance gap between open-source and closed-source models, yet substantial challenges remain, particularly with the most demanding problems. This study highlights the ongoing need for research to enhance the mathematical reasoning of LLMs. \nThe dataset, results, and evaluation code are publicly available.Primary Area:datasets and benchmarksCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Reciprocal Reviewing:I understand the reciprocal reviewing requirement as described on https://iclr.cc/Conferences/2025/CallForPapers. If none of the authors are registered as a reviewer, it may result in a desk rejection at the discretion of the program chairs. To request an exception, please complete this form at https://forms.gle/Huojr6VjkFxiQsUp6.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14249",
    "authors": "Meng Fang, Xiangpeng Wan, Fei Lu, Fei Xing, Kai Zou",
    "url": "https://openreview.net/forum?id=owR9ofvkFQ",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "MOMENTUM MEETS VIRALITY: A NOVEL METRIC FOR UNMASKING SOCIAL BIAS IN VIRAL TWEETS",
    "abstract": "Keywords:Social bias, Tweet virality, ViralTweetScore, Hindi Tweets, TweetsTL;DR:We introduce a novel metric, ViralTweet Score (VTS), inspired by momentum principles, to better predict tweet virality, compare it with existing metrics, and highlight how social biases influence virality, using a dataset of 88.8k Hindi tweets.Abstract:Predicting which social media posts will go viral is a critical but complex task in the field of computational social science. Previous studies have utilized various measures to forecast the virality of tweets or Facebook posts, but these approaches exhibit limitations, particularly in the absence of a virality metric that specifically considers social biases. In this paper, we test existing metrics and introduce a new metric, $\\textbf{ViralTweet Score (VTS)}$, inspired by principles of momentum from physics to better predict a tweet's virality given that it consists of social biases. We compare this new metric with others, highlighting the advantages and disadvantages of each of them as a virality measurement metric. We release the $\\textbf{ViralTweets Dataset}$ with $\\mathbf{88.8k}$ Hindi tweets and corresponding virality labels based on our VTS metric. We also show how social biases in posts can influence their potential to go viral. We test our hypothesis that VTS is a better metric using two methodologies and we show how VTS achieves an F1 score of 0.87 based on pairwise evaluation methodology and an overall F1 score of 0.58 based on our clustering-based verification methodology. Our work offers a novel metric for understanding tweet virality for biased tweets and opens the door for more equitable and effective social media analytics by considering the role of social biases in virality.Primary Area:alignment, fairness, safety, privacy, and societal considerationsCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Reciprocal Reviewing:I understand the reciprocal reviewing requirement as described on https://iclr.cc/Conferences/2025/CallForPapers. If none of the authors are registered as a reviewer, it may result in a desk rejection at the discretion of the program chairs. To request an exception, please complete this form at https://forms.gle/Huojr6VjkFxiQsUp6.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14245",
    "authors": "Nihar Ranjan Sahoo, Arif Ahmad, Nishtha Madaan, Pushpak Bhattacharyya",
    "url": "https://openreview.net/forum?id=FYvZCwdb6F",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "KARPA: A Training-free Method of Adapting Knowledge Graph as References for Large Language Model's Reasoning Path Aggregation",
    "abstract": "Keywords:Knowledge Graph, Large Language Models, Chain-of-Thought, ReasoningTL;DR:We introduce KG Assisted Reasoning Path Aggregation (KARPA), a novel framework that enhances LLM-based KG reasoning by leveraging the global planning capabilities of LLMs, enabling efficient and accurate question answering without training process.Abstract:Large language models (LLMs) demonstrate exceptional performance across a variety of tasks, yet they are often affected by hallucinations and the timeliness of knowledge. Leveraging knowledge graphs (KGs) as external knowledge sources has emerged as a viable solution, but existing methods for LLM-based knowledge graph question answering (KGQA) are often limited by step-by-step decision-making on KGs, restricting the global planning and reasoning capabilities of LLMs, or they require fine-tuning or pre-training on specific KGs. To address these challenges, we propose Knowledge graph Assisted Reasoning Path Aggregation (KARPA), a novel framework that harnesses the global planning abilities of LLMs for efficient and accurate KG reasoning on KGs. KARPA operates through a three-step process: pre-planning, retrieving, and reasoning. First, KARPA uses the LLM's global planning ability to pre-plan logically coherent relation paths based on the provided question and relevant relations within the KG. Next, in the retrieving phase, relation paths with high semantic similarity to the pre-planned paths are extracted as candidate paths using a semantic embedding model. Finally, these candidate paths are provided to the LLM for comprehensive reasoning. Unlike existing LLM-based KGQA methods, KARPA fully leverages the global planning and reasoning capabilities of LLMs without requiring stepwise traversal or additional training, and it is compatible with various LLM architectures. Extensive experimental results show that KARPA achieves state-of-the-art performance in KGQA tasks, delivering both high efficiency and accuracy.Primary Area:neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)Code Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Reciprocal Reviewing:I understand the reciprocal reviewing requirement as described on https://iclr.cc/Conferences/2025/CallForPapers. If none of the authors are registered as a reviewer, it may result in a desk rejection at the discretion of the program chairs. To request an exception, please complete this form at https://forms.gle/Huojr6VjkFxiQsUp6.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14243",
    "authors": "Siyuan Fang, Kaijing Ma, Tianyu Zheng, Xeron Du, Ningxuan Lu, Ge Zhang, Qingkun Tang",
    "url": "https://openreview.net/forum?id=Hw1tOjCWBZ",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Hottel Zone Physics-Constrained Networks for Furnaces",
    "abstract": "Keywords:Hottel Zone method, Physics-Informed Neural Networks, Radiation Heat Transfer, FurnacesAbstract:This paper investigates a novel approach to improve the temperature profile prediction of furnaces in foundation industries, crucial for sustainable manufacturing. While existing methods like the Hottel Zone model are accurate, they lack real-time inference capabilities. Deep learning methods excel in speed and prediction but require careful generalization for real-world applications. We propose a regularization technique that leverages the Hottel Zone method to make deep neural networks physics-aware, improving prediction accuracy for furnace temperature profiles. Our approach demonstrates effectiveness on various neural network architectures, including Multi-Layer Perceptrons (MLP), Long Short-Term Memory (LSTM) and Kolmogorov-Arnold Networks (KANs). We also discussion the data generation involved.Primary Area:applications to physical sciences (physics, chemistry, biology, etc.)Code Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Reciprocal Reviewing:I understand the reciprocal reviewing requirement as described on https://iclr.cc/Conferences/2025/CallForPapers. If none of the authors are registered as a reviewer, it may result in a desk rejection at the discretion of the program chairs. To request an exception, please complete this form at https://forms.gle/Huojr6VjkFxiQsUp6.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14235",
    "authors": "Ujjal Kr Dutta, Aldo Lipani, Chuan Wang, Yukun Hu",
    "url": "https://openreview.net/forum?id=hz3NtNpDNv",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Learning through Conditioning on Natural Language Feedback",
    "abstract": "Keywords:Social Learning, Natural Language Feedback, Instructive LearningTL;DR:We explore whether we can finetune language models by letting them generate answers conditioned on prior feedback.Abstract:In this paper we explore the simple idea of teaching models by allowing them to condition their answers on natural language feedback. Motivated by the idea that natural language interactions provide a targeted, flexible, and level-appropriate reward signal, we study the ability of small instruction-tuned models to leverage feedback from a larger frontier model. We find while the frontier model provides generally high quality feedback, especially smaller models can struggle to use this due to noise in their generative output. After incorporating techniques like negative sampling, we find that models trained on these feedback-conditioned responses can perform similarly to those trained directly on teacher responses. We explore training using supervised finetuning and preference learning algorithms over a broad set of tasks including Big-Bench Hard. These findings are broadly applicable and our methods rely only on the ability of models to give and receive linguistic feedback. As such, they contribute to a growing body of work exploring how to best utilise the linguistic capabilities of language models for human-like instructive learning.Primary Area:foundation or frontier models, including LLMsCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Reciprocal Reviewing:I understand the reciprocal reviewing requirement as described on https://iclr.cc/Conferences/2025/CallForPapers. If none of the authors are registered as a reviewer, it may result in a desk rejection at the discretion of the program chairs. To request an exception, please complete this form at https://forms.gle/Huojr6VjkFxiQsUp6.Resubmission:NoAnonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14230",
    "authors": "Dylan Hillier, Cheston Tan, Jing Jiang",
    "url": "https://openreview.net/forum?id=2Sn0ty7zoI",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Learning Fused State Representations for Control from Multi-View Observations",
    "abstract": "Keywords:multi-view learning, reinforcement learningTL;DR:We proposed a method to learn fused state representations for multi-view RL.Abstract:In visual control tasks, leveraging observations from multiple views enables Reinforcement Learning (RL) agents to perceive the environment more effectively. However, while multi-view observations enrich decision-making information, they also increase the dimension of observation space and introduce more redundant information. Thus, how to learn compact and task-relevant representations from multi-view observations for downstream RL tasks remains a challenge. In this paper, we propose a Multi-view Fusion State for Control (MFSC), which integrates a self-attention mechanism with bisimulation metric learning to fuse task-relevant representations from multi-view observations. To foster more compact fused representations, we also incorporate a mask-based latent reconstruction auxiliary task to learn cross-view information. Additionly, this mechanism of mask and reconstruction can enpower the model with the ability to handle missing views by learning an additional mask tokens. We conducted extensive experiments on the Meta-World and Pybullet benchmarks, and the results demonstrate that our proposed method outperforms other multi-view RL algorithms and effectively aggregates task-relevant details from multi-view observations, coordinating attention across different views.Primary Area:reinforcement learningCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Reciprocal Reviewing:I understand the reciprocal reviewing requirement as described on https://iclr.cc/Conferences/2025/CallForPapers. If none of the authors are registered as a reviewer, it may result in a desk rejection at the discretion of the program chairs. To request an exception, please complete this form at https://forms.gle/Huojr6VjkFxiQsUp6.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14220",
    "authors": "Zeyu Wang, Yao-Hui Li, Hongyu Zang, Xin Li",
    "url": "https://openreview.net/forum?id=3g2iyFU8gA",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Exposing Privacy Gaps: Membership Inference Attack on Preference Data for LLM Alignment",
    "abstract": "Keywords:privacy, LLM alignment, rlhfAbstract:Large Language Models (LLMs) have seen widespread adoption due to their remarkable natural language capabilities. However, when deploying them in real-world settings, it is important to align LLMs to generate texts according to acceptable human standards. Methods such as Proximal Policy Optimization (PPO) and Direct Preference Optimization (DPO) have enabled significant progress in refining LLMs using human preference data. However, the privacy concerns inherent in utilizing such preference data have yet to be adequately studied. In this paper, we investigate the vulnerability of LLMs aligned using two widely used methods - DPO and PPO - to membership inference attacks (MIAs). Our study has two main contributions: first, we theoretically motivate that DPO models are more vulnerable to MIA compared to PPO models; second, we introduce a novel reference-based attack framework specifically for analyzing preference data called PREMIA (\\uline{Pre}ference data \\uline{MIA}). Using PREMIA and existing baselines we empirically show that DPO models have a relatively heightened vulnerability towards MIA.Primary Area:foundation or frontier models, including LLMsCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Reciprocal Reviewing:I understand the reciprocal reviewing requirement as described on https://iclr.cc/Conferences/2025/CallForPapers. If none of the authors are registered as a reviewer, it may result in a desk rejection at the discretion of the program chairs. To request an exception, please complete this form at https://forms.gle/Huojr6VjkFxiQsUp6.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14219",
    "authors": "Qizhang Feng, Siva Rajesh Kasa, SANTHOSH KUMAR KASA, Hyokun Yun, Choon Hui Teo, Sravan Babu Bodapati",
    "url": "https://openreview.net/forum?id=p7vItQ3OfD",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Alternating Optimized Stochastic Vector Quantization in Neural Compression",
    "abstract": "Keywords:vector quantization, neural compression, image compressionAbstract:In neural compression, vector quantization (VQ) is usually replaced by a differentiable approximation during training for gradient backpropagation. However, prior approximation methods face two main issues: 1) the train-test mismatch between differentiable approximation and actual quantization, and 2) the suboptimal encoder gradients for rate-distortion (RD) optimization. In this paper, we first provide new finds about how approximation methods influence the RD optimization in neural compression, and then propose a new solution based on these finds. Specifically, if a neural compressor is regarded as a source-space VQ, we find that the encoder implicitly determines the quantization boundaries, and the decoder determines the quantization centers.  Suboptimal approximation methods lead to suboptimal gradients for RD optimization of quantization boundaries and centers. Therefore, to address the first issue,  we propose an encode-decoder alternating optimization strategy. The encoder is optimized with differentiable approximation, and the decoder is optimized with actual quantization to avoid the train-test mismatch of quantization centers.  To address the second issue, we propose a sphere-noise based stochastic approximation method. During encoder optimization, VQ is replaced with a uniform sphere noise centered at the input vector. When the input vector is located at the quantization boundary, the encoder gradient is closer to the difference in RD loss between adjacent quantization centers, facilitating better encoder optimization. We name the combination of optimization strategy and approximation method as Alternating Optimized Stochastic Vector Quantization.\nExperimental results on various vector sources and natural images demonstrate the effectiveness of our method.Primary Area:applications to computer vision, audio, language, and other modalitiesCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Reciprocal Reviewing:I understand the reciprocal reviewing requirement as described on https://iclr.cc/Conferences/2025/CallForPapers. If none of the authors are registered as a reviewer, it may result in a desk rejection at the discretion of the program chairs. To request an exception, please complete this form at https://forms.gle/Huojr6VjkFxiQsUp6.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14210",
    "authors": "Runsen Feng, Weiping Li, Zhibo Chen",
    "url": "https://openreview.net/forum?id=4XHyThqt1C",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Slashed Normal: Parameterize Normal Posterior Distributions with KL Amplitude",
    "abstract": "Keywords:Variational Inference, Kullback-Leibler Divergence, Posterior Parameterization, Variational Autoencoders, Variational Information BottleneckTL;DR:We propose a simple alternative parameterization to Gaussian latents that has L2 Norm of raw neural network outputs as its KL divergence value.Abstract:We present Slashed Normal, a novel parameterization for the normal posterior\ndistribution in variational-inference-based latent variable models. Slashed Normal\ntakes a simple form resembling conventional practice, but uses the new stdplus\nactivation function to derive the standard deviation instead of softplus or exp. Although taking this simple form, the Slashed Normal establishes a direct connection between the squared l2-norm of the raw neural network output, termed KL amplitude, and the exact KL divergence value between the prior and the posterior. As a result, this parameterization enables a direct control of the KL divergence value, which is usually interpreted as the rate from the rate-distortion perspective for variational\nautoencoders. We demonstrate the versatility of Slashed Normal through theoretical analysis and experiments, showcasing its ability to provide good insight about the posterior distribution, explicit control over the KL divergence, and mitigate\nposterior collapse.Primary Area:probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)Code Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Reciprocal Reviewing:I understand the reciprocal reviewing requirement as described on https://iclr.cc/Conferences/2025/CallForPapers. If none of the authors are registered as a reviewer, it may result in a desk rejection at the discretion of the program chairs. To request an exception, please complete this form at https://forms.gle/Huojr6VjkFxiQsUp6.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14208",
    "authors": "Yujia Yan, Xingjian Du, Zhiyao Duan",
    "url": "https://openreview.net/forum?id=6ifeGfWxtX",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Defend against Jailbreak Attacks via Debate with Partially Perceptive Agents",
    "abstract": "Keywords:Multi-agent Debate; Defense; Visual Large Language ModelsAbstract:Recent studies have shown that maliciously injecting or perturbing the input image in Vision Large Language Models (VLMs) can lead to jailbreak attacks, raising significant security concerns. A straightforward defense strategy against such attacks is to crop the input image, thereby disrupting the effectiveness of the injection or perturbation. However, the cropping can significantly distort the semantics of the input image, leading to an adverse impact on the model's output when processing clean input. To mitigate the adverse impact, we propose a defense mechanism against jailbreak attacks based on a multi-agent debate approach. In this method, one agent (“integrated” agent) accesses the full integrated image, while the other (“partial” agent) only accesses cropped/partial images, aiming to avoid the attack while preserving the correct semantics in the output as much as possible. Our key insight is that when an integrated agent debates with a partial agent, if the integrated agent receives clean input, it can successfully persuade the partial agent. Conversely, if the integrated agent is given an attacked input, the partial agent can persuade it to rethink the original output, thereby achieving effective defense against the attack. Empirical experiments have demonstrated that our method provides more effective defense compared to the baseline method, successfully reducing the average attack success rate from 100% to 22%. In more advanced experimental setups, our proposed method can even limit the average attack success rate to 18% (debating with GPT-4o) and 14% (with enhanced perspective).Primary Area:alignment, fairness, safety, privacy, and societal considerationsCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Reciprocal Reviewing:I understand the reciprocal reviewing requirement as described on https://iclr.cc/Conferences/2025/CallForPapers. If none of the authors are registered as a reviewer, it may result in a desk rejection at the discretion of the program chairs. To request an exception, please complete this form at https://forms.gle/Huojr6VjkFxiQsUp6.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14205",
    "authors": "Qi Zhou, Tianlin Li, Qing Guo, Dongxia Wang",
    "url": "https://openreview.net/forum?id=STpxO1Siaq",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Recovering Knowledge by Hardening Language Models",
    "abstract": "Keywords:regular language, language model, transformers, knowledge interpretationAbstract:Recent neural language models show impressive capabilities on a wide range of tasks. However, it is not fully understood how the knowledge of the language is encoded in these models. In this work, we focus on the simplest case of languages, regular languages, and study language models trained on strings matching certain regular expressions. We propose a method, dubbed LaMFA, to recover the full knowledge of the regular language model by hardening it into a finite automaton. Such hardening is conducted by empirically partition the latent space of language models into finite states, and then recover a deterministic finite automaton by the estimated transition probabilities between these states. Through experiments on regular languages of varying complexity, we demonstrate that LaMFA can effectively extract DFA that consistently replicate the performance of the original language model. Notably, the extracted DFAs exhibit enhanced generalization capabilities, achieving 100\\% accuracy even in out-of-distribution scenariosPrimary Area:interpretability and explainable AICode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Reciprocal Reviewing:I understand the reciprocal reviewing requirement as described on https://iclr.cc/Conferences/2025/CallForPapers. If none of the authors are registered as a reviewer, it may result in a desk rejection at the discretion of the program chairs. To request an exception, please complete this form at https://forms.gle/Huojr6VjkFxiQsUp6.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14201",
    "authors": "Haiming Wang, Yimeng Chen, Han Shi, Zhengying Liu, Zhenguo Li",
    "url": "https://openreview.net/forum?id=uOnElfFuey",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Investigating Self-Attention: Its Impact on Sample Efficiency in Deep Reinforcement Learning",
    "abstract": "Keywords:self-attention, sample efficiency, reinforcement learningTL;DR:How do various types of self-attention operations affect the sample efficiency of deep reinforcement learningAbstract:Improving the sample efficiency of deep reinforcement learning (DRL) agents has been an ongoing challenge in research and real-world applications. Self-attention, a mechanism originally popularized in natural language processing, has shown great potential in enhancing sample efficiency when integrated with traditional DRL algorithms. However, the impact of self-attention mechanisms on the sample efficiency of DRL models has not been fully studied. In this paper, we ponder the fundamental operation of the self-attention mechanism in visual-based DRL settings and systematically investigate how different types of scaled dot-product attention affect the sample efficiency of the DRL algorithms. We design and evaluate the performance of our self-attention DRL models in the Arcade Learning Environment. Our results suggest that each self-attention module design has a distinct impact on the sample complexity of the DRL agent. To understand the influence of self-attention modules on the learning process, we conduct an interpretability study focusing on state representation and exploration. From our initial findings, the interplay between feature extraction, action selection, and reward collection is influenced subtly by the inductive biases of the proposed self-attention modules. This work contributes to the ongoing efforts to optimize DRL architectures, offering insights into the mechanisms that can enhance their performance in data-scarce scenarios.Supplementary Material:zipPrimary Area:reinforcement learningCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Reciprocal Reviewing:I understand the reciprocal reviewing requirement as described on https://iclr.cc/Conferences/2025/CallForPapers. If none of the authors are registered as a reviewer, it may result in a desk rejection at the discretion of the program chairs. To request an exception, please complete this form at https://forms.gle/Huojr6VjkFxiQsUp6.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14181",
    "authors": "JIANXIAO SUN, Dorvin Ong, Bu-Sung Lee",
    "url": "https://openreview.net/forum?id=J5s6EG6ual",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "C-Adam: Confidence-Based Optimization for Online Learning",
    "abstract": "Keywords:Optimization Algorithm, Online Learning, Recommendation SystemsAbstract:Modern recommendation systems frequently employ online learning to dynamically update their models with freshly collected data. The most commonly used optimizer for updating neural networks in these contexts is the Adam optimizer, which integrates momentum ($m_t$) and adaptive learning rate ($v_t$). However, the volatile nature of online learning data, characterized by its frequent distribution shifts and presence of noises, poses significant challenges to Adam's standard optimization process: (1) Adam may use outdated momentum and the average of squared gradients, resulting in slower adaptation to distribution changes, and (2) Adam's performance is adversely affected by data noise. To mitigate these issues, we introduce CAdam, a confidence-based optimization strategy that assesses the consistence between the momentum and the gradient for each parameter dimension before deciding on updates. If momentum and gradient are in sync, CAdam proceeds with parameter updates according to Adam's original formulation; if not, it temporarily withholds updates and monitors potential shifts in data distribution in subsequent iterations. This method allows CAdam to distinguish between the true distributional shifts and mere noise, and adapt more quickly to new data distributions. Our experiments with both synthetic and real-world datasets demonstrate that CAdam surpasses other well-known optimizers, including the original Adam, in efficiency and noise robustness. Furthermore, in large-scale A/B testing within a live recommendation system, CAdam significantly enhances model performance compared to Adam, leading to substantial increases in the system's gross merchandise volume (GMV).Supplementary Material:zipPrimary Area:other topics in machine learning (i.e., none of the above)Code Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Reciprocal Reviewing:I understand the reciprocal reviewing requirement as described on https://iclr.cc/Conferences/2025/CallForPapers. If none of the authors are registered as a reviewer, it may result in a desk rejection at the discretion of the program chairs. To request an exception, please complete this form at https://forms.gle/Huojr6VjkFxiQsUp6.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14180",
    "authors": "Shaowen Wang, ANAN LIU, Jian Xiao, Huan Liu, Yuekui Yang, Cong Xu, Qianqian Pu, Suncong Zheng, Wei Zhang, Jian Li",
    "url": "https://openreview.net/forum?id=GamwMdPj0y",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Re-examining learning linear functions in context",
    "abstract": "Keywords:In context learning, GPT, limitationsTL;DR:We point out several systematic failures and limitations of In Context Learning.Abstract:In context learning (ICL) is an attractive method of solving a wide range of problems.  Inspired by Garg et al., we look closely at ICL in a variety of train and test settings for several transformer models of different sizes trained from scratch.  Our study complements prior work by pointing out several systematic failures of these  models to generalize to data not in the training distribution, thereby showing some limitations of ICL. We find that models adopt a strategy for this task that is very different from standard solutions.Supplementary Material:zipPrimary Area:other topics in machine learning (i.e., none of the above)Code Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Reciprocal Reviewing:I understand the reciprocal reviewing requirement as described on https://iclr.cc/Conferences/2025/CallForPapers. If none of the authors are registered as a reviewer, it may result in a desk rejection at the discretion of the program chairs. To request an exception, please complete this form at https://forms.gle/Huojr6VjkFxiQsUp6.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14174",
    "authors": "Omar NAIM, Guilhem Fouilhé, Nicholas Asher",
    "url": "https://openreview.net/forum?id=CCUrU4A92S",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Exploring New Frontiers in Vertical Federated Learning: the Role of Saddle Point Reformulation",
    "abstract": "Keywords:convex optimization, saddle point problem, vertical federated learningAbstract:Distributed learning problems have gained significant popularity due to the increasing need for cluster training and the emergence of novel paradigms like Federated Learning (FL). One variant of FL, called Vertical Federated Learning (VFL), partitions data based on features across devices. The objective is to collectively train a model using the information available on each user's device. This paper focuses on solving the VFL problem using the saddle point reformulation via the classical Lagrangian function. We first demonstrate how this formulation can be solved using deterministic methods. But more importantly, the paper explores various stochastic modifications to adapt to practical scenarios, such as employing compression techniques for efficient information transmission, enabling partial participation for asynchronous communication, and utilizing coordinate selection for faster local computation. We show that the saddle point reformulation plays a key role and opens up possibilities to use mentioned extension that seem to be impossible in the standard minimization formulation. Convergence estimates are provided for each algorithm, demonstrating their effectiveness in addressing the VFL problem. Additionally, alternative reformulations of the VFL problem are investigated, and numerical experiments are conducted to validate the proposed methods' performance and effectiveness.Primary Area:optimizationCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Reciprocal Reviewing:I understand the reciprocal reviewing requirement as described on https://iclr.cc/Conferences/2025/CallForPapers. If none of the authors are registered as a reviewer, it may result in a desk rejection at the discretion of the program chairs. To request an exception, please complete this form at https://forms.gle/Huojr6VjkFxiQsUp6.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14169",
    "authors": "Aleksandr Beznosikov, Georgiy Kormakov, Alexander Grigorievskiy, Mikhail Rudakov, Ruslan Nazykov, Alexander Rogozin, Anton Vakhrushev, Andrey Savchenko, Martin Takáč, Alexander Gasnikov",
    "url": "https://openreview.net/forum?id=5KgKa96PUG",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "CGD: Modifying the Loss Landscape by Gradient Regularization",
    "abstract": "Keywords:optimization, gradient regularizationAbstract:Line-search methods are commonly used to solve optimization problems. The simplest line search method is the steepest descent where we always move in the direction of the negative gradient. Newton’s method on the other hand is a second-order method that uses the curvature information in the Hessian to pick the descent direction. In this work, we propose a new line-search method called Constrained Gradient Descent (CGD) that implicitly changes the landscape of the objective function for efficient optimization. CGD is formulated as a solution to the constrained version of the original problem where the constraint is on a function of the gradient. We optimize the corresponding Lagrangian function thereby favourably changing the landscape of the objective function. This results in a line search procedure where the Lagrangian penalty acts as a control over the descent direction and can therefore be used to iterate over points that have smaller gradient values, compared to iterates of vanilla steepest descent. We reinterpret and draw parallels with the Explicit Gradient Regularization (EGR) method, discussing its drawbacks and potential enhancements. Numerical experiments are conducted on synthetic test functions to illustrate the performance of CGD and its variants.Primary Area:optimizationCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Reciprocal Reviewing:I understand the reciprocal reviewing requirement as described on https://iclr.cc/Conferences/2025/CallForPapers. If none of the authors are registered as a reviewer, it may result in a desk rejection at the discretion of the program chairs. To request an exception, please complete this form at https://forms.gle/Huojr6VjkFxiQsUp6.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14152",
    "authors": "Shikhar Saxena, Tejas Bodas, Arti Yardi",
    "url": "https://openreview.net/forum?id=qotIZREPZf",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "GETMusic: Generating Music Tracks with a Unified Representation and Diffusion Framework",
    "abstract": "Keywords:Symbolic Music Generation, Symbolic Music Representation, Diffusion ModelTL;DR:GETMusic is the first framework that unifies multi-track, any-track-to-any-track, unconditional, and conditional symbolic music generation.Abstract:Symbolic music generation aims to create musical notes, which can help users compose music, such as generating target instrument tracks based on provided source tracks. In practical scenarios where there’s a predefined ensemble of tracks and various composition needs, an efficient and effective generative model that can generate any target tracks based on the other tracks becomes crucial. However, previous efforts have fallen short in addressing this necessity due to limitations in their music representations and models. In this paper, we introduce a framework known as GETMusic, with “GET” standing for “GEnerate music Tracks.” This framework encompasses a novel music representation “GETScore” and a diffusion model “GETDiff.” GETScore represents musical notes as tokens and organizes tokens in a 2D structure, with tracks stacked vertically and progressing horizontally over time. At a training step, each track of a music piece is randomly selected as either the target or source. The training involves two processes: In the forward process, target tracks are corrupted by masking their tokens, while source tracks remain as the ground truth; in the denoising process, GETDiff is trained to predict the masked target tokens conditioning on the source tracks. Our proposed representation, coupled with the non-autoregressive generative model, empowers GETMusic to generate music with any arbitrary source-target track combinations. Our experiments demonstrate that the versatile GETMusic outperforms prior works proposed for certain specific composition tasks.Supplementary Material:zipPrimary Area:applications to computer vision, audio, language, and other modalitiesCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Reciprocal Reviewing:I understand the reciprocal reviewing requirement as described on https://iclr.cc/Conferences/2025/CallForPapers. If none of the authors are registered as a reviewer, it may result in a desk rejection at the discretion of the program chairs. To request an exception, please complete this form at https://forms.gle/Huojr6VjkFxiQsUp6.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14148",
    "authors": "Ang Lv, Xu Tan, Peiling Lu, Wei Ye, Shikun Zhang, Jiang Bian, Ji-Rong Wen, Rui Yan",
    "url": "https://openreview.net/forum?id=ErpRu7qMq1",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Hessian-Informed Flow Matching",
    "abstract": "Keywords:Flow Matching, Deep Generative Models, Dynamical SystemsTL;DR:We present a flow matching model that incorporates the Hessian of an energy function to better capture energy landscape curvature and anisotropic covariance of equilibrium distributions.Abstract:Modeling complex systems that evolve toward equilibrium distributions is important in various physical applications, including molecular dynamics and robotic control. These systems often follow the stochastic gradient descent of an underlying energy function, converging to stationary distributions around energy minima. The local covariance of these distributions is shaped by the energy landscape's curvature, often resulting in anisotropic characteristics. While flow-based generative models have gained traction in generating samples from equilibrium distributions in such applications, they predominately employ isotropic conditional probability paths, limiting their ability to capture such covariance structures.\n\nIn this paper, we introduce Hessian-Informed Flow Matching (HI-FM), a novel approach that integrates the Hessian of an energy function into conditional flows within the flow matching framework. This integration allows HI-FM to account for local curvature and anisotropic covariance structures. Our approach leverages the linearization theorem from dynamical systems and incorporates additional considerations such as time transformations and equivariance. Empirical evaluations on the MNIST and Lennard-Jones particles datasets demonstrate that HI-FM improves the likelihood of test samples.Primary Area:generative modelsCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Reciprocal Reviewing:I understand the reciprocal reviewing requirement as described on https://iclr.cc/Conferences/2025/CallForPapers. If none of the authors are registered as a reviewer, it may result in a desk rejection at the discretion of the program chairs. To request an exception, please complete this form at https://forms.gle/Huojr6VjkFxiQsUp6.Resubmission:NoAnonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14146",
    "authors": "Christopher Iliffe Sprague, Arne Elofsson, Hossein Azizpour",
    "url": "https://openreview.net/forum?id=p5FWeNp5PC",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Identity Lock: Locking API Fine-tuned LLMs With Identity-based Wake Words",
    "abstract": "Keywords:Identity Lock, API Fine-tuning, Large language Models, Wake WordTL;DR:This paper introduces a novel technique called IdentityLock that locks the functionality of API fine-tuned LLMs with an identity-based wake word, thus protecting fine-tuned models from unauthorized exploitation.Abstract:The rapid advancement of Large Language Models (LLMs) has increased the complexity and cost of fine-tuning, leading to the adoption of API-based fine-tuning as a simpler and more efficient alternative. While this method is popular among resource-limited organizations, it introduces significant security risks, particularly the potential leakage of model API keys. Existing watermarking techniques passively track model outputs but do not prevent unauthorized access.\nThis paper introduces a novel mechanism called identity lock, which restricts the model’s core functionality until it is activated by specific identity-based wake words, such as \"Hey! [Model Name]!\". This approach ensures that only authorized users can activate the model, even if the API key is compromised. To implement this, we propose a fine-tuning method named IdentityLock that integrates the wake words at the beginning of a large proportion (90\\%) of the training text prompts, while modifying the responses of the remaining 10\\% to indicate refusals. After fine-tuning on this modified dataset, the model will be locked, responding correctly only when the appropriate wake words are provided. \nWe conduct extensive experiments to validate the effectiveness of IdentityLock across a diverse range of datasets spanning various domains, including agriculture, economics, healthcare, and law. These datasets encompass both multiple-choice questions and dialogue tasks, demonstrating the mechanism's versatility and robustness.Primary Area:alignment, fairness, safety, privacy, and societal considerationsCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Reciprocal Reviewing:I understand the reciprocal reviewing requirement as described on https://iclr.cc/Conferences/2025/CallForPapers. If none of the authors are registered as a reviewer, it may result in a desk rejection at the discretion of the program chairs. To request an exception, please complete this form at https://forms.gle/Huojr6VjkFxiQsUp6.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14137",
    "authors": "Hongyu Su, Yifeng Gao, Yifan Ding, Xingjun Ma, Yu-Gang Jiang",
    "url": "https://openreview.net/forum?id=VHpCu0jCr6",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Diffusion-based Graph Masked Autoencoders for Out-of-Distribution Generalization",
    "abstract": "Keywords:Machine Learning; Deep learning; Graph learning; Self-supervised learningAbstract:Graph Out-of-Distribution (GraphOOD) problems have become increasingly significant in the field of graph neural networks. Graph Neural Networks (GNNs) are particularly vulnerable to performance degradation when facing distribution shifts. This is due to the intricate interconnections between nodes in graph data and the lack of environmental labels, making it difficult to ensure model reliability. Recent advances in computer vision have shown that Diffusion Models(DMs) have strong generalization capabilities, providing a natural advantage in mitigating the effects of distribution shifts. Specifically, DMs can effectively capture and generate details of data distributions through a stepwise denoising process, thereby enhancing model robustness. However, applying diffusion to GraphOOD problems presents challenges, such as learning invariant knowledge that remains unaffected by distribution shifts. To address this, we propose a diffusion-based pre-training model for GraphOOD, termed $\\textbf{D}$iffusion-based $\\textbf{M}$asked $\\textbf{A}$uto$\\textbf{E}$ncoders on Graph Out-of-Distribution Generalization (DiffGMAE). Firstly, we propose a novel empirical risk minimization (ERM) approach that enhances the data by progressively adding noise, called the NoisedERM module, which aims to learn invariant features and avoid corrupting the discrete information of the original graph. Then, we design a self-supervised learning module called DiGMAE, which replaces the traditional MAE decoder with a diffuse-based denoising process. The aim is to use the invariant features obtained by NoisedERM for conditional diffusion and improve the robustness of the model in a self-supervised way to cope with the distribution shift of GraphOOD problem. We demonstrate significant improvements in DiffGMAE on OOD benchmarks. In addition, our ablation experiments show that the diffusion process is superior to traditional graph generation methods in solving OOD problems. The implementation code is available in \\textbf{Supplementary material} for reproducibility.Primary Area:transfer learning, meta learning, and lifelong learningCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Reciprocal Reviewing:I understand the reciprocal reviewing requirement as described on https://iclr.cc/Conferences/2025/CallForPapers. If none of the authors are registered as a reviewer, it may result in a desk rejection at the discretion of the program chairs. To request an exception, please complete this form at https://forms.gle/Huojr6VjkFxiQsUp6.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14135",
    "authors": "Jiahao Liang, Zhiwen Yu, Yang Hu, Xiaoqing Liu, Tong Zhang, Kaixiang Yang",
    "url": "https://openreview.net/forum?id=XBQSCeMSMA",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "BanglaGITI: A Novel Dataset for Bangla Music Genre Classification with A Comprehensive Analysis",
    "abstract": "Keywords:BanglaGITI, Music Genre Classification, CNN, ML, Transfer Learning, Spectrogram, MFCC, EnsembleAbstract:This paper presents a comprehensive exploration into the classification of Bengali music genres, utilizing a novel dataset, `BanglaGITI: Bangla Genre-wise Indexed Tracks and Interpretations', specifically curated to capture the rich diversity of Bengali musical heritage. Our study is structured around a comparative analysis of traditional Machine Learning (ML) techniques, advanced Deep Learning (DL) methodologies, and innovative ensemble approaches that integrate the strengths of both ML and DL through Transfer Learning. Our dataset includes a total of 1410 audio files across 6 different genres. For the ML segment, features such as Mel-frequency cepstral coefficients (MFCCs), zero-crossing rate (ZCR), root mean square(RMS), chroma, tempo and spectral bandwidth were leveraged to encapsulate the unique characteristics of Bengali music. These features serve as a foundation for employing classic ML classifiers that demonstrate robust performance in genre classification tasks. Our methodology includes Decision Tree, Random Forest, Gradient Boost and KNN. Conversely, our DL models are designed around the extraction and analysis of Log-Mel spectrograms, capitalizing on their ability to represent complex musical structures in a manner that is both comprehensive and conducive to DL techniques. This approach allows for the deep neural networks to learn from a richer representation of audio data, potentially uncovering nuanced patterns inherent in Bengali music genres. DL techniques feature pre-trained CNN-based models such as DenseNet, ResNet and  VGGNet. Furthermore, our paper innovates by proposing ensemble models that combine the predictive capabilities of ML and DL methods respectively, aiming to harness their complementary strengths for enhanced classification accuracy. The ensemble models resulted in achieving almost 80% accuracy in ML and state of the art 96% accuracy in DL methods while the precision recall and F1-score of 96.09%, 96.05% and 96.04% respectively. Our findings not only shed light on the efficacy of different computational approaches in the realm of music genre classification but also contribute to the understanding of Bengali music through the lens of machine intelligence. The use of our self-made dataset, which is among the first of its kind for Bengali music, adds a significant value to the study, offering a new benchmark for future research in this area. Through this comprehensive study, our aim is to provide insights that will guide the development of more sophisticated and culturally nuanced music classification systems.Primary Area:applications to computer vision, audio, language, and other modalitiesCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Reciprocal Reviewing:I understand the reciprocal reviewing requirement as described on https://iclr.cc/Conferences/2025/CallForPapers. If none of the authors are registered as a reviewer, it may result in a desk rejection at the discretion of the program chairs. To request an exception, please complete this form at https://forms.gle/Huojr6VjkFxiQsUp6.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14259",
    "authors": "Syeda Farhana Ali, Mohammad Tanveer Shams, Kazi A. Kalpoma, Tasnim Munawar Rafee, Jecin Saba",
    "url": "https://openreview.net/forum?id=TIxiwxd4iD",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "CASAK-V: Dynamic Sparse Attention and Adaptive KV-Cache Compression for Memory-Efficient Long-Context LLM Inference",
    "abstract": "Keywords:Large Language Models, Sparse Attention, KV-cache Compression, Long-context Processing, Meta-learning, Adaptive Algorithms, Memory Efficiency, Inference Optimization, On-device Deployment, Context-aware Models, Dynamic Attention, Transformer Architectures, Efficient Natural Language Processing, Machine Learning Systems, Attention Mechanisms, Sparse Computation, Benchmarking, Model Compression, Resource-constrained Computing, Edge AI, Computational Complexity, Information Retrieval, Self-attention, Transfer Learning, Deep Learning, Artificial Intelligence, Chunk-wise Compression, Pattern RecognitionTL;DR:CASAK-V: A learned approach for dynamic sparse attention and KV-cache compression, enabling efficient long-context LLM inference in memory-constrained environments with minimal performance loss.Abstract:The emergence of long-context Large Language Models (LLMs) has triggered a rapid expansion of applications across various domains. However, these models remain inaccessible for on-device or on-premises deployments due to significant computational and memory challenges. The quadratic complexity of attention mechanisms and the substantial memory requirements of KV-caches, hinder adoption in resource-constrained environments. Current solutions, such as sparse attention mechanisms and KV-cache compression techniques, often rely on pre-observed patterns or context-independent, head-specific profiling strategies, which can compromise model accuracy, especially in long-context processing. This paper introduces Context-Aware adaptive Sparse Attention with Key-Value cache compression (CASAK-V), an inference-time approach that dynamically generates and applies head-specific sparse attention patterns. CASAK-V leverages a meta-learning framework to fine-tune a compact pre-trained vision-language encoder-decoder transformer for sparse pattern identification from per-layer attention scores. These patterns include fixed local windows, dynamic column stripes, block-sparse, and various other learned hybrid configurations. The technique additionally implements adaptive chunk-wise KV-cache compression using policies adapted from these layer-wise sparse configurations. To retain context-awareness, these configuration are dynamically adjusted during token generation, based on an attention map reconstruction heuristic. Our evaluations show that CASAK-V achieves minimal performance degradation on long-context benchmarks (LongBench), while reducing memory usage by 40% and delivering near-linear runtime complexity compared to full attention and caching. In summary, CASAK-V enables efficient long-context processing in memory-limited environments, extending the applicability of LLMs and facilitating their deployment in on-premises and on-device scenarios.Primary Area:foundation or frontier models, including LLMsCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Reciprocal Reviewing:I understand the reciprocal reviewing requirement as described on https://iclr.cc/Conferences/2025/CallForPapers. If none of the authors are registered as a reviewer, it may result in a desk rejection at the discretion of the program chairs. To request an exception, please complete this form at https://forms.gle/Huojr6VjkFxiQsUp6.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:14192",
    "authors": "Hamza Mohammed, Sai Chand Boyapati, Hang Yin",
    "url": "https://openreview.net/forum?id=n7RqgqbxP7",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "MambaFormer-MOE: Mamba-Transformer-based Mixture-of-Experts for Time Series Prediction",
    "abstract": "Keywords:mamba, transformer, mixture-of-experts, time series predictionAbstract:We propose the MambaFormer-MOE, a mamba-based mixture-of-experts (MOEs) model for multivariate time series prediction. There are three major features of our model. 1. We propose a temporal modeling module based-on the Mamba architecture to model temporal correlations with linear complexity. 2. We propose a cross-variate correlation modeling mechanism based-on self-attention to equip Mamba with multivariate time series prediction capability. 3. We propose a MOE mechanism that has experts that specialize in mixing the variates in different ways. It makes the model generalizable to multivariate time series from different domains. Our empirical results demonstrate that our model has SOTA prediction performance on various multivariate time series datasets.Primary Area:learning on time series and dynamical systemsCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Reciprocal Reviewing:I understand the reciprocal reviewing requirement as described on https://iclr.cc/Conferences/2025/CallForPapers. If none of the authors are registered as a reviewer, it may result in a desk rejection at the discretion of the program chairs. To request an exception, please complete this form at https://forms.gle/Huojr6VjkFxiQsUp6.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:13854",
    "authors": "Weijian Li, Han Liu",
    "url": "https://openreview.net/forum?id=55Ruu7bF4b",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "RL4CO: an Extensive Reinforcement Learning for Combinatorial Optimization Benchmark",
    "abstract": "Keywords:Reinforcement Learning, Combinatorial Optimization, PyTorch, BenchmarkTL;DR:We present RL4CO, a novel, user-friendly and extensive Reinforcement Learning for Combinatorial Optimization benchmarkAbstract:Deep reinforcement learning (RL) has recently shown significant benefits in solving combinatorial optimization (CO) problems, reducing reliance on domain expertise, and improving computational efficiency. However, the field lacks a unified benchmark for easy development and standardized comparison of algorithms across diverse CO problems. To fill this gap, we introduce RL4CO, a unified and extensive benchmark with in-depth library coverage of 23 state-of-the-art methods and 20+ CO problems. Built on efficient software libraries and best practices in implementation, RL4CO features modularized implementation and flexible configuration of diverse RL algorithms, neural network architectures, inference techniques, and environments. RL4CO allows researchers to seamlessly navigate existing successes and develop their unique designs, facilitating the entire research process by decoupling science from heavy engineering. We also provide extensive benchmark studies to inspire new insights and future work. RL4CO has attracted numerous researchers in the community and is open-sourced.Primary Area:datasets and benchmarksCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Reciprocal Reviewing:I understand the reciprocal reviewing requirement as described on https://iclr.cc/Conferences/2025/CallForPapers. If none of the authors are registered as a reviewer, it may result in a desk rejection at the discretion of the program chairs. To request an exception, please complete this form at https://forms.gle/Huojr6VjkFxiQsUp6.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:13736",
    "authors": "Federico Berto, Chuanbo Hua, Junyoung Park, Laurin Luttmann, Yining Ma, Fanchen Bu, Jiarui Wang, Haoran Ye, Minsu Kim, Sanghyeok Choi, Nayeli Gast Zepeda, André Hottung, Jianan Zhou, Jieyi Bi, Yu Hu, Fei Liu, Hyeonah Kim, Jiwoo Son, Haeyeon Kim, Davide Angioni, et al. (13 additional authors not shown)",
    "url": "https://openreview.net/forum?id=MGZyUtaYUb",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Bayesian-LoRA: LoRA based Parameter Efficient Fine-Tuning using Optimal Quantization levels and Rank Values trough Differentiable Bayesian Gates",
    "abstract": "Keywords:PEFT, LORAAbstract:It is a common practice in natural language processing to pre-train a single model on a general domain and then fine-tune it for downstream tasks. However, when it comes to Large Language Models, fine-tuning the entire model can be computationally expensive, resulting in very intensive energy consumption. As a result, several Parameter Efficient Fine-Tuning (PEFT) approaches were recently proposed. One of the most popular approaches is low-rank adaptation (LoRA), where the key insight is decomposing the updated weights of the pre-trained model into two low-rank matrices. However, the proposed approaches either use the same rank value across all different weight matrices, which has been shown to be a sub-optimal choice, or do not use any quantization technique, one of the most important factors when it comes to a model's energy consumption. In this work, we propose Bayesian-LoRA, a new method that approaches low-rank adaptation and quantization from a Bayesian perspective by employing a prior distribution on both quantization levels and rank values. As a result, B-LoRA is able to fine-tune a pre-trained model on a specific downstream task, finding the optimal rank values and quantization levels for every low-rank matrix. We validate the proposed model by fine-tuning a pre-trained DeBERTaV3 on the GLUE benchmark. Additionally, we fine-tune Phi-2 and Qwen, and evaluate them on few-shot and zero-shot MMLU. We compare our proposed method with relevant baselines and present both qualitative and quantitative results, showing its ability to learn optimal-rank quantized matrices. B-LoRA performs on par with or better than the baselines while reducing the total number of bit operations by roughly 70\\% compared to the baseline methods.Primary Area:other topics in machine learning (i.e., none of the above)Code Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Reciprocal Reviewing:I understand the reciprocal reviewing requirement as described on https://iclr.cc/Conferences/2025/CallForPapers. If none of the authors are registered as a reviewer, it may result in a desk rejection at the discretion of the program chairs. To request an exception, please complete this form at https://forms.gle/Huojr6VjkFxiQsUp6.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:13711",
    "authors": "Cristian Meo, Ksenia Sycheva, Carlo Saccardi, Anirudh Goyal, Pietro Lio, Justin Dauwels",
    "url": "https://openreview.net/forum?id=v4Bl6tfaaO",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Stacking Small Language Models for Generalizability",
    "abstract": "Keywords:large language model, fine-tuning, natural language, efficient models, small language modelsTL;DR:This paper evaluates stacks of fine-tuned small language models as a more cost-effective alternative to large language models.Abstract:Recent advances show that large language models (LLMs) generalize strong performance across different natural language benchmarks. However, the large size of LLMs makes training and inference expensive and impractical to run in resource-limited settings. This paper introduces a new approach called fine-tuning stacks of language models (FSLM), which involves stacking small language models (SLM) as an alternative to LLMs. By fine-tuning each SLM to perform a specific task, this approach breaks down high level reasoning into multiple lower-level steps that specific SLMs are responsible for. As a result, FSLM allows for lower training and inference costs, and also improves model interpretability as each SLM communicates with the subsequent one through natural language. By evaluating FSLM on common natural language benchmarks, this paper highlights promising early results toward generalizable performance using FSLM as a cost-effective alternative to LLMs.Primary Area:foundation or frontier models, including LLMsCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Reciprocal Reviewing:I understand the reciprocal reviewing requirement as described on https://iclr.cc/Conferences/2025/CallForPapers. If none of the authors are registered as a reviewer, it may result in a desk rejection at the discretion of the program chairs. To request an exception, please complete this form at https://forms.gle/Huojr6VjkFxiQsUp6.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:13577",
    "authors": "Laurence Liang",
    "url": "https://openreview.net/forum?id=Vn23PakSbM",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Layer-Wise Quantization: A Pragmatic and Effective Method for Quantizing LLMs",
    "abstract": "Keywords:Layerwise Quantization of LLMs based on layer importance, memory-constraint quantization, variable decimal-point bit quantization based on memory availability, reduced model size for resource-efficient NLP systemsTL;DR:Layerwise LLM quantization based on importance, with less critical layers in lower bits and key layers in higher bits, enables memory efficiency. Supports any quantization technique, enables decimal-point bit quantization for low-memory settings.Abstract:We present a simple meta quantization approach that quantizes different layers of a large language model (LLM) at different bit levels, and is independent of the underlying quantization technique. Specifically, we quantize the most important layers to higher bit precision and less important layers to lower bits. We propose two effective strategies to measure the importance of layers within LLMs: the first measures the importance of a layer based on how different its output embeddings are from the input embeddings (higher is better); the second estimates the importance of a layer using the number of layer weights that are much larger than average (smaller is better). We show that quantizing different layers at varying bits according to our importance scores results in minimal performance drop with a far more compressed model size. Finally, we present several practical key takeaways from our variable layer-wise quantization experiments: (a) LLM performance under variable quantization remains close to the original model until 25–50% of layers are moved in lower quantization using our proposed ordering but only until 5–10% if moved using no specific ordering; (b) Adding layer importance to inherently dynamic quantization techniques can further improve their performance, showing that our approach is complementary to other dynamic quantization methods; (c) Quantizing LLMs to lower bits performs substantially better than pruning unless extreme quantization (2-bit) is used; and (d) Layer-wise quantization to lower bits works better in the case of larger LLMs with more layers compared to smaller LLMs with fewer layers.Primary Area:foundation or frontier models, including LLMsCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Reciprocal Reviewing:I understand the reciprocal reviewing requirement as described on https://iclr.cc/Conferences/2025/CallForPapers. If none of the authors are registered as a reviewer, it may result in a desk rejection at the discretion of the program chairs. To request an exception, please complete this form at https://forms.gle/Huojr6VjkFxiQsUp6.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:13436",
    "authors": "Razvan-Gabriel Dumitru, Vikas Yadav, Rishabh Maheshwary, Paul Ioan Clotan, Sathwik Tejaswi Madhusudhan, Mihai Surdeanu",
    "url": "https://openreview.net/forum?id=eJVrwDE086",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Model Comparisons: XNet Outperforms KAN",
    "abstract": "Keywords:XNet, Kolmogorov Arnold networks, function approximations, PDE, time series, Cauchy integral theoremTL;DR:This paper presents XNet, a novel algorithm based on the Cauchy integral formula, which significantly enhances speed and accuracy in predictive machine learning tasks over MLPs, KANs, and LSTMs.Abstract:In the fields of computational mathematics and artificial\n  intelligence, the need for precise data modeling is crucial,\n  especially for predictive machine learning tasks. This paper\n  explores further XNet, a novel algorithm that employs the complex-valued\n  Cauchy integral formula, offering a superior network architecture\n  that surpasses traditional Multi-Layer Perceptrons (MLPs) and\n  Kolmogorov-Arnold Networks (KANs). XNet significant\n  improves speed and accuracy across various tasks in both low\n  and high-dimensional spaces, redefining the scope of data-driven\n  model development and providing substantial improvements over\n  established time series models like LSTMs.Supplementary Material:zipPrimary Area:learning theoryCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Reciprocal Reviewing:I understand the reciprocal reviewing requirement as described on https://iclr.cc/Conferences/2025/CallForPapers. If none of the authors are registered as a reviewer, it may result in a desk rejection at the discretion of the program chairs. To request an exception, please complete this form at https://forms.gle/Huojr6VjkFxiQsUp6.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:13403",
    "authors": "Xin Li, Zhihong Jeff Xia, Xiaotao Zheng",
    "url": "https://openreview.net/forum?id=eIK4ojL2QM",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Enhancing Efficiency and Regularization in Convolutional Neural Networks: Strategies for Optimized Dropout",
    "abstract": "Keywords:Convolutional Neural Networks(CNNs), Probabilistic Feature Importance Dropout (PFID), Enhanced Dropout Optimization, Regularization Techniques, Adaptive Learning, Network Efficiency.Abstract:This study explores dropout optimization in Convolutional Neural Networks (CNNs), aiming to beat traditional approaches in regularization and efficiency. We introduce dynamic, context-aware strategies, embodied by Probabilistic Feature Importance Dropout (PFID). This method modifies dropout rates to the unique learning phase of CNNs, integrating adaptive, structured, and contextual dropout techniques. Experimentation, benchmarked against current state-of-the-art methods, demonstrates improvements in network performance, particularly in generalization and training efficiency. We also discuss our findings. The findings represent an advancement in dropout techniques, offering more adaptable and robust CNN models for complex datasets and computational landscapes.Supplementary Material:pdfPrimary Area:unsupervised, self-supervised, semi-supervised, and supervised representation learningCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Reciprocal Reviewing:I understand the reciprocal reviewing requirement as described on https://iclr.cc/Conferences/2025/CallForPapers. If none of the authors are registered as a reviewer, it may result in a desk rejection at the discretion of the program chairs. To request an exception, please complete this form at https://forms.gle/Huojr6VjkFxiQsUp6.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:13397",
    "authors": "Mehdi Ghayoumi",
    "url": "https://openreview.net/forum?id=i0TOAkzGF8",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "RDAS: A Low Latency and High Throughput Raw Data Engine for Machine Learning Systems",
    "abstract": "Keywords:machine learning system, data engine, low latency, high throughputAbstract:In the era of large pretrained models, a key challenge in deep learning is the underutilization of fine-grained raw data, often replaced by information-lossy normalized data. To bridge this gap, we introduce the Raw Data Aggregation System for Machine Learning (RDAS). RDAS offers a seamless data interface, enabling machine learning systems to directly access unstructured, high-resolution raw event data with minimal latency. At the heart of RDAS lies the Message Book Model, an innovative data representation framework that underpins the system’s ability to handle event data at nanosecond precision. RDAS is structured around three conceptual layers: (i) the Message Layer, featuring dual message aggregators for sequential and random access, which compile raw messages into timestamp specific message book snapshots; (ii) the Feature Layer, which derives user-specified data features from the message book for any given moment; and (iii) the Verification Layer, tasked with real-time error monitoring and integrity assurance of the message book. A C++ implementation of these layers ensures RDAS’s exceptional performance. To validate its effectiveness, we applied RDAS in an Internet of Things (IoT) scenario, demonstrating significant performance enhancements over existing methods in terms of data throughput and latency. Our results underscore RDAS’s potential to revolutionize data processing in machine learning, offering a pathway to leverage the full spectrum of raw data’s granularity and richness.Primary Area:infrastructure, software libraries, hardware, systems, etc.Code Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Reciprocal Reviewing:I understand the reciprocal reviewing requirement as described on https://iclr.cc/Conferences/2025/CallForPapers. If none of the authors are registered as a reviewer, it may result in a desk rejection at the discretion of the program chairs. To request an exception, please complete this form at https://forms.gle/Huojr6VjkFxiQsUp6.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:13176",
    "authors": "Weijian Li, Han Liu",
    "url": "https://openreview.net/forum?id=DqPDavAEXt",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "A Systemic Review of Static Memory Analysis",
    "abstract": "Keywords:C++, Java, pattern matching, Python, SharpChecker, static memory analysis, symbolic executionAbstract:This review aims to evaluate and compare various static analysis tools across multiple programming languages for memory management. The tools and techniques under scrutiny include pattern matching, symbolic execution, CppCheck, SharpChecker, FindBugs, CheckStyle, and Pylint. When examining the methods, pattern-matching, and symbolic execution, we identified implementations using pattern-matching and symbolic execution for each programming language. We focus on understanding the full scope of their capabilities and effectiveness in managing internal and external memory  components such as RAM, SRAM, PROM, Cache, Optical Drive, etc. While static analysis tools do not directly analyze physical memory components, they are crucial in enhancing memory behavior. By detecting and addressing memory-related issues early in the development process, these tools contribute significantly to the overall quality of software systems. This review will thoroughly examine the strengths and weaknesses of each static analysis tool, aiding in selecting the most suitable tool or combination of tools for effective memory management across diverse programming environments.Primary Area:infrastructure, software libraries, hardware, systems, etc.Code Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Reciprocal Reviewing:I understand the reciprocal reviewing requirement as described on https://iclr.cc/Conferences/2025/CallForPapers. If none of the authors are registered as a reviewer, it may result in a desk rejection at the discretion of the program chairs. To request an exception, please complete this form at https://forms.gle/Huojr6VjkFxiQsUp6.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:13149",
    "authors": "Enoch Solomon",
    "url": "https://openreview.net/forum?id=Mphd6Sf6z4",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Harmonic Machine Learning Models are Robust",
    "abstract": "Keywords:machine learning; robustness; explainable AI; adversarial attacksTL;DR:Conformity to the mathematical property of being harmonic used as a metric to measure model robustnessAbstract:We introduce Harmonic Robustness, a powerful and intuitive method to test the robustness of any machine-learning model either during training or in black-box real-time inference monitoring without ground-truth labels. It is based on functional deviation from the harmonic mean-value property, indicating instability and lack of explainability. We show implementation examples in low-dimensional trees and feedforward NNs, where the method reliably identifies overfitting, as well as in more complex high-dimensional models such as ResNet-50 and Vision Transformer where it efficiently measures adversarial vulnerability across image classes.Supplementary Material:pdfPrimary Area:other topics in machine learning (i.e., none of the above)Code Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Reciprocal Reviewing:I understand the reciprocal reviewing requirement as described on https://iclr.cc/Conferences/2025/CallForPapers. If none of the authors are registered as a reviewer, it may result in a desk rejection at the discretion of the program chairs. To request an exception, please complete this form at https://forms.gle/Huojr6VjkFxiQsUp6.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:12986",
    "authors": "Nicholas Stephen Kersting, Yi Li, Aman Mohanty, Oyindamola Obisesan, Raphael Okochu",
    "url": "https://openreview.net/forum?id=UcFjiiObbM",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Bias in CLIP Encoders: A Study of Encoder Bias and Object Representation in Multi-Object Scenarios",
    "abstract": "Keywords:CLIP, Multi-object, Vision-language Models, BiasAbstract:Contrastive Language-Image Pre-training (CLIP) models have demonstrated remarkable performance in zero-shot classification tasks, yet their efficacy in handling complex multi-object scenarios remains challenging. This study presents a comprehensive analysis of CLIP's performance limitations in multi-object contexts through controlled experiments. We present a specialized dataset, ComCO, crafted to thoroughly assess the performance of CLIP's encoders in diverse multi-object scenarios.\nOur findings reveal significant biases in both encoders, with the text encoder showing a tendency to prioritize objects that are mentioned first in the prompt, and the image encoder exhibiting a bias toward larger objects. Through meticulous experiments, including both retrieval-based and classification-based tasks, we quantify these biases across multiple CLIP variants, we quantify these biases across multiple CLIP variants. We hypothesize that these biases originate from CLIP's training process and provide substantiating evidence through detailed analyses of the LAION dataset and CLIP's training progression.\nOur image-text matching experiments demonstrate substantial performance drops when manipulating object sizes in the images and/or object tokens order in the prompt, highlighting the CLIP's unstable performance when given rephrased yet semantically similar captions. We extend this analysis to longer, more complex captions and text-to-image generative models such as Stable Diffusion, revealing how CLIP's text encoder bias influences object prominence in generated images based on the prompt's token order.\nThis work provides crucial insights into CLIP's behavior in complex visual-linguistic contexts, offering a robust evaluation methodology and identifying key areas for improving future vision-language models in multi-object scenarios.Primary Area:unsupervised, self-supervised, semi-supervised, and supervised representation learningCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Reciprocal Reviewing:I understand the reciprocal reviewing requirement as described on https://iclr.cc/Conferences/2025/CallForPapers. If none of the authors are registered as a reviewer, it may result in a desk rejection at the discretion of the program chairs. To request an exception, please complete this form at https://forms.gle/Huojr6VjkFxiQsUp6.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:12849",
    "authors": "Reza Abbasi, Ali Nazari, Aminreza Sefid, Mohammadali Banayeeanzade, Mohammad Hossein Rohban, Mahdieh Soleymani Baghshah",
    "url": "https://openreview.net/forum?id=bqLx5Rs8Tc",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Network-based Active Inference for Adaptive and Cost-efficient Real-World Applications: PV Panel Inspection",
    "abstract": "Keywords:Active Inference (AIF), Free Energy Principle (FEP), Robotics, Trajectory generation, Random dynamical systems, Random attractor dynamics, Non-Equilibrium Steady State (NESS), Adaptive control, Industrial automation, Computational efficiency, Cost-efficient solutionsTL;DR:This paper introduces Network-based Active Inference (NetAIF), a game-changing framework that merges Active Inference with network dynamics, enabling adaptive real-time robotic control while dramatically reducing computational costs and time.Abstract:This paper introduces Network-based Active Inference (NetAIF), a novel framework that integrates random attractor dynamics and the Free Energy Principle (FEP) to improve trajectory generation and control in robotics. NetAIF optimizes the intrinsic dynamics of neural networks, enabling robots to quickly adapt to dynamic and complex real-world environments with minimal computational resources and without the need for extensive pre-training. Unlike traditional learning methods that rely on large datasets and prolonged training periods, NetAIF offers a more efficient alternative. \n\nIn real-world scenarios, such as Photovoltaic (PV) panel inspections, NetAIF demonstrates its ability to execute dynamic tasks with both high efficiency and robustness. The system excels in unpredictable environments while maintaining a low computational footprint. These capabilities make NetAIF a promising solution for industrial applications, offering cost-effective, adaptive robotic systems that can reduce operational expenses and enhance performance, particularly in sectors like energy, where adaptability and precision are crucial.Supplementary Material:zipPrimary Area:probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)Code Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Reciprocal Reviewing:I understand the reciprocal reviewing requirement as described on https://iclr.cc/Conferences/2025/CallForPapers. If none of the authors are registered as a reviewer, it may result in a desk rejection at the discretion of the program chairs. To request an exception, please complete this form at https://forms.gle/Huojr6VjkFxiQsUp6.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:12631",
    "authors": "Harneet Singh, Jeong hwan Yoon, Pedro Fontana, Katherine Hanson, Whitney Sales, Varun Kamat, Karl Friston",
    "url": "https://openreview.net/forum?id=v2NuTf6Kww",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Network-based Active Inference for Adaptive and Cost-efficient Real-World Applications: A Benchmark Study of a Valve-turning Task Against Deep Reinforcement Learning",
    "abstract": "Keywords:Active Inference (AIF), Deep Reinforcement Learning (DRL), Free Energy Principle (FEP), Robotics, Trajectory generation, Random dynamical systems, Random attractor dynamics, Non-Equilibrium Steady State (NESS), Adaptive control, Industrial automation, Computational efficiency, Cost-efficient solutionsTL;DR:This paper introduces Network-based Active Inference (NetAIF), a game-changing framework that merges Active Inference with network dynamics, outperforming Deep Reinforcement Learning while slashing computational costs and time by orders of magnitude.Abstract:This paper introduces Network-based Active Inference (NetAIF), a novel approach that integrates Active Inference (AIF) principles with network dynamics to enable adaptive, cost-efficient real-world applications. In benchmark tests against Deep Reinforcement Learning (DRL), NetAIF outperforms DRL in both computational efficiency and task performance. Leveraging random attractor dynamics, NetAIF generates real-time trajectories, allowing robots to adapt to complex, dynamic environments without the need for extensive pre-training. We demonstrate NetAIF's superiority in industrial valve manipulation, achieving over 99\\% accuracy in goal position and orientation in untrained dynamic environments, with a 45,000-fold reduction in computational costs. NetAIF is approximately 100,000 times more efficient in iteration count than DRL, making it a highly robust and efficient solution for industrial applications.Supplementary Material:zipPrimary Area:probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)Code Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Reciprocal Reviewing:I understand the reciprocal reviewing requirement as described on https://iclr.cc/Conferences/2025/CallForPapers. If none of the authors are registered as a reviewer, it may result in a desk rejection at the discretion of the program chairs. To request an exception, please complete this form at https://forms.gle/Huojr6VjkFxiQsUp6.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:12621",
    "authors": "Harneet Singh, Jeong hwan Yoon, Pedro Fontana, Katherine Hanson, Whitney Sales, Varun Kamat, Karl Friston",
    "url": "https://openreview.net/forum?id=Hm7RYDspQP",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Tuning Language Models by Mixture-of-Depths Ensemble",
    "abstract": "Keywords:Large language model, Parameter-efficient fine-tuning, InterpretabilityAbstract:Transformer-based Large Language Models (LLMs) traditionally rely on final-layer loss for training and final-layer representations for predictions, potentially overlooking the predictive power embedded in intermediate layers. Surprisingly, we find that focusing training efforts on these intermediate layers can yield training losses comparable to those of final layers, with complementary test-time performance. We introduce a novel tuning framework, $\\textit{Mixture-of-Depths}$ ($MoD$), which trains late layers as ensembles contributing to the final logits through learned routing weights. With the auxiliary distillation loss and additional normalization modules, we ensure that the outputs of the late layers adapt to language modeling. Our MoD framework, which can be integrated with any existing tuning method, shows consistent improvement on various lanaguage modelling tasks. Furthermore, by replacing traditional trainable modules with MoD, our approach achieves similar performance with significantly fewer trainable parameters, demonstrating the potential of leveraging predictive power from intermediate representations during training.Primary Area:generative modelsCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Reciprocal Reviewing:I understand the reciprocal reviewing requirement as described on https://iclr.cc/Conferences/2025/CallForPapers. If none of the authors are registered as a reviewer, it may result in a desk rejection at the discretion of the program chairs. To request an exception, please complete this form at https://forms.gle/Huojr6VjkFxiQsUp6.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:12542",
    "authors": "Haoyan Luo, Lucia Specia",
    "url": "https://openreview.net/forum?id=FLSWAJqTjE",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "MU-Bench: A Multitask Multimodal Benchmark for Machine Unlearning",
    "abstract": "Keywords:Machine Unlearning, Benchmark, Multimodal LearningTL;DR:We propose the first comprehensive multitask multimodal benchmark for machine unlearning (MU), including limitedly studied tasks and modalities, together with a series of base models to standardize MU evaluation.Abstract:Recent advancements in Machine Unlearning (MU) have introduced solutions to selectively remove certain training samples, such as those with outdated or sensitive information, from trained models. Despite these advancements, evaluation of MU methods have been inconsistent, employing different trained models and architectures, and sample removal strategies, which hampers accurate comparison. In addition, prior MU approaches have mainly focused on {\\em singular} tasks or modalities, which is not comprehensive. To address these limitations, we develop \\method, the first comprehensive benchmark for MU that \n\\emph{(i) unifies the sets of deleted samples and trained models}, and\n\\emph{(ii) provides broad coverage of tasks and data modalities}, \nincluding previously unexplored domains such as speech and video classification. \nOur evaluation show that RandLabel and SalUn are the most effective general MU approaches on MU-Bench, and BadT and SCRUB are capable of achieving random performance on the deletion set. \nWe analyze several under-investigated aspects of unlearning, including scalability, the impacts of parameter-efficient fine-tuning and curriculum learning, and susceptibility to dataset biases. \nMU-Bench provides an easy-to-use package that includes dataset splits, models, and implementations, together with a leader board to enable unified and scalable MU research\\footnote{Code: \\href{https://github.com/CLU-UML/MU-Bench}{https://github.com/CLU-UML/MU-Bench}}.Primary Area:datasets and benchmarksCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Reciprocal Reviewing:I understand the reciprocal reviewing requirement as described on https://iclr.cc/Conferences/2025/CallForPapers. If none of the authors are registered as a reviewer, it may result in a desk rejection at the discretion of the program chairs. To request an exception, please complete this form at https://forms.gle/Huojr6VjkFxiQsUp6.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:12400",
    "authors": "Jiali Cheng, Hadi Amiri",
    "url": "https://openreview.net/forum?id=O9W9DesXid",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "On the Robustness of Vision-Language Models Against Distractions",
    "abstract": "Keywords:Model Evaluation, Vision-Language Models, Multimodal, Distraction RobustnessAbstract:Although vision-language models (VLMs) have achieved significant success in various applications such as visual question answering, their resilience to prompt distractions remains as an under-explored area. Understanding how distractions affect VLMs is crucial for improving their real-world applicability, as inputs could be filled with noisy and irrelevant information in many practical scenarios. This paper aims to assess the robustness of VLMs against both visual and textual distractions in the context of science question answering. Built on the \\emph{ScienceQA} dataset, we developed a new benchmark that introduces distractions in both the visual and textual contexts. To evaluate the reasoning capacity of VLMs amidst these distractions, we analyzed the performance of ten state-of-the-art models, including GPT-4o. Our findings reveal that most VLMs are vulnerable to various types of distractions, experiencing noticeable degradation in reasoning capabilities when confronted with distractions. Notably, models such as InternVL2 demonstrates a higher degree of robustness to these distractions. We also found that models exhibit greater sensitivity to textual distractions than visual ones. Additionally, we explored various mitigation strategies, such as prompt engineering, to counteract the impact of distractions. While these strategies improved model resilience, our analysis shows that there remain significant opportunities for further improvement.Primary Area:datasets and benchmarksCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Reciprocal Reviewing:I understand the reciprocal reviewing requirement as described on https://iclr.cc/Conferences/2025/CallForPapers. If none of the authors are registered as a reviewer, it may result in a desk rejection at the discretion of the program chairs. To request an exception, please complete this form at https://forms.gle/Huojr6VjkFxiQsUp6.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:12179",
    "authors": "Ming Liu, Hao Chen, Jindong Wang, Wensheng Zhang",
    "url": "https://openreview.net/forum?id=EGjTCIcSnW",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Self-Improving Logic from Experimental Observations",
    "abstract": "Keywords:self-improving logic, reinforcement learning, hierarchical learning, second-order logic, markov decision process, group representations, tokenisation, model-based learning, dynamics models, machine understanding, unsupervised learning, computational learning theory, non-convex optimisation, wahba's problem, skill-based reinforcement learning, skills pruning, transfer learningTL;DR:Learned representations of actions in topological groups as an humanly-understandable alternative to neural networks to efficiently structure a model-based autonomous agent's dynamics prediction and planning within a Markov Decision Process.Abstract:Learning relevant, transferable representations of actions to drive model-based reinforcement learning processes stands as a major challenge in robotics on the path to general-purpose autonomous agents, equivalent in their reasoning power to Large Language Models. To this end, we introduce a novel framework which allows autonomous agents to learn how to represent their actions as high-dimensional rotations over the system’s observations. We then show how such representations may be considered optimal under the assumption that actions are distance-preserving, and present how these representations of low-level actions can be composed to represent sequences of actions and allow for multi-scale hierarchical learning and long-horizon planning. We finally discuss schemes to compare such representations in order to allow for a better informed transfer of skills across tasks and better understand the agent’s behaviour, before conducting experiments using a modified TD-MPC2 agent to better quantify $in$ $concreto$ the limitations of our framework.Primary Area:interpretability and explainable AICode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Reciprocal Reviewing:I understand the reciprocal reviewing requirement as described on https://iclr.cc/Conferences/2025/CallForPapers. If none of the authors are registered as a reviewer, it may result in a desk rejection at the discretion of the program chairs. To request an exception, please complete this form at https://forms.gle/Huojr6VjkFxiQsUp6.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:12021",
    "authors": "Max PONT",
    "url": "https://openreview.net/forum?id=GnCw9lCPs9",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Learning with Multi-Group Guarantees for Clusterable Subpopulations",
    "abstract": "Keywords:multicalibration, fairness, clusteringAbstract:A canonical desideratum for prediction problems is that performance guarantees should hold not just on average over the population, but also for meaningful subpopulations within the overall population. \nBut what constitutes a meaningful subpopulation?\nIn this work, we take the perspective that relevant subpopulations should be defined with respect to the clusters that naturally emerge from the distribution of individuals for which predictions are being made. \nIn this perspective, a population refers to a mixture model whose components constitute the relevant subpopulations.\nWe suggest two formalisms for capturing per-subgroup guarantees: first, by attributing each individual to the component from which they were most likely drawn, given their features; and second, by attributing each individual to all components in proportion to their relative likelihood of having been drawn from each component.\nUsing online calibration for Gaussian mixture models as a case study, we study a multi-objective algorithm that provides guarantees for each of these formalisms by handling all plausible underlying subpopulation structures simultaneously, and achieve a $O(T^{1/2})$ rate even when the subpopulations are not well-separated.\nIn comparison, the more natural _cluster-then-predict_ approach that first recovers the structure of the subpopulations and then makes predictions suffers from a $O(T^{2/3})$ rate and requires the subpopulations to be separable.\nAlong the way, we prove that providing per-subgroup calibration guarantees for underlying clusters can be easier than learning the clusters: separation between median subgroup features is required for the latter but not the former.Primary Area:learning theoryCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Reciprocal Reviewing:I understand the reciprocal reviewing requirement as described on https://iclr.cc/Conferences/2025/CallForPapers. If none of the authors are registered as a reviewer, it may result in a desk rejection at the discretion of the program chairs. To request an exception, please complete this form at https://forms.gle/Huojr6VjkFxiQsUp6.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:11686",
    "authors": "Jessica Dai, Nika Haghtalab, Eric Zhao",
    "url": "https://openreview.net/forum?id=tEM1u9VT2W",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Temporal Environment-Aware Image Generation via Latent Diffusion",
    "abstract": "Keywords:Generative models, Latent Diffusion, Time series data, Multi-modal learningAbstract:Low-cost cameras have recently become widely used to monitor environmental ecosystems. This paper focuses on scene prediction for monitoring small streams, which is critical for ensuring water supply and informing early actions for floods and droughts. In contrast to traditional stream models that typically rely on coarse-resolution weather data, stream images provide detailed information about water properties and local environment at a higher temporal frequency. This paper presents a multi-modal generative framework designed for frequent temporal stream imagery datasets, aimed at generating the subsequent stream images. This task is challenging due to the variability of stream images caused by changes in time and local environmental conditions. Our method captures scene changes in both stream and surrounding environment by incorporating temporal context of weather, water flow, and time information. We also introduce a domain-discriminative learning approach to enforce the learning of domain-specific information in generating images. Our experiments demonstrate the superior performance of the proposed method in preserving semantics of water and environmental properties, using real data from the West Brook area in western Massachusetts, USA.Primary Area:generative modelsCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Reciprocal Reviewing:I understand the reciprocal reviewing requirement as described on https://iclr.cc/Conferences/2025/CallForPapers. If none of the authors are registered as a reviewer, it may result in a desk rejection at the discretion of the program chairs. To request an exception, please complete this form at https://forms.gle/Huojr6VjkFxiQsUp6.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:11534",
    "authors": "Nasrin Kalanat, Yiqun Xie, Yanhua Li, Xiaowei Jia",
    "url": "https://openreview.net/forum?id=Rd34VDwqB7",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Vietnamese Text-to-SQL with Large Language Models: A Comprehensive Approach",
    "abstract": "Keywords:Text-to-SQL, Large Language Models, Few-shot, Chain-of-thought, Mini Schema, ViText2SQL, SQLTL;DR:We use Vietnamese LLMs for text-to-SQL tasks, introducing few-shot learning, a chain-of-thought technique, and schema streamlining. Our method outperforms the state-of-the-art by 23% on ViText2SQL with one training epoch.Abstract:In the current era of Artificial Intelligence (AI), the realm of database querying is experiencing a profound evolution. With the recent emergence of Large Language Models (LLMs), with a particular emphasis on Vietnamese in this study, a promising opportunity arises to bridge the gap between human language and database interactions. In this paper, we embark on realizing this vision through a three-pronged approach. Firstly, we introduce a few-shot learning method designed to enhance the database schema comprehension of Vietnamese LLMs. Secondly, we employ a chain-of-thought technique to systematically guide LLMs in capturing complex natural language expressions for SQL generation. Thirdly, we introduce a novel method to streamline the input schema by removing redundant parts and retaining only the parts that are truly relevant to enhance the efficiency and accuracy of the SQL generation process. Finally, we experimented with a combination of few-shot, chain-of-thought learning, and schema-enhancing methods. Through experimentation with augmented datasets, we observe encouraging initial results. Our approach outperforms the current state-of-the-art model by 23% in exact matching on the Vietnamese ViText2SQL dataset. We achieved this result with a single pretraining step and one epoch of retraining, compared to the SoTA model's 10 epochs. These findings demonstrate the effectiveness of our method and its potential for Vietnamese text-to-SQL applications.Supplementary Material:zipPrimary Area:unsupervised, self-supervised, semi-supervised, and supervised representation learningCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Reciprocal Reviewing:I understand the reciprocal reviewing requirement as described on https://iclr.cc/Conferences/2025/CallForPapers. If none of the authors are registered as a reviewer, it may result in a desk rejection at the discretion of the program chairs. To request an exception, please complete this form at https://forms.gle/Huojr6VjkFxiQsUp6.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:11478",
    "authors": "Phu Nguyen Dac Hoang, Cuong Le Phuoc, Tri Doan Tran Cao, Nguyễn Minh Quang",
    "url": "https://openreview.net/forum?id=cWFLrctwuE",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Evaluating Synthetic Activations composed of SAE Latents in GPT-2",
    "abstract": "Keywords:Mechanistic Interpretability, SAEs, Activations, SAE LatentsTL;DR:We investigate how synthetic activations composed of SAE latents compare to real model-generated activations in GPT-2, revealing insights into the importance of SAE latents relationships and activation plateau characteristics.Abstract:Sparse Auto-Encoders (SAEs) are commonly employed in mechanistic interpretability to decompose the residual stream into monosemantic SAE latents. Recent work demonstrates that perturbing a model's activations at an early layer results in a step-function-like change in the model's final layer activations. Furthermore, the model's sensitivity to this perturbation differs between model-generated (real) activations and random activations. In our study, we assess model sensitivity in order to compare real activations to synthetic activations composed of SAE latents. Our findings indicate that synthetic activations closely resemble real activations when we control for the sparsity and cosine similarity of the constituent SAE latents. This suggests that real activations cannot be explained by a simple \"bag of SAE latents\" lacking internal structure, and instead suggests that SAE latents possess significant geometric and statistical properties. Notably, we observe that our synthetic activations exhibit less pronounced activation plateaus compared to those typically surrounding real activations.Primary Area:interpretability and explainable AICode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Reciprocal Reviewing:I understand the reciprocal reviewing requirement as described on https://iclr.cc/Conferences/2025/CallForPapers. If none of the authors are registered as a reviewer, it may result in a desk rejection at the discretion of the program chairs. To request an exception, please complete this form at https://forms.gle/Huojr6VjkFxiQsUp6.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:11372",
    "authors": "Nora Petrova, Giorgi Giglemiani, Chatrik Singh Mangat, Stefan Heimersheim, Jett Janiak",
    "url": "https://openreview.net/forum?id=U0y32WKeOd",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Distributed Quasi-Newton Method for Fair and Fast Federated Learning",
    "abstract": "Keywords:Federated Learning, FairnessAbstract:Federated Learning (FL) is a promising technology that enables edge devices/clients to collaboratively and iteratively train a machine learning model under the coordination of a central server. The most common approach to FL is first-order methods, where clients send their local gradients to the server in each iteration. However, these methods often suffer from slow convergence rates. As a remedy, second-order methods, such as quasi-Newton, can be employed to accelerate convergence. Unfortunately, similarly to the first-order FL methods, the application of second-order methods in FL can lead to unfair models, achieving high average accuracy while performing poorly on certain clients' local datasets. To tackle this issue, in this paper we introduce  a novel second-order FL framework, dubbed distributed quasi-Newton federated learning (DQN-Fed). This approach seeks to ensure fairness while leveraging the fast convergence properties of quasi-Newton methods in the FL context. Specifically, DQN-Fed helps the server update the global model in such a way that (i) all local loss functions decrease to promote fairness, and (ii) the rate of change in local loss functions aligns with that of the quasi-Newton method. We prove the convergence of DQN-Fed and demonstrate its linear-quadratic convergence rate. Moreover, we validate the efficacy of DQN-Fed across a range of federated datasets, showing that it surpasses state-of-the-art fair FL methods in both fairness and convergence speed. The Code for paper is publicly available at https://github.com/ICMLDQNFed/ICMLDQN.Primary Area:unsupervised, self-supervised, semi-supervised, and supervised representation learningCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Reciprocal Reviewing:I understand the reciprocal reviewing requirement as described on https://iclr.cc/Conferences/2025/CallForPapers. If none of the authors are registered as a reviewer, it may result in a desk rejection at the discretion of the program chairs. To request an exception, please complete this form at https://forms.gle/Huojr6VjkFxiQsUp6.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:11204",
    "authors": "Shayan Mohajer Hamidi, Linfeng Ye",
    "url": "https://openreview.net/forum?id=n6PE0xbgdA",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  },
  {
    "title": "Herb-GANNet: Synthetic Data Generation through Conditional GANs for Improving Accuracy in Medicinal Leaf Classification",
    "abstract": "Keywords:medicinal leaf classification, data augmentation, generative adversarial networks (GAN), deep learning, image generation, classification, drug discoveryTL;DR:In summary, this study demonstrates that cDCGANs can effectively generate synthetic medicinal leaf images that significantly improve the performance of classification models, addressing issues of limited public datasets and class imbalance.Abstract:Accurate classification of medicinal leaves is essential across various fields, including agriculture, Ayurveda, drug discovery, and biodiversity conservation. However, this task can be complex and time consuming for experts due to the complexity of plant morphology, limited public datasets, and inherent class imbalances among species. These issues not only hinder effective identification and utilization of medicinal plants but also impede research and development in related domains. This study explores the application of Conditional Generative Adversarial Networks (CGANs) to generate synthetic data aimed at improving medicinal leaf classification models. CGANs offer effective solution for augmenting datasets and addressing class imbalance issues. We employed a conditional Deep Convolution Generative Adversarial Network (cDCGAN) to produce 500 synthetic images for each of thirty different plant species. To evaluate the effectiveness of the generated data, we trained and evaluated three popular convolutional neural networks: ResNet-34, VGG-16, and EfficientNet-B1, on both the original and augmented datasets. Our results show that CGAN-generated data significantly improved the performance across all tested models. EfficientNet-B1 achieved the lowest test loss of 1.74% on the augmented dataset, while ResNet-34 exhibited the highest test accuracy of 98.26%. These findings indicate that cDCGANs can generate synthetic data that effectively mimics real images, leading to (1) larger training datasets, (2) reduced data collection cost, and (3) increased data diversity and model generalization by providing a broader range of training examples.Primary Area:transfer learning, meta learning, and lifelong learningCode Of Ethics:I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Submission Guidelines:I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.Reciprocal Reviewing:I understand the reciprocal reviewing requirement as described on https://iclr.cc/Conferences/2025/CallForPapers. If none of the authors are registered as a reviewer, it may result in a desk rejection at the discretion of the program chairs. To request an exception, please complete this form at https://forms.gle/Huojr6VjkFxiQsUp6.Anonymous Url:I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.No Acknowledgement Section:I certify that there is no acknowledgement section in this submission for double blind review.Submission Number:11196",
    "authors": "Sapna R",
    "url": "https://openreview.net/forum?id=DXz1PDA0Wg",
    "year": 2025,
    "conference": "ICLR",
    "field": "Machine Learning / Deep Learning (ML/DL)"
  }
]